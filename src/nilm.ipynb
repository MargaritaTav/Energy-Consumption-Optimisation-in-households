{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, matthews_corrcoef, mean_absolute_error\n",
    "\n",
    "# loading the dataset\n",
    "store = pd.HDFStore(r'\\ukdale.h5')\n",
    "\n",
    "# Helper function to resample the meter data\n",
    "def resample_meter(store=None, building=1, meter=1, period='1min', cutoff=1000.):\n",
    "    key = '/building{}/elec/meter{}'.format(building,meter)\n",
    "    m = store[key]\n",
    "    v = m.values.flatten()\n",
    "    t = m.index\n",
    "    s = pd.Series(v, index=t).clip(0.,cutoff)\n",
    "    s[s < 10.] = 0.\n",
    "    return s.resample('1s').ffill(limit=300).fillna(0.).resample(period).mean().tz_convert('UTC')\n",
    "\n",
    "# Helper function to get a series from the dataset\n",
    "def get_series(datastore, house, label, cutoff):\n",
    "    \n",
    "    filename = './house_%1d_labels.dat' %house\n",
    "    labels = pd.read_csv(filename, delimiter=' ', header=None, index_col=0).to_dict()[1]\n",
    "    \n",
    "    for i in labels:\n",
    "        if labels[i] == label:\n",
    "            s = resample_meter(store, house, i, '1min', cutoff)\n",
    "    \n",
    "    s.index.name = 'datetime'\n",
    "    return s\n",
    "\n",
    "# Helper function to get status\n",
    "def get_status(app, threshold, min_off, min_on):\n",
    "    condition = app > threshold\n",
    "    d = np.diff(condition)\n",
    "    idx, = d.nonzero()\n",
    "    idx += 1\n",
    "\n",
    "    if condition[0]:\n",
    "        idx = np.r_[0, idx]\n",
    "    if condition[-1]:\n",
    "        idx = np.r_[idx, condition.size]\n",
    "\n",
    "    idx.shape = (-1,2)\n",
    "    on_events = idx[:,0].copy()\n",
    "    off_events = idx[:,1].copy()\n",
    "\n",
    "    if len(on_events) > 0:\n",
    "        off_duration = on_events[1:] - off_events[:-1]\n",
    "        off_duration = np.insert(off_duration, 0, 1000.)\n",
    "        on_events = on_events[off_duration > min_off]\n",
    "        off_events = off_events[np.roll(off_duration, -1) > min_off]\n",
    "\n",
    "        on_duration = off_events - on_events\n",
    "        on_events = on_events[on_duration > min_on]\n",
    "        off_events = off_events[on_duration > min_on]\n",
    "\n",
    "    s = app.copy()\n",
    "    s[:] = 0.\n",
    "    for on, off in zip(on_events, off_events):\n",
    "        s[on:off] = 1.\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for power data\n",
    "class Power(Dataset):\n",
    "    def __init__(self, meter=None, appliance=None, status=None, length=256, border=680, max_power=1., train=False):\n",
    "        self.length = length\n",
    "        self.border = border\n",
    "        self.max_power = max_power\n",
    "        self.train = train\n",
    "\n",
    "        self.meter = meter.copy() / self.max_power\n",
    "        self.appliance = appliance.copy() / self.max_power\n",
    "        self.status = status.copy()\n",
    "\n",
    "        self.epochs = (len(self.meter) - 2*self.border) // self.length\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        i = index * self.length + self.border\n",
    "        if self.train:\n",
    "            i = np.random.randint(self.border, len(self.meter) - self.length - self.border)\n",
    "\n",
    "        x = self.meter.iloc[i-self.border:i+self.length+self.border].values.astype('float32')\n",
    "        y = self.appliance.iloc[i:i+self.length].values.astype('float32')\n",
    "        s = self.status.iloc[i:i+self.length].values.astype('float32')\n",
    "        x -= x.mean()\n",
    "        \n",
    "        return x, y, s\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epochs\n",
    "\n",
    "# Encoder, Decoder, and Temporal Pooling classes for the neural network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=3, padding=1, stride=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.drop(self.bn(F.relu(self.conv(x))))\n",
    "\n",
    "class TemporalPooling(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2):\n",
    "        super(TemporalPooling, self).__init__()\n",
    "        self.pool = nn.AvgPool1d(kernel_size=kernel_size, stride=kernel_size)\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=1, padding=0)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.scale_factor = kernel_size  # Assign kernel size as scale factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(F.relu(x))\n",
    "        # Use self.scale_factor directly here for interpolation\n",
    "        x = F.interpolate(x, scale_factor=self.scale_factor, mode='linear', align_corners=True)\n",
    "        return self.drop(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2, stride=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv = nn.ConvTranspose1d(in_features, out_features, kernel_size=kernel_size, stride=stride, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "class PTPNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(PTPNet, self).__init__()\n",
    "        p = 2\n",
    "        k = 1\n",
    "        features = init_features\n",
    "        self.encoder1 = Encoder(in_channels, features, kernel_size=3, padding=0)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder2 = Encoder(features * 1**k, features * 2**k, kernel_size=3, padding=0)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder3 = Encoder(features * 2**k, features * 4**k, kernel_size=3, padding=0)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder4 = Encoder(features * 4**k, features * 8**k, kernel_size=3, padding=0)\n",
    "        \n",
    "        self.tpool1 = TemporalPooling(features*8**k, features*2**k, kernel_size=5)\n",
    "        self.tpool2 = TemporalPooling(features*8**k, features*2**k, kernel_size=10)\n",
    "        self.tpool3 = TemporalPooling(features*8**k, features*2**k, kernel_size=20)\n",
    "        self.tpool4 = TemporalPooling(features*8**k, features*2**k, kernel_size=30)\n",
    "\n",
    "        self.decoder = Decoder(2*features * 8**k, features * 1**k, kernel_size=p**3, stride=p**3)\n",
    "        self.activation = nn.Conv1d(features * 1**k, out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        tp1 = self.tpool1(enc4)\n",
    "        tp2 = self.tpool2(enc4)\n",
    "        tp3 = self.tpool3(enc4)\n",
    "        tp4 = self.tpool4(enc4)\n",
    "\n",
    "        dec = self.decoder(torch.cat([enc4, tp1, tp2, tp3, tp4], dim=1))\n",
    "        act = self.activation(dec)\n",
    "        return act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, batch_size, n_epochs, filename):\n",
    "    \n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the test loss as the model trains\n",
    "    test_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    # to track the average test loss per epoch as the model trains\n",
    "    avg_test_losses = [] \n",
    "    \n",
    "    min_loss = np.inf\n",
    "    \n",
    " \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "\n",
    "        # train the model #\n",
    "  \n",
    "        model.train()\n",
    "        for batch, (data, target_power, target_status) in enumerate(train_loader, 1):\n",
    "            data = data.unsqueeze(1).to('cpu')\n",
    "            target_power = target_power.to('cpu')\n",
    "            target_status = target_status.to('cpu')\n",
    "            \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "    \n",
    "        # validate the model #\n",
    "      \n",
    "        model.eval() # prep model for evaluation\n",
    "        for data, target_power, target_status in valid_loader:\n",
    "            data = data.unsqueeze(1).to('cpu')\n",
    "            target_power = target_power.to('cpu')\n",
    "            target_status = target_status.to('cpu')\n",
    "            \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "     \n",
    "        # test the model #\n",
    "       \n",
    "        model.eval() # prep model for evaluation\n",
    "        for data, target_power, target_status in test_loader:\n",
    "            data = data.unsqueeze(1).to('cpu')\n",
    "            target_power = target_power.to('cpu')\n",
    "            target_status = target_status.to('cpu')\n",
    "            \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # record validation loss\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        test_loss = np.average(test_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        avg_test_losses.append(test_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f} ' +\n",
    "                     f'test_loss: {test_loss:.5f} ')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "       \n",
    "        if valid_loss < min_loss:\n",
    "            print(f'Validation loss decreased ({min_loss:.6f} --> {valid_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), filename)\n",
    "            min_loss = valid_loss\n",
    "        \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    \n",
    "    return  model, avg_train_losses, avg_valid_losses, avg_test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\2593947627.py:16: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  ds_1.fillna(method='pad', inplace=True)\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\2593947627.py:35: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  ds_2.fillna(method='pad', inplace=True)\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\2593947627.py:56: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  ds_3.fillna(method='pad', inplace=True)\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\2593947627.py:76: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  ds_4.fillna(method='pad', inplace=True)\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\2593947627.py:97: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  ds_5.fillna(method='pad', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# UKDALE dataset processing\n",
    "house = 1\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'fridge', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = get_series(store, house, 'washing_machine', 2500.)\n",
    "a3.name = 'washing_machine'\n",
    "a4 = get_series(store, house, 'microwave', 3000.)\n",
    "a4.name = 'microwave'\n",
    "a5 = get_series(store, house, 'dishwasher', 2500.)\n",
    "a5.name = 'dish_washer'\n",
    "ds_1 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_1.fillna(method='pad', inplace=True)\n",
    "\n",
    "# Train and validation splits\n",
    "ds_1_train = ds_1[pd.to_datetime('2013-04-12', utc=True):pd.to_datetime('2014-12-15', utc=True)]\n",
    "ds_1_valid = ds_1[pd.to_datetime('2014-12-15', utc=True):]\n",
    "house = 2\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'fridge', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = get_series(store, house, 'washing_machine', 2500.)\n",
    "a3.name = 'washing_machine'\n",
    "a4 = get_series(store, house, 'microwave', 3000.)\n",
    "a4.name = 'microwave'\n",
    "a5 = get_series(store, house, 'dish_washer', 2500.)\n",
    "a5.name = 'dish_washer'\n",
    "ds_2 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_2.fillna(method='pad', inplace=True)\n",
    "\n",
    "# Example of adding timezone information\n",
    "ds_2_train = ds_2[pd.to_datetime('2013-05-22').tz_localize('UTC'):pd.to_datetime('2013-10-03 06:16').tz_localize('UTC')]\n",
    "ds_2_valid = ds_2[pd.to_datetime('2013-10-03 06:16').tz_localize('UTC'):]\n",
    "\n",
    "\n",
    "house = 3\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = 0.*m\n",
    "a2.name = 'fridge'\n",
    "a3 = 0.*m\n",
    "a3.name = 'washing_machine'\n",
    "a4 = 0.*m\n",
    "a4.name = 'microwave'\n",
    "a5 = 0.*m\n",
    "a5.name = 'dish_washer'\n",
    "ds_3 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_3.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_3_train = ds_3[pd.to_datetime('2013-02-27').tz_localize('UTC'):pd.to_datetime('2013-04-01 06:15').tz_localize('UTC')]\n",
    "ds_3_valid = ds_3[pd.to_datetime('2013-04-01 06:15').tz_localize('UTC'):]\n",
    "\n",
    "\n",
    "house = 4\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle_radio', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'freezer', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = 0.*m\n",
    "a3.name = 'washing_machine'\n",
    "a4 = 0.*m\n",
    "a4.name = 'microwave'\n",
    "a5 = 0.*m\n",
    "a5.name = 'dish_washer'\n",
    "ds_4 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_4.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_4_train = ds_4[pd.to_datetime('2013-03-09').tz_localize('UTC'):pd.to_datetime('2013-09-24 06:15').tz_localize('UTC')]\n",
    "ds_4_valid = ds_4[pd.to_datetime('2013-09-24 06:15').tz_localize('UTC'):]\n",
    "\n",
    "\n",
    "\n",
    "house = 5\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'fridge_freezer', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = get_series(store, house, 'washer_dryer', 2500.)\n",
    "a3.name = 'washing_machine'\n",
    "a4 = get_series(store, house, 'microwave', 3000.)\n",
    "a4.name = 'microwave'\n",
    "a5 = get_series(store, house, 'dishwasher', 2500.)\n",
    "a5.name = 'dish_washer'\n",
    "ds_5 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_5.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_5_train = ds_5[pd.to_datetime('2014-06-29').tz_localize('UTC'):pd.to_datetime('2014-09-01').tz_localize('UTC')]\n",
    "ds_5_valid = ds_5[pd.to_datetime('2014-09-01').tz_localize('UTC'):]\n",
    "\n",
    "ds_1_train.reset_index().to_feather('./UKDALE_1_train.feather')\n",
    "ds_2_train.reset_index().to_feather('./UKDALE_2_train.feather')\n",
    "ds_3_train.reset_index().to_feather('./UKDALE_3_train.feather')\n",
    "ds_4_train.reset_index().to_feather('./UKDALE_4_train.feather')\n",
    "ds_5_train.reset_index().to_feather('./UKDALE_5_train.feather')\n",
    "\n",
    "ds_1_valid.reset_index().to_feather('./UKDALE_1_valid.feather')\n",
    "ds_2_valid.reset_index().to_feather('./UKDALE_2_valid.feather')\n",
    "ds_3_valid.reset_index().to_feather('./UKDALE_3_valid.feather')\n",
    "ds_4_valid.reset_index().to_feather('./UKDALE_4_valid.feather')\n",
    "ds_5_valid.reset_index().to_feather('./UKDALE_5_valid.feather')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[0]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[-1]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[0]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[-1]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[0]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[-1]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[0]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[-1]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[0]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[-1]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[0]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[-1]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[0]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[-1]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[0]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[-1]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[0]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[-1]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[0]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[-1]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[0]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[-1]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[0]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[-1]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[0]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[-1]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[0]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[-1]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[0]:\n",
      "C:\\Users\\tavkazam\\AppData\\Local\\Temp\\3\\ipykernel_16320\\1118172651.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if condition[-1]:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "APPLIANCE = ['fridge', 'dish_washer', 'washing_machine']\n",
    "THRESHOLD = [50., 10., 20.]\n",
    "MIN_ON = [1., 30., 30.]\n",
    "MIN_OFF = [1., 30., 3.]\n",
    "\n",
    "METER = 'aggregate'\n",
    "SEQ_LEN = 60*8\n",
    "BORDER = 16\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MAX_POWER = 2000.\n",
    "\n",
    "# Now combine the data for all houses and prepare them for the model\n",
    "ds_meter = []\n",
    "ds_appliance = []\n",
    "ds_status = []\n",
    "for i in range(5):\n",
    "    ds = pd.read_feather(f'./UKDALE_{i+1}_train.feather')\n",
    "    ds.set_index('datetime', inplace=True)\n",
    "\n",
    "    meter = ds[METER]\n",
    "    appliances = ds[APPLIANCE]\n",
    "\n",
    "    status = pd.DataFrame()\n",
    "    for a in range(len(APPLIANCE)):\n",
    "        status = pd.concat([status, get_status(ds[APPLIANCE[a]], THRESHOLD[a], MIN_OFF[a], MIN_ON[a])], axis=1)\n",
    "\n",
    "    ds_meter.append(meter)\n",
    "    ds_appliance.append(appliances)\n",
    "    ds_status.append(status)\n",
    "# Calculate the length of each dataset\n",
    "ds_len = [len(ds_meter[i]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds_timestamps = []\n",
    "for i in range(5):\n",
    "    ds = pd.read_feather(f'./UKDALE_{i+1}_train.feather')\n",
    "    # Ensuring datetime is properly parsed and set as index if not already\n",
    "    if 'datetime' not in ds.columns:\n",
    "        ds['datetime'] = pd.to_datetime(ds['timestamp'])  # Adjust if the column name differs\n",
    "    ds.set_index('datetime', inplace=True)\n",
    "    \n",
    "    timestamps = ds.index  # Assuming the index is the timestamp after setting it\n",
    "    ds_timestamps.append(timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_house_train = [Power(ds_meter[i][:int(0.8*ds_len[i])], \n",
    "                        ds_appliance[i][:int(0.8*ds_len[i])], \n",
    "                        ds_status[i][:int(0.8*ds_len[i])], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, True) for i in range(5+0)]\n",
    "\n",
    "ds_house_valid = [Power(ds_meter[i][int(0.8*ds_len[i]):int(0.9*ds_len[i])], \n",
    "                        ds_appliance[i][int(0.8*ds_len[i]):int(0.9*ds_len[i])],\n",
    "                        ds_status[i][int(0.8*ds_len[i]):int(0.9*ds_len[i])], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_house_test  = [Power(ds_meter[i][int(0.9*ds_len[i]):], \n",
    "                        ds_appliance[i][int(0.9*ds_len[i]):],\n",
    "                        ds_status[i][int(0.9*ds_len[i]):], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_house_total  = [Power(ds_meter[i], ds_appliance[i], ds_status[i], \n",
    "                         SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_train_seen = torch.utils.data.ConcatDataset([ds_house_train[0], \n",
    "                                                ds_house_train[1], \n",
    "                                               \n",
    "                                                ds_house_train[4]\n",
    "                                                ])\n",
    "ds_valid_seen = torch.utils.data.ConcatDataset([ds_house_valid[0], \n",
    "                                             \n",
    "                                                ])\n",
    "\n",
    "dl_train_seen = DataLoader(dataset = ds_train_seen, batch_size = BATCH_SIZE, shuffle=True)\n",
    "dl_valid_seen = DataLoader(dataset = ds_valid_seen, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_test_seen = DataLoader(dataset = ds_house_test[0], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "ds_train_unseen = torch.utils.data.ConcatDataset([ds_house_train[0], \n",
    "                                                \n",
    "                                                  ds_house_train[4]\n",
    "                                                  ])\n",
    "ds_valid_unseen = torch.utils.data.ConcatDataset([ds_house_valid[0], \n",
    "                                               \n",
    "                                                  ds_house_valid[4]\n",
    "                                                  ])\n",
    "dl_train_unseen = DataLoader(dataset = ds_train_unseen, batch_size = BATCH_SIZE, shuffle=True)\n",
    "dl_valid_unseen = DataLoader(dataset = ds_valid_unseen, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_test_unseen = DataLoader(dataset = ds_house_total[1], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "dl_house_test = [DataLoader(dataset = ds_house_test[i], batch_size = 1, shuffle=False) for i in range(5)]\n",
    "dl_house_valid = [DataLoader(dataset = ds_house_valid[i], batch_size = 1, shuffle=False) for i in range(5)]\n",
    "dl_house_total = [DataLoader(dataset = ds_house_total[i], batch_size = 1, shuffle=False) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dl_house_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL for seen house 1\n",
      "[  1/200] train_loss: 0.65242 valid_loss: 0.53899 test_loss: 0.53672 \n",
      "Validation loss decreased (inf --> 0.538994).  Saving model ...\n",
      "[  2/200] train_loss: 0.44846 valid_loss: 0.38626 test_loss: 0.38893 \n",
      "Validation loss decreased (0.538994 --> 0.386258).  Saving model ...\n",
      "[  3/200] train_loss: 0.33178 valid_loss: 0.31579 test_loss: 0.32059 \n",
      "Validation loss decreased (0.386258 --> 0.315795).  Saving model ...\n",
      "[  4/200] train_loss: 0.27023 valid_loss: 0.26329 test_loss: 0.27423 \n",
      "Validation loss decreased (0.315795 --> 0.263293).  Saving model ...\n",
      "[  5/200] train_loss: 0.23301 valid_loss: 0.22926 test_loss: 0.24047 \n",
      "Validation loss decreased (0.263293 --> 0.229263).  Saving model ...\n",
      "[  6/200] train_loss: 0.20447 valid_loss: 0.20894 test_loss: 0.22131 \n",
      "Validation loss decreased (0.229263 --> 0.208936).  Saving model ...\n",
      "[  7/200] train_loss: 0.19142 valid_loss: 0.19124 test_loss: 0.20507 \n",
      "Validation loss decreased (0.208936 --> 0.191240).  Saving model ...\n",
      "[  8/200] train_loss: 0.17784 valid_loss: 0.17932 test_loss: 0.19315 \n",
      "Validation loss decreased (0.191240 --> 0.179319).  Saving model ...\n",
      "[  9/200] train_loss: 0.16983 valid_loss: 0.17176 test_loss: 0.18559 \n",
      "Validation loss decreased (0.179319 --> 0.171760).  Saving model ...\n",
      "[ 10/200] train_loss: 0.16210 valid_loss: 0.16709 test_loss: 0.17955 \n",
      "Validation loss decreased (0.171760 --> 0.167089).  Saving model ...\n",
      "[ 11/200] train_loss: 0.15508 valid_loss: 0.16077 test_loss: 0.17333 \n",
      "Validation loss decreased (0.167089 --> 0.160774).  Saving model ...\n",
      "[ 12/200] train_loss: 0.15136 valid_loss: 0.15780 test_loss: 0.17027 \n",
      "Validation loss decreased (0.160774 --> 0.157799).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14669 valid_loss: 0.15384 test_loss: 0.16670 \n",
      "Validation loss decreased (0.157799 --> 0.153838).  Saving model ...\n",
      "[ 14/200] train_loss: 0.14426 valid_loss: 0.15343 test_loss: 0.16401 \n",
      "Validation loss decreased (0.153838 --> 0.153425).  Saving model ...\n",
      "[ 15/200] train_loss: 0.14188 valid_loss: 0.14599 test_loss: 0.15851 \n",
      "Validation loss decreased (0.153425 --> 0.145988).  Saving model ...\n",
      "[ 16/200] train_loss: 0.13990 valid_loss: 0.14538 test_loss: 0.16018 \n",
      "Validation loss decreased (0.145988 --> 0.145378).  Saving model ...\n",
      "[ 17/200] train_loss: 0.13467 valid_loss: 0.14332 test_loss: 0.15708 \n",
      "Validation loss decreased (0.145378 --> 0.143317).  Saving model ...\n",
      "[ 18/200] train_loss: 0.13137 valid_loss: 0.14145 test_loss: 0.15306 \n",
      "Validation loss decreased (0.143317 --> 0.141454).  Saving model ...\n",
      "[ 19/200] train_loss: 0.13353 valid_loss: 0.13902 test_loss: 0.15139 \n",
      "Validation loss decreased (0.141454 --> 0.139019).  Saving model ...\n",
      "[ 20/200] train_loss: 0.13153 valid_loss: 0.13729 test_loss: 0.15117 \n",
      "Validation loss decreased (0.139019 --> 0.137287).  Saving model ...\n",
      "[ 21/200] train_loss: 0.12867 valid_loss: 0.14009 test_loss: 0.14976 \n",
      "[ 22/200] train_loss: 0.12652 valid_loss: 0.13436 test_loss: 0.14763 \n",
      "Validation loss decreased (0.137287 --> 0.134357).  Saving model ...\n",
      "[ 23/200] train_loss: 0.12709 valid_loss: 0.13229 test_loss: 0.14624 \n",
      "Validation loss decreased (0.134357 --> 0.132289).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12254 valid_loss: 0.13095 test_loss: 0.14518 \n",
      "Validation loss decreased (0.132289 --> 0.130954).  Saving model ...\n",
      "[ 25/200] train_loss: 0.12216 valid_loss: 0.13342 test_loss: 0.15001 \n",
      "[ 26/200] train_loss: 0.12289 valid_loss: 0.13153 test_loss: 0.14353 \n",
      "[ 27/200] train_loss: 0.11695 valid_loss: 0.12795 test_loss: 0.14231 \n",
      "Validation loss decreased (0.130954 --> 0.127946).  Saving model ...\n",
      "[ 28/200] train_loss: 0.12096 valid_loss: 0.13201 test_loss: 0.14561 \n",
      "[ 29/200] train_loss: 0.11838 valid_loss: 0.12798 test_loss: 0.14421 \n",
      "[ 30/200] train_loss: 0.11823 valid_loss: 0.12803 test_loss: 0.14312 \n",
      "[ 31/200] train_loss: 0.11982 valid_loss: 0.12493 test_loss: 0.13903 \n",
      "Validation loss decreased (0.127946 --> 0.124931).  Saving model ...\n",
      "[ 32/200] train_loss: 0.11801 valid_loss: 0.12125 test_loss: 0.13633 \n",
      "Validation loss decreased (0.124931 --> 0.121248).  Saving model ...\n",
      "[ 33/200] train_loss: 0.11714 valid_loss: 0.12187 test_loss: 0.13578 \n",
      "[ 34/200] train_loss: 0.11466 valid_loss: 0.12206 test_loss: 0.13723 \n",
      "[ 35/200] train_loss: 0.11236 valid_loss: 0.12433 test_loss: 0.14037 \n",
      "[ 36/200] train_loss: 0.11393 valid_loss: 0.11809 test_loss: 0.13423 \n",
      "Validation loss decreased (0.121248 --> 0.118095).  Saving model ...\n",
      "[ 37/200] train_loss: 0.11238 valid_loss: 0.11979 test_loss: 0.13496 \n",
      "[ 38/200] train_loss: 0.11132 valid_loss: 0.12244 test_loss: 0.13800 \n",
      "[ 39/200] train_loss: 0.11069 valid_loss: 0.11716 test_loss: 0.13376 \n",
      "Validation loss decreased (0.118095 --> 0.117162).  Saving model ...\n",
      "[ 40/200] train_loss: 0.10682 valid_loss: 0.11564 test_loss: 0.13350 \n",
      "Validation loss decreased (0.117162 --> 0.115640).  Saving model ...\n",
      "[ 41/200] train_loss: 0.10767 valid_loss: 0.11591 test_loss: 0.13091 \n",
      "[ 42/200] train_loss: 0.10812 valid_loss: 0.11401 test_loss: 0.13169 \n",
      "Validation loss decreased (0.115640 --> 0.114010).  Saving model ...\n",
      "[ 43/200] train_loss: 0.10790 valid_loss: 0.11713 test_loss: 0.13196 \n",
      "[ 44/200] train_loss: 0.10885 valid_loss: 0.11549 test_loss: 0.12876 \n",
      "[ 45/200] train_loss: 0.10700 valid_loss: 0.11593 test_loss: 0.13273 \n",
      "[ 46/200] train_loss: 0.10698 valid_loss: 0.11287 test_loss: 0.12860 \n",
      "Validation loss decreased (0.114010 --> 0.112867).  Saving model ...\n",
      "[ 47/200] train_loss: 0.10601 valid_loss: 0.11334 test_loss: 0.12980 \n",
      "[ 48/200] train_loss: 0.10455 valid_loss: 0.11701 test_loss: 0.12886 \n",
      "[ 49/200] train_loss: 0.10220 valid_loss: 0.11302 test_loss: 0.12797 \n",
      "[ 50/200] train_loss: 0.10041 valid_loss: 0.11060 test_loss: 0.12688 \n",
      "Validation loss decreased (0.112867 --> 0.110603).  Saving model ...\n",
      "[ 51/200] train_loss: 0.10367 valid_loss: 0.10942 test_loss: 0.12605 \n",
      "Validation loss decreased (0.110603 --> 0.109420).  Saving model ...\n",
      "[ 52/200] train_loss: 0.10266 valid_loss: 0.11230 test_loss: 0.12589 \n",
      "[ 53/200] train_loss: 0.10285 valid_loss: 0.10843 test_loss: 0.12612 \n",
      "Validation loss decreased (0.109420 --> 0.108430).  Saving model ...\n",
      "[ 54/200] train_loss: 0.10286 valid_loss: 0.11103 test_loss: 0.12581 \n",
      "[ 55/200] train_loss: 0.10208 valid_loss: 0.11129 test_loss: 0.12547 \n",
      "[ 56/200] train_loss: 0.10241 valid_loss: 0.10909 test_loss: 0.12421 \n",
      "[ 57/200] train_loss: 0.10159 valid_loss: 0.11727 test_loss: 0.12904 \n",
      "[ 58/200] train_loss: 0.09590 valid_loss: 0.10885 test_loss: 0.12264 \n",
      "[ 59/200] train_loss: 0.09884 valid_loss: 0.11089 test_loss: 0.12509 \n",
      "[ 60/200] train_loss: 0.09953 valid_loss: 0.10667 test_loss: 0.12191 \n",
      "Validation loss decreased (0.108430 --> 0.106665).  Saving model ...\n",
      "[ 61/200] train_loss: 0.10031 valid_loss: 0.11018 test_loss: 0.12352 \n",
      "[ 62/200] train_loss: 0.09989 valid_loss: 0.10617 test_loss: 0.12186 \n",
      "Validation loss decreased (0.106665 --> 0.106174).  Saving model ...\n",
      "[ 63/200] train_loss: 0.09942 valid_loss: 0.10725 test_loss: 0.12057 \n",
      "[ 64/200] train_loss: 0.09875 valid_loss: 0.10515 test_loss: 0.12015 \n",
      "Validation loss decreased (0.106174 --> 0.105147).  Saving model ...\n",
      "[ 65/200] train_loss: 0.09620 valid_loss: 0.10588 test_loss: 0.11947 \n",
      "[ 66/200] train_loss: 0.09492 valid_loss: 0.10176 test_loss: 0.12057 \n",
      "Validation loss decreased (0.105147 --> 0.101764).  Saving model ...\n",
      "[ 67/200] train_loss: 0.09698 valid_loss: 0.10753 test_loss: 0.11995 \n",
      "[ 68/200] train_loss: 0.09665 valid_loss: 0.10355 test_loss: 0.11855 \n",
      "[ 69/200] train_loss: 0.09820 valid_loss: 0.10606 test_loss: 0.11941 \n",
      "[ 70/200] train_loss: 0.09621 valid_loss: 0.10637 test_loss: 0.12577 \n",
      "[ 71/200] train_loss: 0.09552 valid_loss: 0.10357 test_loss: 0.11880 \n",
      "[ 72/200] train_loss: 0.09540 valid_loss: 0.10234 test_loss: 0.11795 \n",
      "[ 73/200] train_loss: 0.09513 valid_loss: 0.10343 test_loss: 0.11747 \n",
      "[ 74/200] train_loss: 0.09461 valid_loss: 0.10046 test_loss: 0.11735 \n",
      "Validation loss decreased (0.101764 --> 0.100460).  Saving model ...\n",
      "[ 75/200] train_loss: 0.09509 valid_loss: 0.09970 test_loss: 0.11639 \n",
      "Validation loss decreased (0.100460 --> 0.099702).  Saving model ...\n",
      "[ 76/200] train_loss: 0.09583 valid_loss: 0.09964 test_loss: 0.11590 \n",
      "Validation loss decreased (0.099702 --> 0.099637).  Saving model ...\n",
      "[ 77/200] train_loss: 0.09702 valid_loss: 0.10297 test_loss: 0.11533 \n",
      "[ 78/200] train_loss: 0.09371 valid_loss: 0.09982 test_loss: 0.11630 \n",
      "[ 79/200] train_loss: 0.09559 valid_loss: 0.10007 test_loss: 0.11507 \n",
      "[ 80/200] train_loss: 0.09428 valid_loss: 0.09958 test_loss: 0.11408 \n",
      "Validation loss decreased (0.099637 --> 0.099577).  Saving model ...\n",
      "[ 81/200] train_loss: 0.09487 valid_loss: 0.10260 test_loss: 0.11639 \n",
      "[ 82/200] train_loss: 0.09109 valid_loss: 0.09896 test_loss: 0.11387 \n",
      "Validation loss decreased (0.099577 --> 0.098956).  Saving model ...\n",
      "[ 83/200] train_loss: 0.09192 valid_loss: 0.10385 test_loss: 0.11600 \n",
      "[ 84/200] train_loss: 0.09063 valid_loss: 0.09959 test_loss: 0.11360 \n",
      "[ 85/200] train_loss: 0.09015 valid_loss: 0.10016 test_loss: 0.11283 \n",
      "[ 86/200] train_loss: 0.09112 valid_loss: 0.09854 test_loss: 0.11287 \n",
      "Validation loss decreased (0.098956 --> 0.098542).  Saving model ...\n",
      "[ 87/200] train_loss: 0.09336 valid_loss: 0.10007 test_loss: 0.11340 \n",
      "[ 88/200] train_loss: 0.09294 valid_loss: 0.09885 test_loss: 0.11308 \n",
      "[ 89/200] train_loss: 0.09090 valid_loss: 0.09987 test_loss: 0.11432 \n",
      "[ 90/200] train_loss: 0.09279 valid_loss: 0.09873 test_loss: 0.11281 \n",
      "[ 91/200] train_loss: 0.09048 valid_loss: 0.10380 test_loss: 0.11336 \n",
      "[ 92/200] train_loss: 0.09121 valid_loss: 0.10103 test_loss: 0.11216 \n",
      "[ 93/200] train_loss: 0.09071 valid_loss: 0.10164 test_loss: 0.11155 \n",
      "[ 94/200] train_loss: 0.08878 valid_loss: 0.09631 test_loss: 0.11180 \n",
      "Validation loss decreased (0.098542 --> 0.096313).  Saving model ...\n",
      "[ 95/200] train_loss: 0.08853 valid_loss: 0.09750 test_loss: 0.11136 \n",
      "[ 96/200] train_loss: 0.08909 valid_loss: 0.09815 test_loss: 0.11124 \n",
      "[ 97/200] train_loss: 0.09002 valid_loss: 0.09852 test_loss: 0.11115 \n",
      "[ 98/200] train_loss: 0.09086 valid_loss: 0.09966 test_loss: 0.11195 \n",
      "[ 99/200] train_loss: 0.08905 valid_loss: 0.09624 test_loss: 0.11050 \n",
      "Validation loss decreased (0.096313 --> 0.096243).  Saving model ...\n",
      "[100/200] train_loss: 0.08748 valid_loss: 0.10170 test_loss: 0.11090 \n",
      "[101/200] train_loss: 0.08782 valid_loss: 0.09969 test_loss: 0.11297 \n",
      "[102/200] train_loss: 0.08956 valid_loss: 0.09706 test_loss: 0.11010 \n",
      "[103/200] train_loss: 0.09130 valid_loss: 0.09578 test_loss: 0.11101 \n",
      "Validation loss decreased (0.096243 --> 0.095781).  Saving model ...\n",
      "[104/200] train_loss: 0.08830 valid_loss: 0.10150 test_loss: 0.11258 \n",
      "[105/200] train_loss: 0.08622 valid_loss: 0.09702 test_loss: 0.11017 \n",
      "[106/200] train_loss: 0.08625 valid_loss: 0.09689 test_loss: 0.10862 \n",
      "[107/200] train_loss: 0.08802 valid_loss: 0.09488 test_loss: 0.10957 \n",
      "Validation loss decreased (0.095781 --> 0.094881).  Saving model ...\n",
      "[108/200] train_loss: 0.08974 valid_loss: 0.09370 test_loss: 0.10747 \n",
      "Validation loss decreased (0.094881 --> 0.093699).  Saving model ...\n",
      "[109/200] train_loss: 0.08774 valid_loss: 0.09548 test_loss: 0.10897 \n",
      "[110/200] train_loss: 0.08520 valid_loss: 0.09625 test_loss: 0.10860 \n",
      "[111/200] train_loss: 0.08899 valid_loss: 0.09568 test_loss: 0.10815 \n",
      "[112/200] train_loss: 0.08785 valid_loss: 0.09404 test_loss: 0.10804 \n",
      "[113/200] train_loss: 0.08647 valid_loss: 0.09529 test_loss: 0.10745 \n",
      "[114/200] train_loss: 0.08571 valid_loss: 0.09570 test_loss: 0.10830 \n",
      "[115/200] train_loss: 0.08487 valid_loss: 0.09710 test_loss: 0.10805 \n",
      "[116/200] train_loss: 0.08513 valid_loss: 0.09369 test_loss: 0.10702 \n",
      "Validation loss decreased (0.093699 --> 0.093689).  Saving model ...\n",
      "[117/200] train_loss: 0.08436 valid_loss: 0.09421 test_loss: 0.10723 \n",
      "[118/200] train_loss: 0.08400 valid_loss: 0.09397 test_loss: 0.10656 \n",
      "[119/200] train_loss: 0.08490 valid_loss: 0.09350 test_loss: 0.10606 \n",
      "Validation loss decreased (0.093689 --> 0.093504).  Saving model ...\n",
      "[120/200] train_loss: 0.08453 valid_loss: 0.09272 test_loss: 0.10651 \n",
      "Validation loss decreased (0.093504 --> 0.092720).  Saving model ...\n",
      "[121/200] train_loss: 0.08712 valid_loss: 0.09426 test_loss: 0.10631 \n",
      "[122/200] train_loss: 0.08453 valid_loss: 0.09408 test_loss: 0.10798 \n",
      "[123/200] train_loss: 0.08389 valid_loss: 0.09213 test_loss: 0.10452 \n",
      "Validation loss decreased (0.092720 --> 0.092133).  Saving model ...\n",
      "[124/200] train_loss: 0.08434 valid_loss: 0.09185 test_loss: 0.10476 \n",
      "Validation loss decreased (0.092133 --> 0.091846).  Saving model ...\n",
      "[125/200] train_loss: 0.08286 valid_loss: 0.09166 test_loss: 0.10474 \n",
      "Validation loss decreased (0.091846 --> 0.091658).  Saving model ...\n",
      "[126/200] train_loss: 0.08719 valid_loss: 0.09213 test_loss: 0.10565 \n",
      "[127/200] train_loss: 0.08512 valid_loss: 0.09811 test_loss: 0.10613 \n",
      "[128/200] train_loss: 0.08680 valid_loss: 0.09611 test_loss: 0.10603 \n",
      "[129/200] train_loss: 0.08308 valid_loss: 0.09563 test_loss: 0.10558 \n",
      "[130/200] train_loss: 0.08490 valid_loss: 0.09275 test_loss: 0.10434 \n",
      "[131/200] train_loss: 0.08362 valid_loss: 0.09568 test_loss: 0.10668 \n",
      "[132/200] train_loss: 0.08228 valid_loss: 0.09406 test_loss: 0.10467 \n",
      "[133/200] train_loss: 0.08242 valid_loss: 0.09120 test_loss: 0.10362 \n",
      "Validation loss decreased (0.091658 --> 0.091197).  Saving model ...\n",
      "[134/200] train_loss: 0.08356 valid_loss: 0.09067 test_loss: 0.10341 \n",
      "Validation loss decreased (0.091197 --> 0.090667).  Saving model ...\n",
      "[135/200] train_loss: 0.08247 valid_loss: 0.09235 test_loss: 0.10508 \n",
      "[136/200] train_loss: 0.08292 valid_loss: 0.09305 test_loss: 0.10465 \n",
      "[137/200] train_loss: 0.08161 valid_loss: 0.09273 test_loss: 0.10426 \n",
      "[138/200] train_loss: 0.07970 valid_loss: 0.09412 test_loss: 0.10478 \n",
      "[139/200] train_loss: 0.08297 valid_loss: 0.09100 test_loss: 0.10468 \n",
      "[140/200] train_loss: 0.08643 valid_loss: 0.08975 test_loss: 0.10373 \n",
      "Validation loss decreased (0.090667 --> 0.089754).  Saving model ...\n",
      "[141/200] train_loss: 0.08149 valid_loss: 0.09121 test_loss: 0.10325 \n",
      "[142/200] train_loss: 0.08109 valid_loss: 0.09025 test_loss: 0.10385 \n",
      "[143/200] train_loss: 0.08284 valid_loss: 0.09228 test_loss: 0.10349 \n",
      "[144/200] train_loss: 0.07887 valid_loss: 0.09005 test_loss: 0.10323 \n",
      "[145/200] train_loss: 0.08295 valid_loss: 0.09010 test_loss: 0.10206 \n",
      "[146/200] train_loss: 0.08015 valid_loss: 0.08895 test_loss: 0.10289 \n",
      "Validation loss decreased (0.089754 --> 0.088947).  Saving model ...\n",
      "[147/200] train_loss: 0.08285 valid_loss: 0.09054 test_loss: 0.10416 \n",
      "[148/200] train_loss: 0.08191 valid_loss: 0.08979 test_loss: 0.10317 \n",
      "[149/200] train_loss: 0.08338 valid_loss: 0.09045 test_loss: 0.10178 \n",
      "[150/200] train_loss: 0.08166 valid_loss: 0.08923 test_loss: 0.10125 \n",
      "[151/200] train_loss: 0.07968 valid_loss: 0.09059 test_loss: 0.10267 \n",
      "[152/200] train_loss: 0.08084 valid_loss: 0.09080 test_loss: 0.10216 \n",
      "[153/200] train_loss: 0.08180 valid_loss: 0.08720 test_loss: 0.10116 \n",
      "Validation loss decreased (0.088947 --> 0.087202).  Saving model ...\n",
      "[154/200] train_loss: 0.08311 valid_loss: 0.08931 test_loss: 0.10028 \n",
      "[155/200] train_loss: 0.08068 valid_loss: 0.08915 test_loss: 0.10099 \n",
      "[156/200] train_loss: 0.08180 valid_loss: 0.08790 test_loss: 0.10056 \n",
      "[157/200] train_loss: 0.07951 valid_loss: 0.09222 test_loss: 0.10149 \n",
      "[158/200] train_loss: 0.08078 valid_loss: 0.09139 test_loss: 0.09996 \n",
      "[159/200] train_loss: 0.08075 valid_loss: 0.08882 test_loss: 0.10141 \n",
      "[160/200] train_loss: 0.08112 valid_loss: 0.08837 test_loss: 0.10005 \n",
      "[161/200] train_loss: 0.07981 valid_loss: 0.09283 test_loss: 0.10277 \n",
      "[162/200] train_loss: 0.08185 valid_loss: 0.08973 test_loss: 0.09959 \n",
      "[163/200] train_loss: 0.07747 valid_loss: 0.09319 test_loss: 0.10269 \n",
      "[164/200] train_loss: 0.07988 valid_loss: 0.08904 test_loss: 0.09942 \n",
      "[165/200] train_loss: 0.07862 valid_loss: 0.08790 test_loss: 0.10091 \n",
      "[166/200] train_loss: 0.07820 valid_loss: 0.09147 test_loss: 0.10127 \n",
      "[167/200] train_loss: 0.07820 valid_loss: 0.08828 test_loss: 0.10027 \n",
      "[168/200] train_loss: 0.07897 valid_loss: 0.09014 test_loss: 0.10066 \n",
      "[169/200] train_loss: 0.07850 valid_loss: 0.09087 test_loss: 0.09993 \n",
      "[170/200] train_loss: 0.08086 valid_loss: 0.08621 test_loss: 0.09964 \n",
      "Validation loss decreased (0.087202 --> 0.086207).  Saving model ...\n",
      "[171/200] train_loss: 0.07839 valid_loss: 0.08780 test_loss: 0.10013 \n",
      "[172/200] train_loss: 0.07697 valid_loss: 0.08637 test_loss: 0.09929 \n",
      "[173/200] train_loss: 0.07704 valid_loss: 0.09110 test_loss: 0.10031 \n",
      "[174/200] train_loss: 0.07503 valid_loss: 0.09149 test_loss: 0.10099 \n",
      "[175/200] train_loss: 0.07855 valid_loss: 0.08587 test_loss: 0.09880 \n",
      "Validation loss decreased (0.086207 --> 0.085869).  Saving model ...\n",
      "[176/200] train_loss: 0.07744 valid_loss: 0.08554 test_loss: 0.10045 \n",
      "Validation loss decreased (0.085869 --> 0.085536).  Saving model ...\n",
      "[177/200] train_loss: 0.07682 valid_loss: 0.08591 test_loss: 0.09896 \n",
      "[178/200] train_loss: 0.07781 valid_loss: 0.08942 test_loss: 0.09943 \n",
      "[179/200] train_loss: 0.07680 valid_loss: 0.08651 test_loss: 0.09777 \n",
      "[180/200] train_loss: 0.07767 valid_loss: 0.08775 test_loss: 0.09857 \n",
      "[181/200] train_loss: 0.07711 valid_loss: 0.09439 test_loss: 0.09921 \n",
      "[182/200] train_loss: 0.07528 valid_loss: 0.08873 test_loss: 0.09817 \n",
      "[183/200] train_loss: 0.07718 valid_loss: 0.09206 test_loss: 0.09788 \n",
      "[184/200] train_loss: 0.07982 valid_loss: 0.08841 test_loss: 0.09801 \n",
      "[185/200] train_loss: 0.07876 valid_loss: 0.08724 test_loss: 0.09833 \n",
      "[186/200] train_loss: 0.07626 valid_loss: 0.08688 test_loss: 0.09740 \n",
      "[187/200] train_loss: 0.07626 valid_loss: 0.08453 test_loss: 0.09792 \n",
      "Validation loss decreased (0.085536 --> 0.084531).  Saving model ...\n",
      "[188/200] train_loss: 0.07671 valid_loss: 0.08826 test_loss: 0.09795 \n",
      "[189/200] train_loss: 0.07427 valid_loss: 0.08833 test_loss: 0.09878 \n",
      "[190/200] train_loss: 0.07413 valid_loss: 0.08740 test_loss: 0.09819 \n",
      "[191/200] train_loss: 0.07467 valid_loss: 0.08883 test_loss: 0.09833 \n",
      "[192/200] train_loss: 0.07653 valid_loss: 0.08808 test_loss: 0.09870 \n",
      "[193/200] train_loss: 0.07754 valid_loss: 0.08852 test_loss: 0.09882 \n",
      "[194/200] train_loss: 0.07468 valid_loss: 0.08924 test_loss: 0.09814 \n",
      "[195/200] train_loss: 0.07392 valid_loss: 0.08494 test_loss: 0.09747 \n",
      "[196/200] train_loss: 0.07538 valid_loss: 0.08764 test_loss: 0.09898 \n",
      "[197/200] train_loss: 0.07720 valid_loss: 0.08828 test_loss: 0.09776 \n",
      "[198/200] train_loss: 0.07622 valid_loss: 0.08636 test_loss: 0.09721 \n",
      "[199/200] train_loss: 0.07732 valid_loss: 0.08450 test_loss: 0.09837 \n",
      "Validation loss decreased (0.084531 --> 0.084498).  Saving model ...\n",
      "[200/200] train_loss: 0.07582 valid_loss: 0.08633 test_loss: 0.09841 \n",
      "Model 1 trained for seen data.\n",
      "TRAINING MODEL for seen house 2\n",
      "[  1/200] train_loss: 0.54414 valid_loss: 0.42951 test_loss: 0.43231 \n",
      "Validation loss decreased (inf --> 0.429506).  Saving model ...\n",
      "[  2/200] train_loss: 0.34752 valid_loss: 0.31359 test_loss: 0.32723 \n",
      "Validation loss decreased (0.429506 --> 0.313595).  Saving model ...\n",
      "[  3/200] train_loss: 0.27038 valid_loss: 0.26944 test_loss: 0.28446 \n",
      "Validation loss decreased (0.313595 --> 0.269442).  Saving model ...\n",
      "[  4/200] train_loss: 0.23320 valid_loss: 0.23333 test_loss: 0.24959 \n",
      "Validation loss decreased (0.269442 --> 0.233325).  Saving model ...\n",
      "[  5/200] train_loss: 0.20926 valid_loss: 0.20856 test_loss: 0.22312 \n",
      "Validation loss decreased (0.233325 --> 0.208559).  Saving model ...\n",
      "[  6/200] train_loss: 0.19108 valid_loss: 0.19351 test_loss: 0.20748 \n",
      "Validation loss decreased (0.208559 --> 0.193506).  Saving model ...\n",
      "[  7/200] train_loss: 0.17897 valid_loss: 0.18301 test_loss: 0.19660 \n",
      "Validation loss decreased (0.193506 --> 0.183006).  Saving model ...\n",
      "[  8/200] train_loss: 0.16902 valid_loss: 0.17242 test_loss: 0.18638 \n",
      "Validation loss decreased (0.183006 --> 0.172422).  Saving model ...\n",
      "[  9/200] train_loss: 0.16210 valid_loss: 0.16808 test_loss: 0.17963 \n",
      "Validation loss decreased (0.172422 --> 0.168076).  Saving model ...\n",
      "[ 10/200] train_loss: 0.15799 valid_loss: 0.16094 test_loss: 0.17649 \n",
      "Validation loss decreased (0.168076 --> 0.160936).  Saving model ...\n",
      "[ 11/200] train_loss: 0.15045 valid_loss: 0.16113 test_loss: 0.17336 \n",
      "[ 12/200] train_loss: 0.14731 valid_loss: 0.15695 test_loss: 0.16832 \n",
      "Validation loss decreased (0.160936 --> 0.156954).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14536 valid_loss: 0.15260 test_loss: 0.16521 \n",
      "Validation loss decreased (0.156954 --> 0.152601).  Saving model ...\n",
      "[ 14/200] train_loss: 0.13702 valid_loss: 0.15139 test_loss: 0.16455 \n",
      "Validation loss decreased (0.152601 --> 0.151389).  Saving model ...\n",
      "[ 15/200] train_loss: 0.13826 valid_loss: 0.15375 test_loss: 0.16220 \n",
      "[ 16/200] train_loss: 0.13958 valid_loss: 0.14423 test_loss: 0.15784 \n",
      "Validation loss decreased (0.151389 --> 0.144233).  Saving model ...\n",
      "[ 17/200] train_loss: 0.13527 valid_loss: 0.14690 test_loss: 0.16036 \n",
      "[ 18/200] train_loss: 0.13366 valid_loss: 0.14480 test_loss: 0.15676 \n",
      "[ 19/200] train_loss: 0.13039 valid_loss: 0.13633 test_loss: 0.15290 \n",
      "Validation loss decreased (0.144233 --> 0.136333).  Saving model ...\n",
      "[ 20/200] train_loss: 0.13279 valid_loss: 0.13901 test_loss: 0.15215 \n",
      "[ 21/200] train_loss: 0.12618 valid_loss: 0.14401 test_loss: 0.15278 \n",
      "[ 22/200] train_loss: 0.12623 valid_loss: 0.13964 test_loss: 0.15194 \n",
      "[ 23/200] train_loss: 0.12516 valid_loss: 0.13598 test_loss: 0.14944 \n",
      "Validation loss decreased (0.136333 --> 0.135975).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12489 valid_loss: 0.13423 test_loss: 0.14853 \n",
      "Validation loss decreased (0.135975 --> 0.134229).  Saving model ...\n",
      "[ 25/200] train_loss: 0.12282 valid_loss: 0.13217 test_loss: 0.14712 \n",
      "Validation loss decreased (0.134229 --> 0.132175).  Saving model ...\n",
      "[ 26/200] train_loss: 0.12440 valid_loss: 0.13421 test_loss: 0.14746 \n",
      "[ 27/200] train_loss: 0.11799 valid_loss: 0.13190 test_loss: 0.14844 \n",
      "Validation loss decreased (0.132175 --> 0.131903).  Saving model ...\n",
      "[ 28/200] train_loss: 0.12002 valid_loss: 0.12934 test_loss: 0.14310 \n",
      "Validation loss decreased (0.131903 --> 0.129342).  Saving model ...\n",
      "[ 29/200] train_loss: 0.12147 valid_loss: 0.12964 test_loss: 0.14612 \n",
      "[ 30/200] train_loss: 0.11787 valid_loss: 0.12792 test_loss: 0.14319 \n",
      "Validation loss decreased (0.129342 --> 0.127925).  Saving model ...\n",
      "[ 31/200] train_loss: 0.11639 valid_loss: 0.12904 test_loss: 0.14225 \n",
      "[ 32/200] train_loss: 0.11809 valid_loss: 0.12885 test_loss: 0.14327 \n",
      "[ 33/200] train_loss: 0.11598 valid_loss: 0.12940 test_loss: 0.14206 \n",
      "[ 34/200] train_loss: 0.11228 valid_loss: 0.12211 test_loss: 0.13914 \n",
      "Validation loss decreased (0.127925 --> 0.122108).  Saving model ...\n",
      "[ 35/200] train_loss: 0.11346 valid_loss: 0.12075 test_loss: 0.13791 \n",
      "Validation loss decreased (0.122108 --> 0.120749).  Saving model ...\n",
      "[ 36/200] train_loss: 0.11280 valid_loss: 0.12679 test_loss: 0.14134 \n",
      "[ 37/200] train_loss: 0.11415 valid_loss: 0.12955 test_loss: 0.14103 \n",
      "[ 38/200] train_loss: 0.11388 valid_loss: 0.12074 test_loss: 0.13865 \n",
      "Validation loss decreased (0.120749 --> 0.120743).  Saving model ...\n",
      "[ 39/200] train_loss: 0.11072 valid_loss: 0.12318 test_loss: 0.13762 \n",
      "[ 40/200] train_loss: 0.11089 valid_loss: 0.12069 test_loss: 0.13551 \n",
      "Validation loss decreased (0.120743 --> 0.120691).  Saving model ...\n",
      "[ 41/200] train_loss: 0.11100 valid_loss: 0.11842 test_loss: 0.13571 \n",
      "Validation loss decreased (0.120691 --> 0.118416).  Saving model ...\n",
      "[ 42/200] train_loss: 0.11265 valid_loss: 0.12055 test_loss: 0.13361 \n",
      "[ 43/200] train_loss: 0.10837 valid_loss: 0.11974 test_loss: 0.13592 \n",
      "[ 44/200] train_loss: 0.10787 valid_loss: 0.11640 test_loss: 0.13481 \n",
      "Validation loss decreased (0.118416 --> 0.116395).  Saving model ...\n",
      "[ 45/200] train_loss: 0.10759 valid_loss: 0.12448 test_loss: 0.13498 \n",
      "[ 46/200] train_loss: 0.10638 valid_loss: 0.12051 test_loss: 0.13252 \n",
      "[ 47/200] train_loss: 0.10774 valid_loss: 0.11786 test_loss: 0.13332 \n",
      "[ 48/200] train_loss: 0.10710 valid_loss: 0.11847 test_loss: 0.13235 \n",
      "[ 49/200] train_loss: 0.10410 valid_loss: 0.11402 test_loss: 0.13221 \n",
      "Validation loss decreased (0.116395 --> 0.114019).  Saving model ...\n",
      "[ 50/200] train_loss: 0.10818 valid_loss: 0.11081 test_loss: 0.12946 \n",
      "Validation loss decreased (0.114019 --> 0.110813).  Saving model ...\n",
      "[ 51/200] train_loss: 0.10420 valid_loss: 0.11583 test_loss: 0.12980 \n",
      "[ 52/200] train_loss: 0.10360 valid_loss: 0.11441 test_loss: 0.12998 \n",
      "[ 53/200] train_loss: 0.10441 valid_loss: 0.11440 test_loss: 0.13149 \n",
      "[ 54/200] train_loss: 0.10192 valid_loss: 0.11741 test_loss: 0.13057 \n",
      "[ 55/200] train_loss: 0.10497 valid_loss: 0.11550 test_loss: 0.12926 \n",
      "[ 56/200] train_loss: 0.10312 valid_loss: 0.11386 test_loss: 0.12600 \n",
      "[ 57/200] train_loss: 0.10260 valid_loss: 0.11076 test_loss: 0.12679 \n",
      "Validation loss decreased (0.110813 --> 0.110765).  Saving model ...\n",
      "[ 58/200] train_loss: 0.10008 valid_loss: 0.11240 test_loss: 0.12522 \n",
      "[ 59/200] train_loss: 0.10076 valid_loss: 0.11225 test_loss: 0.12528 \n",
      "[ 60/200] train_loss: 0.10154 valid_loss: 0.10988 test_loss: 0.12332 \n",
      "Validation loss decreased (0.110765 --> 0.109885).  Saving model ...\n",
      "[ 61/200] train_loss: 0.10176 valid_loss: 0.11693 test_loss: 0.12831 \n",
      "[ 62/200] train_loss: 0.10023 valid_loss: 0.10982 test_loss: 0.12465 \n",
      "Validation loss decreased (0.109885 --> 0.109821).  Saving model ...\n",
      "[ 63/200] train_loss: 0.09999 valid_loss: 0.11543 test_loss: 0.12303 \n",
      "[ 64/200] train_loss: 0.09892 valid_loss: 0.11487 test_loss: 0.12414 \n",
      "[ 65/200] train_loss: 0.10028 valid_loss: 0.10757 test_loss: 0.12334 \n",
      "Validation loss decreased (0.109821 --> 0.107572).  Saving model ...\n",
      "[ 66/200] train_loss: 0.09652 valid_loss: 0.11264 test_loss: 0.12221 \n",
      "[ 67/200] train_loss: 0.09865 valid_loss: 0.10818 test_loss: 0.12390 \n",
      "[ 68/200] train_loss: 0.09659 valid_loss: 0.10767 test_loss: 0.12204 \n",
      "[ 69/200] train_loss: 0.09697 valid_loss: 0.10753 test_loss: 0.12269 \n",
      "Validation loss decreased (0.107572 --> 0.107529).  Saving model ...\n",
      "[ 70/200] train_loss: 0.09932 valid_loss: 0.10548 test_loss: 0.12086 \n",
      "Validation loss decreased (0.107529 --> 0.105482).  Saving model ...\n",
      "[ 71/200] train_loss: 0.09791 valid_loss: 0.10970 test_loss: 0.11991 \n",
      "[ 72/200] train_loss: 0.09936 valid_loss: 0.10534 test_loss: 0.11976 \n",
      "Validation loss decreased (0.105482 --> 0.105335).  Saving model ...\n",
      "[ 73/200] train_loss: 0.09557 valid_loss: 0.10532 test_loss: 0.12135 \n",
      "Validation loss decreased (0.105335 --> 0.105321).  Saving model ...\n",
      "[ 74/200] train_loss: 0.09631 valid_loss: 0.10422 test_loss: 0.11899 \n",
      "Validation loss decreased (0.105321 --> 0.104218).  Saving model ...\n",
      "[ 75/200] train_loss: 0.09716 valid_loss: 0.10967 test_loss: 0.12035 \n",
      "[ 76/200] train_loss: 0.09715 valid_loss: 0.10485 test_loss: 0.11884 \n",
      "[ 77/200] train_loss: 0.09650 valid_loss: 0.10268 test_loss: 0.11842 \n",
      "Validation loss decreased (0.104218 --> 0.102679).  Saving model ...\n",
      "[ 78/200] train_loss: 0.09438 valid_loss: 0.10537 test_loss: 0.11715 \n",
      "[ 79/200] train_loss: 0.09365 valid_loss: 0.10484 test_loss: 0.11732 \n",
      "[ 80/200] train_loss: 0.09511 valid_loss: 0.10162 test_loss: 0.11841 \n",
      "Validation loss decreased (0.102679 --> 0.101617).  Saving model ...\n",
      "[ 81/200] train_loss: 0.09435 valid_loss: 0.10484 test_loss: 0.12153 \n",
      "[ 82/200] train_loss: 0.09076 valid_loss: 0.10247 test_loss: 0.11828 \n",
      "[ 83/200] train_loss: 0.09309 valid_loss: 0.10676 test_loss: 0.11724 \n",
      "[ 84/200] train_loss: 0.09241 valid_loss: 0.10267 test_loss: 0.11645 \n",
      "[ 85/200] train_loss: 0.09055 valid_loss: 0.10008 test_loss: 0.11668 \n",
      "Validation loss decreased (0.101617 --> 0.100078).  Saving model ...\n",
      "[ 86/200] train_loss: 0.09228 valid_loss: 0.10234 test_loss: 0.11707 \n",
      "[ 87/200] train_loss: 0.09388 valid_loss: 0.10164 test_loss: 0.11452 \n",
      "[ 88/200] train_loss: 0.09256 valid_loss: 0.09852 test_loss: 0.11353 \n",
      "Validation loss decreased (0.100078 --> 0.098519).  Saving model ...\n",
      "[ 89/200] train_loss: 0.09215 valid_loss: 0.09941 test_loss: 0.11513 \n",
      "[ 90/200] train_loss: 0.09095 valid_loss: 0.10188 test_loss: 0.11714 \n",
      "[ 91/200] train_loss: 0.09325 valid_loss: 0.10256 test_loss: 0.11701 \n",
      "[ 92/200] train_loss: 0.09166 valid_loss: 0.10166 test_loss: 0.11348 \n",
      "[ 93/200] train_loss: 0.09014 valid_loss: 0.10571 test_loss: 0.11255 \n",
      "[ 94/200] train_loss: 0.09298 valid_loss: 0.10415 test_loss: 0.11520 \n",
      "[ 95/200] train_loss: 0.09036 valid_loss: 0.10494 test_loss: 0.11325 \n",
      "[ 96/200] train_loss: 0.09463 valid_loss: 0.10312 test_loss: 0.11255 \n",
      "[ 97/200] train_loss: 0.09140 valid_loss: 0.10273 test_loss: 0.11158 \n",
      "[ 98/200] train_loss: 0.08984 valid_loss: 0.10253 test_loss: 0.11345 \n",
      "[ 99/200] train_loss: 0.08953 valid_loss: 0.10447 test_loss: 0.11188 \n",
      "[100/200] train_loss: 0.09139 valid_loss: 0.09795 test_loss: 0.11182 \n",
      "Validation loss decreased (0.098519 --> 0.097946).  Saving model ...\n",
      "[101/200] train_loss: 0.09017 valid_loss: 0.09880 test_loss: 0.11003 \n",
      "[102/200] train_loss: 0.08608 valid_loss: 0.09951 test_loss: 0.11094 \n",
      "[103/200] train_loss: 0.08845 valid_loss: 0.10023 test_loss: 0.10987 \n",
      "[104/200] train_loss: 0.08765 valid_loss: 0.10069 test_loss: 0.11573 \n",
      "[105/200] train_loss: 0.08796 valid_loss: 0.10880 test_loss: 0.11100 \n",
      "[106/200] train_loss: 0.09047 valid_loss: 0.09709 test_loss: 0.11036 \n",
      "Validation loss decreased (0.097946 --> 0.097086).  Saving model ...\n",
      "[107/200] train_loss: 0.08729 valid_loss: 0.09687 test_loss: 0.11164 \n",
      "Validation loss decreased (0.097086 --> 0.096874).  Saving model ...\n",
      "[108/200] train_loss: 0.08698 valid_loss: 0.10016 test_loss: 0.10938 \n",
      "[109/200] train_loss: 0.08737 valid_loss: 0.09664 test_loss: 0.11091 \n",
      "Validation loss decreased (0.096874 --> 0.096641).  Saving model ...\n",
      "[110/200] train_loss: 0.08810 valid_loss: 0.09661 test_loss: 0.11145 \n",
      "Validation loss decreased (0.096641 --> 0.096612).  Saving model ...\n",
      "[111/200] train_loss: 0.08721 valid_loss: 0.09715 test_loss: 0.10864 \n",
      "[112/200] train_loss: 0.08936 valid_loss: 0.10149 test_loss: 0.10947 \n",
      "[113/200] train_loss: 0.08590 valid_loss: 0.09914 test_loss: 0.11103 \n",
      "[114/200] train_loss: 0.08711 valid_loss: 0.09675 test_loss: 0.10936 \n",
      "[115/200] train_loss: 0.08549 valid_loss: 0.10229 test_loss: 0.11016 \n",
      "[116/200] train_loss: 0.08597 valid_loss: 0.09966 test_loss: 0.10971 \n",
      "[117/200] train_loss: 0.08469 valid_loss: 0.09397 test_loss: 0.10856 \n",
      "Validation loss decreased (0.096612 --> 0.093975).  Saving model ...\n",
      "[118/200] train_loss: 0.08472 valid_loss: 0.09555 test_loss: 0.11146 \n",
      "[119/200] train_loss: 0.08853 valid_loss: 0.09704 test_loss: 0.11076 \n",
      "[120/200] train_loss: 0.08422 valid_loss: 0.09589 test_loss: 0.10915 \n",
      "[121/200] train_loss: 0.08551 valid_loss: 0.09616 test_loss: 0.10931 \n",
      "[122/200] train_loss: 0.08545 valid_loss: 0.09683 test_loss: 0.10894 \n",
      "[123/200] train_loss: 0.08680 valid_loss: 0.09389 test_loss: 0.10798 \n",
      "Validation loss decreased (0.093975 --> 0.093894).  Saving model ...\n",
      "[124/200] train_loss: 0.08721 valid_loss: 0.09311 test_loss: 0.10760 \n",
      "Validation loss decreased (0.093894 --> 0.093114).  Saving model ...\n",
      "[125/200] train_loss: 0.08376 valid_loss: 0.09466 test_loss: 0.10725 \n",
      "[126/200] train_loss: 0.08653 valid_loss: 0.09436 test_loss: 0.10840 \n",
      "[127/200] train_loss: 0.08493 valid_loss: 0.09332 test_loss: 0.10582 \n",
      "[128/200] train_loss: 0.08426 valid_loss: 0.09290 test_loss: 0.10612 \n",
      "Validation loss decreased (0.093114 --> 0.092900).  Saving model ...\n",
      "[129/200] train_loss: 0.08444 valid_loss: 0.09616 test_loss: 0.10641 \n",
      "[130/200] train_loss: 0.08388 valid_loss: 0.09667 test_loss: 0.10936 \n",
      "[131/200] train_loss: 0.08272 valid_loss: 0.09629 test_loss: 0.10730 \n",
      "[132/200] train_loss: 0.08667 valid_loss: 0.09267 test_loss: 0.10617 \n",
      "Validation loss decreased (0.092900 --> 0.092667).  Saving model ...\n",
      "[133/200] train_loss: 0.08263 valid_loss: 0.09427 test_loss: 0.10566 \n",
      "[134/200] train_loss: 0.08425 valid_loss: 0.09381 test_loss: 0.10739 \n",
      "[135/200] train_loss: 0.08144 valid_loss: 0.09353 test_loss: 0.10635 \n",
      "[136/200] train_loss: 0.08354 valid_loss: 0.09711 test_loss: 0.10654 \n",
      "[137/200] train_loss: 0.08559 valid_loss: 0.09556 test_loss: 0.10806 \n",
      "[138/200] train_loss: 0.08421 valid_loss: 0.09566 test_loss: 0.10549 \n",
      "[139/200] train_loss: 0.08382 valid_loss: 0.09232 test_loss: 0.10500 \n",
      "Validation loss decreased (0.092667 --> 0.092320).  Saving model ...\n",
      "[140/200] train_loss: 0.07783 valid_loss: 0.09110 test_loss: 0.10506 \n",
      "Validation loss decreased (0.092320 --> 0.091098).  Saving model ...\n",
      "[141/200] train_loss: 0.08390 valid_loss: 0.10085 test_loss: 0.10622 \n",
      "[142/200] train_loss: 0.08303 valid_loss: 0.09137 test_loss: 0.10471 \n",
      "[143/200] train_loss: 0.08459 valid_loss: 0.09241 test_loss: 0.10541 \n",
      "[144/200] train_loss: 0.08504 valid_loss: 0.09419 test_loss: 0.10529 \n",
      "[145/200] train_loss: 0.08212 valid_loss: 0.09252 test_loss: 0.10468 \n",
      "[146/200] train_loss: 0.08169 valid_loss: 0.09226 test_loss: 0.10557 \n",
      "[147/200] train_loss: 0.07844 valid_loss: 0.09327 test_loss: 0.10395 \n",
      "[148/200] train_loss: 0.08156 valid_loss: 0.09207 test_loss: 0.10470 \n",
      "[149/200] train_loss: 0.08265 valid_loss: 0.09358 test_loss: 0.10383 \n",
      "[150/200] train_loss: 0.08163 valid_loss: 0.09220 test_loss: 0.10440 \n",
      "[151/200] train_loss: 0.08361 valid_loss: 0.09088 test_loss: 0.10443 \n",
      "Validation loss decreased (0.091098 --> 0.090879).  Saving model ...\n",
      "[152/200] train_loss: 0.08237 valid_loss: 0.09588 test_loss: 0.10470 \n",
      "[153/200] train_loss: 0.08187 valid_loss: 0.09092 test_loss: 0.10241 \n",
      "[154/200] train_loss: 0.07906 valid_loss: 0.09201 test_loss: 0.10345 \n",
      "[155/200] train_loss: 0.07977 valid_loss: 0.09723 test_loss: 0.10384 \n",
      "[156/200] train_loss: 0.08031 valid_loss: 0.09520 test_loss: 0.10297 \n",
      "[157/200] train_loss: 0.07980 valid_loss: 0.09272 test_loss: 0.10245 \n",
      "[158/200] train_loss: 0.08385 valid_loss: 0.09190 test_loss: 0.10219 \n",
      "[159/200] train_loss: 0.07953 valid_loss: 0.09231 test_loss: 0.10375 \n",
      "[160/200] train_loss: 0.08022 valid_loss: 0.09027 test_loss: 0.10240 \n",
      "Validation loss decreased (0.090879 --> 0.090271).  Saving model ...\n",
      "[161/200] train_loss: 0.07911 valid_loss: 0.09062 test_loss: 0.10233 \n",
      "[162/200] train_loss: 0.07900 valid_loss: 0.08927 test_loss: 0.10267 \n",
      "Validation loss decreased (0.090271 --> 0.089265).  Saving model ...\n",
      "[163/200] train_loss: 0.08416 valid_loss: 0.08963 test_loss: 0.10237 \n",
      "[164/200] train_loss: 0.07989 valid_loss: 0.09008 test_loss: 0.10261 \n",
      "[165/200] train_loss: 0.07847 valid_loss: 0.09136 test_loss: 0.10227 \n",
      "[166/200] train_loss: 0.07947 valid_loss: 0.08952 test_loss: 0.10233 \n",
      "[167/200] train_loss: 0.07691 valid_loss: 0.09702 test_loss: 0.10225 \n",
      "[168/200] train_loss: 0.07974 valid_loss: 0.08968 test_loss: 0.10241 \n",
      "[169/200] train_loss: 0.07782 valid_loss: 0.09013 test_loss: 0.10347 \n",
      "[170/200] train_loss: 0.08154 valid_loss: 0.08882 test_loss: 0.10039 \n",
      "Validation loss decreased (0.089265 --> 0.088824).  Saving model ...\n",
      "[171/200] train_loss: 0.07856 valid_loss: 0.09122 test_loss: 0.10169 \n",
      "[172/200] train_loss: 0.07776 valid_loss: 0.09026 test_loss: 0.10325 \n",
      "[173/200] train_loss: 0.08048 valid_loss: 0.09100 test_loss: 0.10302 \n",
      "[174/200] train_loss: 0.08121 valid_loss: 0.09328 test_loss: 0.10152 \n",
      "[175/200] train_loss: 0.08105 valid_loss: 0.09109 test_loss: 0.10108 \n",
      "[176/200] train_loss: 0.07685 valid_loss: 0.08955 test_loss: 0.10037 \n",
      "[177/200] train_loss: 0.07900 valid_loss: 0.08891 test_loss: 0.10091 \n",
      "[178/200] train_loss: 0.07523 valid_loss: 0.08996 test_loss: 0.10165 \n",
      "[179/200] train_loss: 0.07634 valid_loss: 0.08716 test_loss: 0.10011 \n",
      "Validation loss decreased (0.088824 --> 0.087164).  Saving model ...\n",
      "[180/200] train_loss: 0.08049 valid_loss: 0.08858 test_loss: 0.10014 \n",
      "[181/200] train_loss: 0.07804 valid_loss: 0.08929 test_loss: 0.10103 \n",
      "[182/200] train_loss: 0.07841 valid_loss: 0.08969 test_loss: 0.10077 \n",
      "[183/200] train_loss: 0.07680 valid_loss: 0.09127 test_loss: 0.10117 \n",
      "[184/200] train_loss: 0.07928 valid_loss: 0.08746 test_loss: 0.09910 \n",
      "[185/200] train_loss: 0.07677 valid_loss: 0.08843 test_loss: 0.10045 \n",
      "[186/200] train_loss: 0.07773 valid_loss: 0.08738 test_loss: 0.09995 \n",
      "[187/200] train_loss: 0.07805 valid_loss: 0.08869 test_loss: 0.10095 \n",
      "[188/200] train_loss: 0.07784 valid_loss: 0.08819 test_loss: 0.09951 \n",
      "[189/200] train_loss: 0.07658 valid_loss: 0.08748 test_loss: 0.10039 \n",
      "[190/200] train_loss: 0.07714 valid_loss: 0.09041 test_loss: 0.10097 \n",
      "[191/200] train_loss: 0.07626 valid_loss: 0.08885 test_loss: 0.10035 \n",
      "[192/200] train_loss: 0.07770 valid_loss: 0.08715 test_loss: 0.09938 \n",
      "Validation loss decreased (0.087164 --> 0.087149).  Saving model ...\n",
      "[193/200] train_loss: 0.07566 valid_loss: 0.08841 test_loss: 0.10061 \n",
      "[194/200] train_loss: 0.07447 valid_loss: 0.08858 test_loss: 0.09999 \n",
      "[195/200] train_loss: 0.07984 valid_loss: 0.08614 test_loss: 0.09858 \n",
      "Validation loss decreased (0.087149 --> 0.086140).  Saving model ...\n",
      "[196/200] train_loss: 0.07653 valid_loss: 0.09015 test_loss: 0.09908 \n",
      "[197/200] train_loss: 0.07487 valid_loss: 0.08663 test_loss: 0.09831 \n",
      "[198/200] train_loss: 0.07677 valid_loss: 0.08864 test_loss: 0.09988 \n",
      "[199/200] train_loss: 0.07434 valid_loss: 0.08632 test_loss: 0.09803 \n",
      "[200/200] train_loss: 0.07407 valid_loss: 0.08742 test_loss: 0.09798 \n",
      "Model 2 trained for seen data.\n",
      "TRAINING MODEL for seen house 3\n",
      "[  1/200] train_loss: 0.49769 valid_loss: 0.39894 test_loss: 0.40445 \n",
      "Validation loss decreased (inf --> 0.398935).  Saving model ...\n",
      "[  2/200] train_loss: 0.33146 valid_loss: 0.31550 test_loss: 0.32636 \n",
      "Validation loss decreased (0.398935 --> 0.315501).  Saving model ...\n",
      "[  3/200] train_loss: 0.26729 valid_loss: 0.26484 test_loss: 0.28357 \n",
      "Validation loss decreased (0.315501 --> 0.264836).  Saving model ...\n",
      "[  4/200] train_loss: 0.23042 valid_loss: 0.22724 test_loss: 0.24445 \n",
      "Validation loss decreased (0.264836 --> 0.227242).  Saving model ...\n",
      "[  5/200] train_loss: 0.20256 valid_loss: 0.20428 test_loss: 0.22118 \n",
      "Validation loss decreased (0.227242 --> 0.204283).  Saving model ...\n",
      "[  6/200] train_loss: 0.18518 valid_loss: 0.18691 test_loss: 0.20135 \n",
      "Validation loss decreased (0.204283 --> 0.186914).  Saving model ...\n",
      "[  7/200] train_loss: 0.17593 valid_loss: 0.17844 test_loss: 0.19189 \n",
      "Validation loss decreased (0.186914 --> 0.178439).  Saving model ...\n",
      "[  8/200] train_loss: 0.16421 valid_loss: 0.16835 test_loss: 0.18095 \n",
      "Validation loss decreased (0.178439 --> 0.168347).  Saving model ...\n",
      "[  9/200] train_loss: 0.15607 valid_loss: 0.16222 test_loss: 0.17477 \n",
      "Validation loss decreased (0.168347 --> 0.162215).  Saving model ...\n",
      "[ 10/200] train_loss: 0.15458 valid_loss: 0.15893 test_loss: 0.17103 \n",
      "Validation loss decreased (0.162215 --> 0.158933).  Saving model ...\n",
      "[ 11/200] train_loss: 0.14704 valid_loss: 0.15598 test_loss: 0.16746 \n",
      "Validation loss decreased (0.158933 --> 0.155979).  Saving model ...\n",
      "[ 12/200] train_loss: 0.14250 valid_loss: 0.15099 test_loss: 0.16209 \n",
      "Validation loss decreased (0.155979 --> 0.150995).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14073 valid_loss: 0.14778 test_loss: 0.16087 \n",
      "Validation loss decreased (0.150995 --> 0.147778).  Saving model ...\n",
      "[ 14/200] train_loss: 0.13861 valid_loss: 0.14431 test_loss: 0.15582 \n",
      "Validation loss decreased (0.147778 --> 0.144313).  Saving model ...\n",
      "[ 15/200] train_loss: 0.13594 valid_loss: 0.14341 test_loss: 0.15667 \n",
      "Validation loss decreased (0.144313 --> 0.143408).  Saving model ...\n",
      "[ 16/200] train_loss: 0.13338 valid_loss: 0.13859 test_loss: 0.15111 \n",
      "Validation loss decreased (0.143408 --> 0.138589).  Saving model ...\n",
      "[ 17/200] train_loss: 0.13114 valid_loss: 0.14284 test_loss: 0.15250 \n",
      "[ 18/200] train_loss: 0.12822 valid_loss: 0.14060 test_loss: 0.15002 \n",
      "[ 19/200] train_loss: 0.12936 valid_loss: 0.13400 test_loss: 0.14767 \n",
      "Validation loss decreased (0.138589 --> 0.133997).  Saving model ...\n",
      "[ 20/200] train_loss: 0.12772 valid_loss: 0.13414 test_loss: 0.14685 \n",
      "[ 21/200] train_loss: 0.12732 valid_loss: 0.13180 test_loss: 0.14684 \n",
      "Validation loss decreased (0.133997 --> 0.131800).  Saving model ...\n",
      "[ 22/200] train_loss: 0.12280 valid_loss: 0.12976 test_loss: 0.14361 \n",
      "Validation loss decreased (0.131800 --> 0.129757).  Saving model ...\n",
      "[ 23/200] train_loss: 0.12194 valid_loss: 0.12926 test_loss: 0.14370 \n",
      "Validation loss decreased (0.129757 --> 0.129264).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12526 valid_loss: 0.12633 test_loss: 0.14087 \n",
      "Validation loss decreased (0.129264 --> 0.126335).  Saving model ...\n",
      "[ 25/200] train_loss: 0.11955 valid_loss: 0.12494 test_loss: 0.14055 \n",
      "Validation loss decreased (0.126335 --> 0.124943).  Saving model ...\n",
      "[ 26/200] train_loss: 0.11419 valid_loss: 0.12576 test_loss: 0.13986 \n",
      "[ 27/200] train_loss: 0.12126 valid_loss: 0.12370 test_loss: 0.13790 \n",
      "Validation loss decreased (0.124943 --> 0.123699).  Saving model ...\n",
      "[ 28/200] train_loss: 0.12079 valid_loss: 0.12734 test_loss: 0.13920 \n",
      "[ 29/200] train_loss: 0.11756 valid_loss: 0.12433 test_loss: 0.13900 \n",
      "[ 30/200] train_loss: 0.11593 valid_loss: 0.12563 test_loss: 0.13566 \n",
      "[ 31/200] train_loss: 0.11412 valid_loss: 0.12379 test_loss: 0.13635 \n",
      "[ 32/200] train_loss: 0.11613 valid_loss: 0.12389 test_loss: 0.13640 \n",
      "[ 33/200] train_loss: 0.11647 valid_loss: 0.12119 test_loss: 0.13505 \n",
      "Validation loss decreased (0.123699 --> 0.121194).  Saving model ...\n",
      "[ 34/200] train_loss: 0.11177 valid_loss: 0.12307 test_loss: 0.13482 \n",
      "[ 35/200] train_loss: 0.10888 valid_loss: 0.12386 test_loss: 0.13330 \n",
      "[ 36/200] train_loss: 0.10929 valid_loss: 0.12114 test_loss: 0.13213 \n",
      "Validation loss decreased (0.121194 --> 0.121135).  Saving model ...\n",
      "[ 37/200] train_loss: 0.11041 valid_loss: 0.11678 test_loss: 0.13109 \n",
      "Validation loss decreased (0.121135 --> 0.116777).  Saving model ...\n",
      "[ 38/200] train_loss: 0.10586 valid_loss: 0.11619 test_loss: 0.13045 \n",
      "Validation loss decreased (0.116777 --> 0.116192).  Saving model ...\n",
      "[ 39/200] train_loss: 0.10969 valid_loss: 0.12016 test_loss: 0.13258 \n",
      "[ 40/200] train_loss: 0.11148 valid_loss: 0.11926 test_loss: 0.13056 \n",
      "[ 41/200] train_loss: 0.10797 valid_loss: 0.11453 test_loss: 0.12850 \n",
      "Validation loss decreased (0.116192 --> 0.114533).  Saving model ...\n",
      "[ 42/200] train_loss: 0.11089 valid_loss: 0.11655 test_loss: 0.13007 \n",
      "[ 43/200] train_loss: 0.10985 valid_loss: 0.11590 test_loss: 0.12953 \n",
      "[ 44/200] train_loss: 0.10876 valid_loss: 0.11440 test_loss: 0.13036 \n",
      "Validation loss decreased (0.114533 --> 0.114403).  Saving model ...\n",
      "[ 45/200] train_loss: 0.10781 valid_loss: 0.11332 test_loss: 0.12771 \n",
      "Validation loss decreased (0.114403 --> 0.113325).  Saving model ...\n",
      "[ 46/200] train_loss: 0.10550 valid_loss: 0.11254 test_loss: 0.12481 \n",
      "Validation loss decreased (0.113325 --> 0.112536).  Saving model ...\n",
      "[ 47/200] train_loss: 0.10463 valid_loss: 0.11128 test_loss: 0.12655 \n",
      "Validation loss decreased (0.112536 --> 0.111277).  Saving model ...\n",
      "[ 48/200] train_loss: 0.10514 valid_loss: 0.11437 test_loss: 0.12519 \n",
      "[ 49/200] train_loss: 0.10645 valid_loss: 0.11298 test_loss: 0.12786 \n",
      "[ 50/200] train_loss: 0.10660 valid_loss: 0.11349 test_loss: 0.12596 \n",
      "[ 51/200] train_loss: 0.10326 valid_loss: 0.11127 test_loss: 0.12392 \n",
      "Validation loss decreased (0.111277 --> 0.111267).  Saving model ...\n",
      "[ 52/200] train_loss: 0.10282 valid_loss: 0.11070 test_loss: 0.12724 \n",
      "Validation loss decreased (0.111267 --> 0.110699).  Saving model ...\n",
      "[ 53/200] train_loss: 0.10137 valid_loss: 0.11190 test_loss: 0.12493 \n",
      "[ 54/200] train_loss: 0.09972 valid_loss: 0.11333 test_loss: 0.12509 \n",
      "[ 55/200] train_loss: 0.10091 valid_loss: 0.10999 test_loss: 0.12368 \n",
      "Validation loss decreased (0.110699 --> 0.109992).  Saving model ...\n",
      "[ 56/200] train_loss: 0.10189 valid_loss: 0.11189 test_loss: 0.12414 \n",
      "[ 57/200] train_loss: 0.10194 valid_loss: 0.11018 test_loss: 0.12261 \n",
      "[ 58/200] train_loss: 0.09830 valid_loss: 0.11081 test_loss: 0.12125 \n",
      "[ 59/200] train_loss: 0.09936 valid_loss: 0.11078 test_loss: 0.12525 \n",
      "[ 60/200] train_loss: 0.09741 valid_loss: 0.11150 test_loss: 0.12308 \n",
      "[ 61/200] train_loss: 0.09924 valid_loss: 0.11009 test_loss: 0.12052 \n",
      "[ 62/200] train_loss: 0.09996 valid_loss: 0.10904 test_loss: 0.11995 \n",
      "Validation loss decreased (0.109992 --> 0.109038).  Saving model ...\n",
      "[ 63/200] train_loss: 0.09856 valid_loss: 0.11082 test_loss: 0.12040 \n",
      "[ 64/200] train_loss: 0.09825 valid_loss: 0.10941 test_loss: 0.12026 \n",
      "[ 65/200] train_loss: 0.09751 valid_loss: 0.11093 test_loss: 0.11898 \n",
      "[ 66/200] train_loss: 0.09936 valid_loss: 0.11086 test_loss: 0.12191 \n",
      "[ 67/200] train_loss: 0.09677 valid_loss: 0.11239 test_loss: 0.12134 \n",
      "[ 68/200] train_loss: 0.09564 valid_loss: 0.10933 test_loss: 0.11938 \n",
      "[ 69/200] train_loss: 0.09554 valid_loss: 0.10700 test_loss: 0.11891 \n",
      "Validation loss decreased (0.109038 --> 0.107002).  Saving model ...\n",
      "[ 70/200] train_loss: 0.09565 valid_loss: 0.10454 test_loss: 0.11662 \n",
      "Validation loss decreased (0.107002 --> 0.104539).  Saving model ...\n",
      "[ 71/200] train_loss: 0.09713 valid_loss: 0.11037 test_loss: 0.11985 \n",
      "[ 72/200] train_loss: 0.09705 valid_loss: 0.10440 test_loss: 0.11693 \n",
      "Validation loss decreased (0.104539 --> 0.104399).  Saving model ...\n",
      "[ 73/200] train_loss: 0.09534 valid_loss: 0.10458 test_loss: 0.11676 \n",
      "[ 74/200] train_loss: 0.09136 valid_loss: 0.10580 test_loss: 0.11644 \n",
      "[ 75/200] train_loss: 0.09814 valid_loss: 0.10566 test_loss: 0.11917 \n",
      "[ 76/200] train_loss: 0.09652 valid_loss: 0.10552 test_loss: 0.11542 \n",
      "[ 77/200] train_loss: 0.09373 valid_loss: 0.10815 test_loss: 0.11649 \n",
      "[ 78/200] train_loss: 0.09081 valid_loss: 0.10813 test_loss: 0.11745 \n",
      "[ 79/200] train_loss: 0.09208 valid_loss: 0.10391 test_loss: 0.11474 \n",
      "Validation loss decreased (0.104399 --> 0.103908).  Saving model ...\n",
      "[ 80/200] train_loss: 0.09170 valid_loss: 0.10947 test_loss: 0.11543 \n",
      "[ 81/200] train_loss: 0.09292 valid_loss: 0.10600 test_loss: 0.11433 \n",
      "[ 82/200] train_loss: 0.09354 valid_loss: 0.10738 test_loss: 0.11635 \n",
      "[ 83/200] train_loss: 0.09331 valid_loss: 0.10493 test_loss: 0.11495 \n",
      "[ 84/200] train_loss: 0.09062 valid_loss: 0.10474 test_loss: 0.11343 \n",
      "[ 85/200] train_loss: 0.09295 valid_loss: 0.10092 test_loss: 0.11366 \n",
      "Validation loss decreased (0.103908 --> 0.100922).  Saving model ...\n",
      "[ 86/200] train_loss: 0.09474 valid_loss: 0.10235 test_loss: 0.11465 \n",
      "[ 87/200] train_loss: 0.09093 valid_loss: 0.10281 test_loss: 0.11404 \n",
      "[ 88/200] train_loss: 0.09183 valid_loss: 0.10162 test_loss: 0.11474 \n",
      "[ 89/200] train_loss: 0.09354 valid_loss: 0.09982 test_loss: 0.11239 \n",
      "Validation loss decreased (0.100922 --> 0.099820).  Saving model ...\n",
      "[ 90/200] train_loss: 0.09315 valid_loss: 0.09891 test_loss: 0.11236 \n",
      "Validation loss decreased (0.099820 --> 0.098907).  Saving model ...\n",
      "[ 91/200] train_loss: 0.09079 valid_loss: 0.10172 test_loss: 0.11175 \n",
      "[ 92/200] train_loss: 0.09064 valid_loss: 0.10030 test_loss: 0.11287 \n",
      "[ 93/200] train_loss: 0.09182 valid_loss: 0.10172 test_loss: 0.11297 \n",
      "[ 94/200] train_loss: 0.09234 valid_loss: 0.10033 test_loss: 0.11154 \n",
      "[ 95/200] train_loss: 0.08954 valid_loss: 0.10298 test_loss: 0.11744 \n",
      "[ 96/200] train_loss: 0.09040 valid_loss: 0.10783 test_loss: 0.11148 \n",
      "[ 97/200] train_loss: 0.08922 valid_loss: 0.10009 test_loss: 0.11267 \n",
      "[ 98/200] train_loss: 0.09081 valid_loss: 0.10235 test_loss: 0.11095 \n",
      "[ 99/200] train_loss: 0.09241 valid_loss: 0.10071 test_loss: 0.11094 \n",
      "[100/200] train_loss: 0.08891 valid_loss: 0.09998 test_loss: 0.11041 \n",
      "[101/200] train_loss: 0.09065 valid_loss: 0.09841 test_loss: 0.11147 \n",
      "Validation loss decreased (0.098907 --> 0.098409).  Saving model ...\n",
      "[102/200] train_loss: 0.08977 valid_loss: 0.10005 test_loss: 0.11053 \n",
      "[103/200] train_loss: 0.08811 valid_loss: 0.09972 test_loss: 0.11092 \n",
      "[104/200] train_loss: 0.08894 valid_loss: 0.09928 test_loss: 0.11033 \n",
      "[105/200] train_loss: 0.08589 valid_loss: 0.10159 test_loss: 0.11207 \n",
      "[106/200] train_loss: 0.08711 valid_loss: 0.09962 test_loss: 0.10972 \n",
      "[107/200] train_loss: 0.08691 valid_loss: 0.09818 test_loss: 0.10942 \n",
      "Validation loss decreased (0.098409 --> 0.098179).  Saving model ...\n",
      "[108/200] train_loss: 0.09057 valid_loss: 0.09814 test_loss: 0.11030 \n",
      "Validation loss decreased (0.098179 --> 0.098144).  Saving model ...\n",
      "[109/200] train_loss: 0.08562 valid_loss: 0.09757 test_loss: 0.10939 \n",
      "Validation loss decreased (0.098144 --> 0.097570).  Saving model ...\n",
      "[110/200] train_loss: 0.08578 valid_loss: 0.09955 test_loss: 0.10857 \n",
      "[111/200] train_loss: 0.08984 valid_loss: 0.09684 test_loss: 0.10893 \n",
      "Validation loss decreased (0.097570 --> 0.096838).  Saving model ...\n",
      "[112/200] train_loss: 0.08640 valid_loss: 0.09666 test_loss: 0.10867 \n",
      "Validation loss decreased (0.096838 --> 0.096655).  Saving model ...\n",
      "[113/200] train_loss: 0.08815 valid_loss: 0.09514 test_loss: 0.10770 \n",
      "Validation loss decreased (0.096655 --> 0.095138).  Saving model ...\n",
      "[114/200] train_loss: 0.08725 valid_loss: 0.09881 test_loss: 0.11172 \n",
      "[115/200] train_loss: 0.08776 valid_loss: 0.09875 test_loss: 0.10972 \n",
      "[116/200] train_loss: 0.08567 valid_loss: 0.09766 test_loss: 0.11091 \n",
      "[117/200] train_loss: 0.08534 valid_loss: 0.09601 test_loss: 0.10782 \n",
      "[118/200] train_loss: 0.08549 valid_loss: 0.09490 test_loss: 0.10798 \n",
      "Validation loss decreased (0.095138 --> 0.094900).  Saving model ...\n",
      "[119/200] train_loss: 0.08558 valid_loss: 0.09656 test_loss: 0.10796 \n",
      "[120/200] train_loss: 0.08352 valid_loss: 0.09987 test_loss: 0.10730 \n",
      "[121/200] train_loss: 0.08810 valid_loss: 0.10082 test_loss: 0.10748 \n",
      "[122/200] train_loss: 0.08567 valid_loss: 0.09550 test_loss: 0.10776 \n",
      "[123/200] train_loss: 0.08361 valid_loss: 0.09473 test_loss: 0.10567 \n",
      "Validation loss decreased (0.094900 --> 0.094728).  Saving model ...\n",
      "[124/200] train_loss: 0.08651 valid_loss: 0.09982 test_loss: 0.10751 \n",
      "[125/200] train_loss: 0.08444 valid_loss: 0.09690 test_loss: 0.10769 \n",
      "[126/200] train_loss: 0.08633 valid_loss: 0.09695 test_loss: 0.10932 \n",
      "[127/200] train_loss: 0.08359 valid_loss: 0.09593 test_loss: 0.10772 \n",
      "[128/200] train_loss: 0.08300 valid_loss: 0.09607 test_loss: 0.10744 \n",
      "[129/200] train_loss: 0.08427 valid_loss: 0.09399 test_loss: 0.10658 \n",
      "Validation loss decreased (0.094728 --> 0.093990).  Saving model ...\n",
      "[130/200] train_loss: 0.08284 valid_loss: 0.09555 test_loss: 0.10674 \n",
      "[131/200] train_loss: 0.08320 valid_loss: 0.09401 test_loss: 0.10666 \n",
      "[132/200] train_loss: 0.08743 valid_loss: 0.09403 test_loss: 0.10668 \n",
      "[133/200] train_loss: 0.08175 valid_loss: 0.09352 test_loss: 0.10643 \n",
      "Validation loss decreased (0.093990 --> 0.093525).  Saving model ...\n",
      "[134/200] train_loss: 0.08207 valid_loss: 0.09407 test_loss: 0.10698 \n",
      "[135/200] train_loss: 0.08139 valid_loss: 0.09505 test_loss: 0.10613 \n",
      "[136/200] train_loss: 0.08381 valid_loss: 0.09263 test_loss: 0.10541 \n",
      "Validation loss decreased (0.093525 --> 0.092630).  Saving model ...\n",
      "[137/200] train_loss: 0.08409 valid_loss: 0.09262 test_loss: 0.10506 \n",
      "Validation loss decreased (0.092630 --> 0.092621).  Saving model ...\n",
      "[138/200] train_loss: 0.08089 valid_loss: 0.09685 test_loss: 0.10443 \n",
      "[139/200] train_loss: 0.08210 valid_loss: 0.09233 test_loss: 0.10496 \n",
      "Validation loss decreased (0.092621 --> 0.092329).  Saving model ...\n",
      "[140/200] train_loss: 0.08177 valid_loss: 0.09361 test_loss: 0.10578 \n",
      "[141/200] train_loss: 0.08147 valid_loss: 0.09365 test_loss: 0.10516 \n",
      "[142/200] train_loss: 0.08041 valid_loss: 0.09394 test_loss: 0.10547 \n",
      "[143/200] train_loss: 0.08166 valid_loss: 0.09441 test_loss: 0.10511 \n",
      "[144/200] train_loss: 0.08467 valid_loss: 0.09194 test_loss: 0.10283 \n",
      "Validation loss decreased (0.092329 --> 0.091945).  Saving model ...\n",
      "[145/200] train_loss: 0.08391 valid_loss: 0.09298 test_loss: 0.10369 \n",
      "[146/200] train_loss: 0.08479 valid_loss: 0.09575 test_loss: 0.10530 \n",
      "[147/200] train_loss: 0.07777 valid_loss: 0.09312 test_loss: 0.10484 \n",
      "[148/200] train_loss: 0.08105 valid_loss: 0.09383 test_loss: 0.10619 \n",
      "[149/200] train_loss: 0.08131 valid_loss: 0.09283 test_loss: 0.10345 \n",
      "[150/200] train_loss: 0.07987 valid_loss: 0.09092 test_loss: 0.10353 \n",
      "Validation loss decreased (0.091945 --> 0.090921).  Saving model ...\n",
      "[151/200] train_loss: 0.07945 valid_loss: 0.09516 test_loss: 0.10431 \n",
      "[152/200] train_loss: 0.08037 valid_loss: 0.09344 test_loss: 0.10337 \n",
      "[153/200] train_loss: 0.08180 valid_loss: 0.09090 test_loss: 0.10361 \n",
      "Validation loss decreased (0.090921 --> 0.090899).  Saving model ...\n",
      "[154/200] train_loss: 0.08160 valid_loss: 0.09124 test_loss: 0.10336 \n",
      "[155/200] train_loss: 0.07869 valid_loss: 0.09032 test_loss: 0.10388 \n",
      "Validation loss decreased (0.090899 --> 0.090322).  Saving model ...\n",
      "[156/200] train_loss: 0.08071 valid_loss: 0.09231 test_loss: 0.10427 \n",
      "[157/200] train_loss: 0.08153 valid_loss: 0.09179 test_loss: 0.10317 \n",
      "[158/200] train_loss: 0.08127 valid_loss: 0.09259 test_loss: 0.10449 \n",
      "[159/200] train_loss: 0.07879 valid_loss: 0.09209 test_loss: 0.10475 \n",
      "[160/200] train_loss: 0.07996 valid_loss: 0.09054 test_loss: 0.10399 \n",
      "[161/200] train_loss: 0.07955 valid_loss: 0.09262 test_loss: 0.10271 \n",
      "[162/200] train_loss: 0.07891 valid_loss: 0.08985 test_loss: 0.10226 \n",
      "Validation loss decreased (0.090322 --> 0.089845).  Saving model ...\n",
      "[163/200] train_loss: 0.08056 valid_loss: 0.09294 test_loss: 0.10523 \n",
      "[164/200] train_loss: 0.08010 valid_loss: 0.09168 test_loss: 0.10157 \n",
      "[165/200] train_loss: 0.07942 valid_loss: 0.09344 test_loss: 0.10420 \n",
      "[166/200] train_loss: 0.07850 valid_loss: 0.09359 test_loss: 0.10222 \n",
      "[167/200] train_loss: 0.07772 valid_loss: 0.09161 test_loss: 0.10235 \n",
      "[168/200] train_loss: 0.08072 valid_loss: 0.08991 test_loss: 0.10128 \n",
      "[169/200] train_loss: 0.07962 valid_loss: 0.09014 test_loss: 0.10224 \n",
      "[170/200] train_loss: 0.07919 valid_loss: 0.08991 test_loss: 0.10097 \n",
      "[171/200] train_loss: 0.07842 valid_loss: 0.08883 test_loss: 0.10146 \n",
      "Validation loss decreased (0.089845 --> 0.088828).  Saving model ...\n",
      "[172/200] train_loss: 0.07860 valid_loss: 0.08979 test_loss: 0.10251 \n",
      "[173/200] train_loss: 0.07804 valid_loss: 0.09280 test_loss: 0.10319 \n",
      "[174/200] train_loss: 0.07758 valid_loss: 0.09434 test_loss: 0.10210 \n",
      "[175/200] train_loss: 0.07760 valid_loss: 0.09208 test_loss: 0.10141 \n",
      "[176/200] train_loss: 0.07958 valid_loss: 0.09164 test_loss: 0.10093 \n",
      "[177/200] train_loss: 0.07834 valid_loss: 0.09000 test_loss: 0.10089 \n",
      "[178/200] train_loss: 0.07579 valid_loss: 0.09060 test_loss: 0.10152 \n",
      "[179/200] train_loss: 0.07683 valid_loss: 0.08950 test_loss: 0.10048 \n",
      "[180/200] train_loss: 0.07909 valid_loss: 0.09116 test_loss: 0.10187 \n",
      "[181/200] train_loss: 0.07604 valid_loss: 0.08964 test_loss: 0.09999 \n",
      "[182/200] train_loss: 0.07640 valid_loss: 0.09116 test_loss: 0.10012 \n",
      "[183/200] train_loss: 0.07655 valid_loss: 0.08961 test_loss: 0.10024 \n",
      "[184/200] train_loss: 0.07760 valid_loss: 0.09075 test_loss: 0.10077 \n",
      "[185/200] train_loss: 0.07966 valid_loss: 0.09264 test_loss: 0.10213 \n",
      "[186/200] train_loss: 0.07692 valid_loss: 0.09154 test_loss: 0.10022 \n",
      "[187/200] train_loss: 0.07498 valid_loss: 0.09040 test_loss: 0.10045 \n",
      "[188/200] train_loss: 0.07781 valid_loss: 0.09000 test_loss: 0.09922 \n",
      "[189/200] train_loss: 0.07830 valid_loss: 0.08828 test_loss: 0.09995 \n",
      "Validation loss decreased (0.088828 --> 0.088281).  Saving model ...\n",
      "[190/200] train_loss: 0.07641 valid_loss: 0.08839 test_loss: 0.09986 \n",
      "[191/200] train_loss: 0.07781 valid_loss: 0.09141 test_loss: 0.10072 \n",
      "[192/200] train_loss: 0.07904 valid_loss: 0.09089 test_loss: 0.10236 \n",
      "[193/200] train_loss: 0.07458 valid_loss: 0.09104 test_loss: 0.10129 \n",
      "[194/200] train_loss: 0.07537 valid_loss: 0.08814 test_loss: 0.09993 \n",
      "Validation loss decreased (0.088281 --> 0.088144).  Saving model ...\n",
      "[195/200] train_loss: 0.07802 valid_loss: 0.09481 test_loss: 0.10101 \n",
      "[196/200] train_loss: 0.07375 valid_loss: 0.08983 test_loss: 0.09957 \n",
      "[197/200] train_loss: 0.07385 valid_loss: 0.08966 test_loss: 0.09876 \n",
      "[198/200] train_loss: 0.07574 valid_loss: 0.08775 test_loss: 0.09814 \n",
      "Validation loss decreased (0.088144 --> 0.087746).  Saving model ...\n",
      "[199/200] train_loss: 0.07421 valid_loss: 0.08951 test_loss: 0.09994 \n",
      "[200/200] train_loss: 0.07675 valid_loss: 0.08808 test_loss: 0.09930 \n",
      "Model 3 trained for seen data.\n",
      "TRAINING MODEL for seen house 4\n",
      "[  1/200] train_loss: 0.59183 valid_loss: 0.48827 test_loss: 0.48379 \n",
      "Validation loss decreased (inf --> 0.488266).  Saving model ...\n",
      "[  2/200] train_loss: 0.39045 valid_loss: 0.35149 test_loss: 0.35885 \n",
      "Validation loss decreased (0.488266 --> 0.351488).  Saving model ...\n",
      "[  3/200] train_loss: 0.30589 valid_loss: 0.30199 test_loss: 0.31216 \n",
      "Validation loss decreased (0.351488 --> 0.301988).  Saving model ...\n",
      "[  4/200] train_loss: 0.26138 valid_loss: 0.25731 test_loss: 0.27301 \n",
      "Validation loss decreased (0.301988 --> 0.257308).  Saving model ...\n",
      "[  5/200] train_loss: 0.22933 valid_loss: 0.22878 test_loss: 0.24366 \n",
      "Validation loss decreased (0.257308 --> 0.228775).  Saving model ...\n",
      "[  6/200] train_loss: 0.20735 valid_loss: 0.20602 test_loss: 0.22200 \n",
      "Validation loss decreased (0.228775 --> 0.206016).  Saving model ...\n",
      "[  7/200] train_loss: 0.18903 valid_loss: 0.19274 test_loss: 0.20639 \n",
      "Validation loss decreased (0.206016 --> 0.192735).  Saving model ...\n",
      "[  8/200] train_loss: 0.17974 valid_loss: 0.18010 test_loss: 0.19471 \n",
      "Validation loss decreased (0.192735 --> 0.180096).  Saving model ...\n",
      "[  9/200] train_loss: 0.17175 valid_loss: 0.17097 test_loss: 0.18551 \n",
      "Validation loss decreased (0.180096 --> 0.170971).  Saving model ...\n",
      "[ 10/200] train_loss: 0.16180 valid_loss: 0.16709 test_loss: 0.18144 \n",
      "Validation loss decreased (0.170971 --> 0.167089).  Saving model ...\n",
      "[ 11/200] train_loss: 0.15837 valid_loss: 0.16144 test_loss: 0.17378 \n",
      "Validation loss decreased (0.167089 --> 0.161439).  Saving model ...\n",
      "[ 12/200] train_loss: 0.14971 valid_loss: 0.15648 test_loss: 0.17178 \n",
      "Validation loss decreased (0.161439 --> 0.156483).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14553 valid_loss: 0.15405 test_loss: 0.16710 \n",
      "Validation loss decreased (0.156483 --> 0.154055).  Saving model ...\n",
      "[ 14/200] train_loss: 0.14476 valid_loss: 0.14920 test_loss: 0.16330 \n",
      "Validation loss decreased (0.154055 --> 0.149195).  Saving model ...\n",
      "[ 15/200] train_loss: 0.14176 valid_loss: 0.14918 test_loss: 0.16567 \n",
      "Validation loss decreased (0.149195 --> 0.149176).  Saving model ...\n",
      "[ 16/200] train_loss: 0.14015 valid_loss: 0.14528 test_loss: 0.15930 \n",
      "Validation loss decreased (0.149176 --> 0.145284).  Saving model ...\n",
      "[ 17/200] train_loss: 0.13561 valid_loss: 0.14355 test_loss: 0.15856 \n",
      "Validation loss decreased (0.145284 --> 0.143548).  Saving model ...\n",
      "[ 18/200] train_loss: 0.13160 valid_loss: 0.13844 test_loss: 0.15294 \n",
      "Validation loss decreased (0.143548 --> 0.138444).  Saving model ...\n",
      "[ 19/200] train_loss: 0.13440 valid_loss: 0.13806 test_loss: 0.15292 \n",
      "Validation loss decreased (0.138444 --> 0.138058).  Saving model ...\n",
      "[ 20/200] train_loss: 0.13466 valid_loss: 0.13936 test_loss: 0.15344 \n",
      "[ 21/200] train_loss: 0.12870 valid_loss: 0.13263 test_loss: 0.14959 \n",
      "Validation loss decreased (0.138058 --> 0.132629).  Saving model ...\n",
      "[ 22/200] train_loss: 0.12668 valid_loss: 0.13488 test_loss: 0.14934 \n",
      "[ 23/200] train_loss: 0.12547 valid_loss: 0.13144 test_loss: 0.14851 \n",
      "Validation loss decreased (0.132629 --> 0.131442).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12837 valid_loss: 0.13157 test_loss: 0.14698 \n",
      "[ 25/200] train_loss: 0.12572 valid_loss: 0.13531 test_loss: 0.14705 \n",
      "[ 26/200] train_loss: 0.12246 valid_loss: 0.13072 test_loss: 0.14445 \n",
      "Validation loss decreased (0.131442 --> 0.130721).  Saving model ...\n",
      "[ 27/200] train_loss: 0.12521 valid_loss: 0.12866 test_loss: 0.14439 \n",
      "Validation loss decreased (0.130721 --> 0.128657).  Saving model ...\n",
      "[ 28/200] train_loss: 0.12271 valid_loss: 0.12712 test_loss: 0.14395 \n",
      "Validation loss decreased (0.128657 --> 0.127120).  Saving model ...\n",
      "[ 29/200] train_loss: 0.12197 valid_loss: 0.12914 test_loss: 0.14241 \n",
      "[ 30/200] train_loss: 0.11680 valid_loss: 0.12527 test_loss: 0.14114 \n",
      "Validation loss decreased (0.127120 --> 0.125274).  Saving model ...\n",
      "[ 31/200] train_loss: 0.11736 valid_loss: 0.12568 test_loss: 0.13930 \n",
      "[ 32/200] train_loss: 0.11661 valid_loss: 0.12167 test_loss: 0.13666 \n",
      "Validation loss decreased (0.125274 --> 0.121674).  Saving model ...\n",
      "[ 33/200] train_loss: 0.11712 valid_loss: 0.12454 test_loss: 0.14040 \n",
      "[ 34/200] train_loss: 0.11550 valid_loss: 0.11973 test_loss: 0.13663 \n",
      "Validation loss decreased (0.121674 --> 0.119731).  Saving model ...\n",
      "[ 35/200] train_loss: 0.11321 valid_loss: 0.12308 test_loss: 0.13904 \n",
      "[ 36/200] train_loss: 0.11501 valid_loss: 0.12176 test_loss: 0.13806 \n",
      "[ 37/200] train_loss: 0.11312 valid_loss: 0.12063 test_loss: 0.13569 \n",
      "[ 38/200] train_loss: 0.11348 valid_loss: 0.12103 test_loss: 0.13787 \n",
      "[ 39/200] train_loss: 0.11322 valid_loss: 0.11901 test_loss: 0.13441 \n",
      "Validation loss decreased (0.119731 --> 0.119012).  Saving model ...\n",
      "[ 40/200] train_loss: 0.10755 valid_loss: 0.11518 test_loss: 0.13350 \n",
      "Validation loss decreased (0.119012 --> 0.115176).  Saving model ...\n",
      "[ 41/200] train_loss: 0.11013 valid_loss: 0.11815 test_loss: 0.13406 \n",
      "[ 42/200] train_loss: 0.10893 valid_loss: 0.11596 test_loss: 0.13163 \n",
      "[ 43/200] train_loss: 0.10732 valid_loss: 0.11755 test_loss: 0.13340 \n",
      "[ 44/200] train_loss: 0.10935 valid_loss: 0.11738 test_loss: 0.13253 \n",
      "[ 45/200] train_loss: 0.11154 valid_loss: 0.11942 test_loss: 0.12947 \n",
      "[ 46/200] train_loss: 0.10962 valid_loss: 0.11805 test_loss: 0.13234 \n",
      "[ 47/200] train_loss: 0.10698 valid_loss: 0.11519 test_loss: 0.12966 \n",
      "[ 48/200] train_loss: 0.10860 valid_loss: 0.11573 test_loss: 0.13061 \n",
      "[ 49/200] train_loss: 0.10524 valid_loss: 0.11326 test_loss: 0.12746 \n",
      "Validation loss decreased (0.115176 --> 0.113255).  Saving model ...\n",
      "[ 50/200] train_loss: 0.10549 valid_loss: 0.11666 test_loss: 0.13176 \n",
      "[ 51/200] train_loss: 0.10420 valid_loss: 0.11363 test_loss: 0.12995 \n",
      "[ 52/200] train_loss: 0.10431 valid_loss: 0.11374 test_loss: 0.12772 \n",
      "[ 53/200] train_loss: 0.10563 valid_loss: 0.11299 test_loss: 0.12740 \n",
      "Validation loss decreased (0.113255 --> 0.112994).  Saving model ...\n",
      "[ 54/200] train_loss: 0.10206 valid_loss: 0.11748 test_loss: 0.12958 \n",
      "[ 55/200] train_loss: 0.10354 valid_loss: 0.10987 test_loss: 0.12496 \n",
      "Validation loss decreased (0.112994 --> 0.109874).  Saving model ...\n",
      "[ 56/200] train_loss: 0.10603 valid_loss: 0.10851 test_loss: 0.12475 \n",
      "Validation loss decreased (0.109874 --> 0.108512).  Saving model ...\n",
      "[ 57/200] train_loss: 0.10459 valid_loss: 0.11113 test_loss: 0.12459 \n",
      "[ 58/200] train_loss: 0.10314 valid_loss: 0.10720 test_loss: 0.12264 \n",
      "Validation loss decreased (0.108512 --> 0.107204).  Saving model ...\n",
      "[ 59/200] train_loss: 0.10477 valid_loss: 0.10747 test_loss: 0.12255 \n",
      "[ 60/200] train_loss: 0.10451 valid_loss: 0.11188 test_loss: 0.12265 \n",
      "[ 61/200] train_loss: 0.10103 valid_loss: 0.10982 test_loss: 0.12223 \n",
      "[ 62/200] train_loss: 0.09833 valid_loss: 0.11190 test_loss: 0.12437 \n",
      "[ 63/200] train_loss: 0.10091 valid_loss: 0.11005 test_loss: 0.12315 \n",
      "[ 64/200] train_loss: 0.09993 valid_loss: 0.10730 test_loss: 0.12332 \n",
      "[ 65/200] train_loss: 0.09812 valid_loss: 0.10911 test_loss: 0.12055 \n",
      "[ 66/200] train_loss: 0.09880 valid_loss: 0.10832 test_loss: 0.12035 \n",
      "[ 67/200] train_loss: 0.09898 valid_loss: 0.10712 test_loss: 0.12024 \n",
      "Validation loss decreased (0.107204 --> 0.107118).  Saving model ...\n",
      "[ 68/200] train_loss: 0.10024 valid_loss: 0.10637 test_loss: 0.12100 \n",
      "Validation loss decreased (0.107118 --> 0.106368).  Saving model ...\n",
      "[ 69/200] train_loss: 0.09947 valid_loss: 0.10619 test_loss: 0.11964 \n",
      "Validation loss decreased (0.106368 --> 0.106190).  Saving model ...\n",
      "[ 70/200] train_loss: 0.09678 valid_loss: 0.10587 test_loss: 0.11994 \n",
      "Validation loss decreased (0.106190 --> 0.105875).  Saving model ...\n",
      "[ 71/200] train_loss: 0.09808 valid_loss: 0.10683 test_loss: 0.11858 \n",
      "[ 72/200] train_loss: 0.09681 valid_loss: 0.10531 test_loss: 0.11824 \n",
      "Validation loss decreased (0.105875 --> 0.105315).  Saving model ...\n",
      "[ 73/200] train_loss: 0.09967 valid_loss: 0.10490 test_loss: 0.11875 \n",
      "Validation loss decreased (0.105315 --> 0.104896).  Saving model ...\n",
      "[ 74/200] train_loss: 0.09748 valid_loss: 0.10693 test_loss: 0.11919 \n",
      "[ 75/200] train_loss: 0.09696 valid_loss: 0.10248 test_loss: 0.11675 \n",
      "Validation loss decreased (0.104896 --> 0.102477).  Saving model ...\n",
      "[ 76/200] train_loss: 0.09662 valid_loss: 0.10324 test_loss: 0.11785 \n",
      "[ 77/200] train_loss: 0.09464 valid_loss: 0.10187 test_loss: 0.11777 \n",
      "Validation loss decreased (0.102477 --> 0.101871).  Saving model ...\n",
      "[ 78/200] train_loss: 0.10008 valid_loss: 0.10357 test_loss: 0.11657 \n",
      "[ 79/200] train_loss: 0.09771 valid_loss: 0.10306 test_loss: 0.11676 \n",
      "[ 80/200] train_loss: 0.09652 valid_loss: 0.10706 test_loss: 0.11595 \n",
      "[ 81/200] train_loss: 0.09498 valid_loss: 0.10360 test_loss: 0.11609 \n",
      "[ 82/200] train_loss: 0.09519 valid_loss: 0.10062 test_loss: 0.11497 \n",
      "Validation loss decreased (0.101871 --> 0.100622).  Saving model ...\n",
      "[ 83/200] train_loss: 0.09543 valid_loss: 0.10395 test_loss: 0.11555 \n",
      "[ 84/200] train_loss: 0.09458 valid_loss: 0.10431 test_loss: 0.11667 \n",
      "[ 85/200] train_loss: 0.09619 valid_loss: 0.10291 test_loss: 0.11749 \n",
      "[ 86/200] train_loss: 0.09498 valid_loss: 0.10140 test_loss: 0.11511 \n",
      "[ 87/200] train_loss: 0.09408 valid_loss: 0.09941 test_loss: 0.11508 \n",
      "Validation loss decreased (0.100622 --> 0.099410).  Saving model ...\n",
      "[ 88/200] train_loss: 0.09297 valid_loss: 0.10188 test_loss: 0.11639 \n",
      "[ 89/200] train_loss: 0.09305 valid_loss: 0.10239 test_loss: 0.11463 \n",
      "[ 90/200] train_loss: 0.09432 valid_loss: 0.09890 test_loss: 0.11411 \n",
      "Validation loss decreased (0.099410 --> 0.098898).  Saving model ...\n",
      "[ 91/200] train_loss: 0.09441 valid_loss: 0.09865 test_loss: 0.11314 \n",
      "Validation loss decreased (0.098898 --> 0.098652).  Saving model ...\n",
      "[ 92/200] train_loss: 0.09212 valid_loss: 0.09958 test_loss: 0.11435 \n",
      "[ 93/200] train_loss: 0.09090 valid_loss: 0.09925 test_loss: 0.11267 \n",
      "[ 94/200] train_loss: 0.09136 valid_loss: 0.10004 test_loss: 0.11363 \n",
      "[ 95/200] train_loss: 0.09273 valid_loss: 0.09830 test_loss: 0.11302 \n",
      "Validation loss decreased (0.098652 --> 0.098302).  Saving model ...\n",
      "[ 96/200] train_loss: 0.09235 valid_loss: 0.09977 test_loss: 0.11420 \n",
      "[ 97/200] train_loss: 0.09000 valid_loss: 0.09794 test_loss: 0.11145 \n",
      "Validation loss decreased (0.098302 --> 0.097940).  Saving model ...\n",
      "[ 98/200] train_loss: 0.09063 valid_loss: 0.09667 test_loss: 0.11334 \n",
      "Validation loss decreased (0.097940 --> 0.096673).  Saving model ...\n",
      "[ 99/200] train_loss: 0.08889 valid_loss: 0.10148 test_loss: 0.11314 \n",
      "[100/200] train_loss: 0.09005 valid_loss: 0.09965 test_loss: 0.11117 \n",
      "[101/200] train_loss: 0.08800 valid_loss: 0.09837 test_loss: 0.11223 \n",
      "[102/200] train_loss: 0.08918 valid_loss: 0.09754 test_loss: 0.11235 \n",
      "[103/200] train_loss: 0.09171 valid_loss: 0.09632 test_loss: 0.11082 \n",
      "Validation loss decreased (0.096673 --> 0.096318).  Saving model ...\n",
      "[104/200] train_loss: 0.08887 valid_loss: 0.09816 test_loss: 0.11098 \n",
      "[105/200] train_loss: 0.08568 valid_loss: 0.09818 test_loss: 0.11048 \n",
      "[106/200] train_loss: 0.09006 valid_loss: 0.09610 test_loss: 0.11051 \n",
      "Validation loss decreased (0.096318 --> 0.096105).  Saving model ...\n",
      "[107/200] train_loss: 0.08920 valid_loss: 0.09715 test_loss: 0.11124 \n",
      "[108/200] train_loss: 0.09007 valid_loss: 0.09590 test_loss: 0.10939 \n",
      "Validation loss decreased (0.096105 --> 0.095898).  Saving model ...\n",
      "[109/200] train_loss: 0.08699 valid_loss: 0.09537 test_loss: 0.10979 \n",
      "Validation loss decreased (0.095898 --> 0.095374).  Saving model ...\n",
      "[110/200] train_loss: 0.08869 valid_loss: 0.09769 test_loss: 0.11155 \n",
      "[111/200] train_loss: 0.08969 valid_loss: 0.09614 test_loss: 0.11239 \n",
      "[112/200] train_loss: 0.08732 valid_loss: 0.09634 test_loss: 0.11016 \n",
      "[113/200] train_loss: 0.08532 valid_loss: 0.09436 test_loss: 0.10853 \n",
      "Validation loss decreased (0.095374 --> 0.094361).  Saving model ...\n",
      "[114/200] train_loss: 0.08671 valid_loss: 0.09600 test_loss: 0.10978 \n",
      "[115/200] train_loss: 0.08819 valid_loss: 0.09298 test_loss: 0.10831 \n",
      "Validation loss decreased (0.094361 --> 0.092977).  Saving model ...\n",
      "[116/200] train_loss: 0.08928 valid_loss: 0.09329 test_loss: 0.10702 \n",
      "[117/200] train_loss: 0.08534 valid_loss: 0.09692 test_loss: 0.10930 \n",
      "[118/200] train_loss: 0.08784 valid_loss: 0.09316 test_loss: 0.10797 \n",
      "[119/200] train_loss: 0.08393 valid_loss: 0.09435 test_loss: 0.10971 \n",
      "[120/200] train_loss: 0.08863 valid_loss: 0.09666 test_loss: 0.10754 \n",
      "[121/200] train_loss: 0.08657 valid_loss: 0.09603 test_loss: 0.10898 \n",
      "[122/200] train_loss: 0.08685 valid_loss: 0.09553 test_loss: 0.10793 \n",
      "[123/200] train_loss: 0.08242 valid_loss: 0.09465 test_loss: 0.10797 \n",
      "[124/200] train_loss: 0.08604 valid_loss: 0.09474 test_loss: 0.10772 \n",
      "[125/200] train_loss: 0.08472 valid_loss: 0.09299 test_loss: 0.10590 \n",
      "[126/200] train_loss: 0.08806 valid_loss: 0.09324 test_loss: 0.10695 \n",
      "[127/200] train_loss: 0.08559 valid_loss: 0.09316 test_loss: 0.10777 \n",
      "[128/200] train_loss: 0.08442 valid_loss: 0.09632 test_loss: 0.10756 \n",
      "[129/200] train_loss: 0.08453 valid_loss: 0.09332 test_loss: 0.10674 \n",
      "[130/200] train_loss: 0.08508 valid_loss: 0.09296 test_loss: 0.10709 \n",
      "Validation loss decreased (0.092977 --> 0.092961).  Saving model ...\n",
      "[131/200] train_loss: 0.08760 valid_loss: 0.09055 test_loss: 0.10540 \n",
      "Validation loss decreased (0.092961 --> 0.090549).  Saving model ...\n",
      "[132/200] train_loss: 0.08388 valid_loss: 0.09321 test_loss: 0.10638 \n",
      "[133/200] train_loss: 0.08496 valid_loss: 0.09281 test_loss: 0.10623 \n",
      "[134/200] train_loss: 0.08464 valid_loss: 0.09197 test_loss: 0.10557 \n",
      "[135/200] train_loss: 0.08512 valid_loss: 0.09332 test_loss: 0.10558 \n",
      "[136/200] train_loss: 0.08415 valid_loss: 0.09066 test_loss: 0.10420 \n",
      "[137/200] train_loss: 0.08421 valid_loss: 0.09311 test_loss: 0.10570 \n",
      "[138/200] train_loss: 0.08554 valid_loss: 0.09117 test_loss: 0.10546 \n",
      "[139/200] train_loss: 0.08826 valid_loss: 0.09201 test_loss: 0.10577 \n",
      "[140/200] train_loss: 0.08377 valid_loss: 0.09116 test_loss: 0.10332 \n",
      "[141/200] train_loss: 0.08279 valid_loss: 0.09444 test_loss: 0.10508 \n",
      "[142/200] train_loss: 0.08193 valid_loss: 0.09024 test_loss: 0.10418 \n",
      "Validation loss decreased (0.090549 --> 0.090240).  Saving model ...\n",
      "[143/200] train_loss: 0.08183 valid_loss: 0.09090 test_loss: 0.10593 \n",
      "[144/200] train_loss: 0.08077 valid_loss: 0.09190 test_loss: 0.10519 \n",
      "[145/200] train_loss: 0.08217 valid_loss: 0.09247 test_loss: 0.10500 \n",
      "[146/200] train_loss: 0.08571 valid_loss: 0.09158 test_loss: 0.10508 \n",
      "[147/200] train_loss: 0.08184 valid_loss: 0.09299 test_loss: 0.10563 \n",
      "[148/200] train_loss: 0.08030 valid_loss: 0.09362 test_loss: 0.10380 \n",
      "[149/200] train_loss: 0.08341 valid_loss: 0.09262 test_loss: 0.10563 \n",
      "[150/200] train_loss: 0.08501 valid_loss: 0.08963 test_loss: 0.10397 \n",
      "Validation loss decreased (0.090240 --> 0.089633).  Saving model ...\n",
      "[151/200] train_loss: 0.08201 valid_loss: 0.08940 test_loss: 0.10457 \n",
      "Validation loss decreased (0.089633 --> 0.089396).  Saving model ...\n",
      "[152/200] train_loss: 0.08047 valid_loss: 0.08842 test_loss: 0.10352 \n",
      "Validation loss decreased (0.089396 --> 0.088418).  Saving model ...\n",
      "[153/200] train_loss: 0.08203 valid_loss: 0.08817 test_loss: 0.10239 \n",
      "Validation loss decreased (0.088418 --> 0.088168).  Saving model ...\n",
      "[154/200] train_loss: 0.08262 valid_loss: 0.09259 test_loss: 0.10332 \n",
      "[155/200] train_loss: 0.08088 valid_loss: 0.08904 test_loss: 0.10351 \n",
      "[156/200] train_loss: 0.08292 valid_loss: 0.08885 test_loss: 0.10238 \n",
      "[157/200] train_loss: 0.08153 valid_loss: 0.09038 test_loss: 0.10369 \n",
      "[158/200] train_loss: 0.08431 valid_loss: 0.08748 test_loss: 0.10273 \n",
      "Validation loss decreased (0.088168 --> 0.087482).  Saving model ...\n",
      "[159/200] train_loss: 0.08326 valid_loss: 0.08827 test_loss: 0.10207 \n",
      "[160/200] train_loss: 0.07917 valid_loss: 0.09027 test_loss: 0.10327 \n",
      "[161/200] train_loss: 0.07789 valid_loss: 0.08767 test_loss: 0.10166 \n",
      "[162/200] train_loss: 0.07977 valid_loss: 0.08757 test_loss: 0.10149 \n",
      "[163/200] train_loss: 0.08356 valid_loss: 0.10033 test_loss: 0.10207 \n",
      "[164/200] train_loss: 0.08315 valid_loss: 0.09010 test_loss: 0.10260 \n",
      "[165/200] train_loss: 0.08117 valid_loss: 0.09034 test_loss: 0.10192 \n",
      "[166/200] train_loss: 0.07977 valid_loss: 0.08958 test_loss: 0.10231 \n",
      "[167/200] train_loss: 0.07999 valid_loss: 0.08800 test_loss: 0.10060 \n",
      "[168/200] train_loss: 0.07982 valid_loss: 0.09031 test_loss: 0.10112 \n",
      "[169/200] train_loss: 0.08182 valid_loss: 0.08801 test_loss: 0.10160 \n",
      "[170/200] train_loss: 0.07927 valid_loss: 0.09021 test_loss: 0.10143 \n",
      "[171/200] train_loss: 0.07736 valid_loss: 0.08897 test_loss: 0.10122 \n",
      "[172/200] train_loss: 0.08103 valid_loss: 0.08882 test_loss: 0.10180 \n",
      "[173/200] train_loss: 0.07785 valid_loss: 0.08749 test_loss: 0.10147 \n",
      "[174/200] train_loss: 0.07667 valid_loss: 0.08891 test_loss: 0.10046 \n",
      "[175/200] train_loss: 0.07939 valid_loss: 0.08857 test_loss: 0.10076 \n",
      "[176/200] train_loss: 0.08040 valid_loss: 0.09122 test_loss: 0.10173 \n",
      "[177/200] train_loss: 0.07706 valid_loss: 0.09149 test_loss: 0.10293 \n",
      "[178/200] train_loss: 0.07881 valid_loss: 0.08851 test_loss: 0.10115 \n",
      "[179/200] train_loss: 0.07833 valid_loss: 0.08760 test_loss: 0.10027 \n",
      "[180/200] train_loss: 0.07689 valid_loss: 0.08810 test_loss: 0.09986 \n",
      "[181/200] train_loss: 0.08012 valid_loss: 0.08887 test_loss: 0.10058 \n",
      "[182/200] train_loss: 0.08183 valid_loss: 0.08723 test_loss: 0.09968 \n",
      "Validation loss decreased (0.087482 --> 0.087231).  Saving model ...\n",
      "[183/200] train_loss: 0.07713 valid_loss: 0.08792 test_loss: 0.09882 \n",
      "[184/200] train_loss: 0.07667 valid_loss: 0.08709 test_loss: 0.09896 \n",
      "Validation loss decreased (0.087231 --> 0.087089).  Saving model ...\n",
      "[185/200] train_loss: 0.07763 valid_loss: 0.08736 test_loss: 0.09972 \n",
      "[186/200] train_loss: 0.07768 valid_loss: 0.08867 test_loss: 0.09982 \n",
      "[187/200] train_loss: 0.07816 valid_loss: 0.08641 test_loss: 0.09952 \n",
      "Validation loss decreased (0.087089 --> 0.086413).  Saving model ...\n",
      "[188/200] train_loss: 0.08051 valid_loss: 0.08699 test_loss: 0.10021 \n",
      "[189/200] train_loss: 0.07953 valid_loss: 0.08797 test_loss: 0.10013 \n",
      "[190/200] train_loss: 0.07514 valid_loss: 0.08621 test_loss: 0.09855 \n",
      "Validation loss decreased (0.086413 --> 0.086213).  Saving model ...\n",
      "[191/200] train_loss: 0.07946 valid_loss: 0.08602 test_loss: 0.09845 \n",
      "Validation loss decreased (0.086213 --> 0.086021).  Saving model ...\n",
      "[192/200] train_loss: 0.07731 valid_loss: 0.08654 test_loss: 0.09909 \n",
      "[193/200] train_loss: 0.07697 valid_loss: 0.08707 test_loss: 0.09923 \n",
      "[194/200] train_loss: 0.07588 valid_loss: 0.08563 test_loss: 0.09827 \n",
      "Validation loss decreased (0.086021 --> 0.085625).  Saving model ...\n",
      "[195/200] train_loss: 0.07627 valid_loss: 0.08574 test_loss: 0.09949 \n",
      "[196/200] train_loss: 0.07712 valid_loss: 0.08620 test_loss: 0.09879 \n",
      "[197/200] train_loss: 0.07850 valid_loss: 0.08511 test_loss: 0.09846 \n",
      "Validation loss decreased (0.085625 --> 0.085111).  Saving model ...\n",
      "[198/200] train_loss: 0.07715 valid_loss: 0.08523 test_loss: 0.09813 \n",
      "[199/200] train_loss: 0.07459 valid_loss: 0.08731 test_loss: 0.09823 \n",
      "[200/200] train_loss: 0.07667 valid_loss: 0.08572 test_loss: 0.09836 \n",
      "Model 4 trained for seen data.\n",
      "TRAINING MODEL for seen house 5\n",
      "[  1/200] train_loss: 0.52099 valid_loss: 0.40681 test_loss: 0.40896 \n",
      "Validation loss decreased (inf --> 0.406812).  Saving model ...\n",
      "[  2/200] train_loss: 0.33627 valid_loss: 0.30844 test_loss: 0.31950 \n",
      "Validation loss decreased (0.406812 --> 0.308441).  Saving model ...\n",
      "[  3/200] train_loss: 0.26524 valid_loss: 0.25925 test_loss: 0.27620 \n",
      "Validation loss decreased (0.308441 --> 0.259249).  Saving model ...\n",
      "[  4/200] train_loss: 0.22544 valid_loss: 0.22353 test_loss: 0.23819 \n",
      "Validation loss decreased (0.259249 --> 0.223535).  Saving model ...\n",
      "[  5/200] train_loss: 0.19974 valid_loss: 0.20037 test_loss: 0.21450 \n",
      "Validation loss decreased (0.223535 --> 0.200373).  Saving model ...\n",
      "[  6/200] train_loss: 0.17800 valid_loss: 0.18600 test_loss: 0.19820 \n",
      "Validation loss decreased (0.200373 --> 0.186001).  Saving model ...\n",
      "[  7/200] train_loss: 0.17318 valid_loss: 0.17495 test_loss: 0.18716 \n",
      "Validation loss decreased (0.186001 --> 0.174953).  Saving model ...\n",
      "[  8/200] train_loss: 0.16162 valid_loss: 0.16928 test_loss: 0.18128 \n",
      "Validation loss decreased (0.174953 --> 0.169276).  Saving model ...\n",
      "[  9/200] train_loss: 0.15925 valid_loss: 0.16328 test_loss: 0.17434 \n",
      "Validation loss decreased (0.169276 --> 0.163275).  Saving model ...\n",
      "[ 10/200] train_loss: 0.15325 valid_loss: 0.15936 test_loss: 0.17063 \n",
      "Validation loss decreased (0.163275 --> 0.159364).  Saving model ...\n",
      "[ 11/200] train_loss: 0.14806 valid_loss: 0.15465 test_loss: 0.16625 \n",
      "Validation loss decreased (0.159364 --> 0.154645).  Saving model ...\n",
      "[ 12/200] train_loss: 0.14410 valid_loss: 0.15178 test_loss: 0.16362 \n",
      "Validation loss decreased (0.154645 --> 0.151785).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14194 valid_loss: 0.15053 test_loss: 0.16390 \n",
      "Validation loss decreased (0.151785 --> 0.150525).  Saving model ...\n",
      "[ 14/200] train_loss: 0.14180 valid_loss: 0.14875 test_loss: 0.16010 \n",
      "Validation loss decreased (0.150525 --> 0.148745).  Saving model ...\n",
      "[ 15/200] train_loss: 0.13732 valid_loss: 0.14702 test_loss: 0.15670 \n",
      "Validation loss decreased (0.148745 --> 0.147024).  Saving model ...\n",
      "[ 16/200] train_loss: 0.13442 valid_loss: 0.14390 test_loss: 0.15559 \n",
      "Validation loss decreased (0.147024 --> 0.143904).  Saving model ...\n",
      "[ 17/200] train_loss: 0.13264 valid_loss: 0.14039 test_loss: 0.15282 \n",
      "Validation loss decreased (0.143904 --> 0.140387).  Saving model ...\n",
      "[ 18/200] train_loss: 0.12909 valid_loss: 0.13713 test_loss: 0.15080 \n",
      "Validation loss decreased (0.140387 --> 0.137131).  Saving model ...\n",
      "[ 19/200] train_loss: 0.12722 valid_loss: 0.13852 test_loss: 0.15547 \n",
      "[ 20/200] train_loss: 0.13036 valid_loss: 0.13597 test_loss: 0.14849 \n",
      "Validation loss decreased (0.137131 --> 0.135965).  Saving model ...\n",
      "[ 21/200] train_loss: 0.12598 valid_loss: 0.13664 test_loss: 0.15122 \n",
      "[ 22/200] train_loss: 0.12417 valid_loss: 0.13307 test_loss: 0.14597 \n",
      "Validation loss decreased (0.135965 --> 0.133068).  Saving model ...\n",
      "[ 23/200] train_loss: 0.12178 valid_loss: 0.12972 test_loss: 0.14415 \n",
      "Validation loss decreased (0.133068 --> 0.129723).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12452 valid_loss: 0.13099 test_loss: 0.14449 \n",
      "[ 25/200] train_loss: 0.12121 valid_loss: 0.12684 test_loss: 0.14212 \n",
      "Validation loss decreased (0.129723 --> 0.126839).  Saving model ...\n",
      "[ 26/200] train_loss: 0.11931 valid_loss: 0.12578 test_loss: 0.14348 \n",
      "Validation loss decreased (0.126839 --> 0.125778).  Saving model ...\n",
      "[ 27/200] train_loss: 0.11991 valid_loss: 0.12493 test_loss: 0.14100 \n",
      "Validation loss decreased (0.125778 --> 0.124926).  Saving model ...\n",
      "[ 28/200] train_loss: 0.11660 valid_loss: 0.12900 test_loss: 0.13946 \n",
      "[ 29/200] train_loss: 0.11611 valid_loss: 0.12363 test_loss: 0.13869 \n",
      "Validation loss decreased (0.124926 --> 0.123630).  Saving model ...\n",
      "[ 30/200] train_loss: 0.11815 valid_loss: 0.12286 test_loss: 0.13947 \n",
      "Validation loss decreased (0.123630 --> 0.122865).  Saving model ...\n",
      "[ 31/200] train_loss: 0.11750 valid_loss: 0.12744 test_loss: 0.14447 \n",
      "[ 32/200] train_loss: 0.11291 valid_loss: 0.11986 test_loss: 0.13815 \n",
      "Validation loss decreased (0.122865 --> 0.119858).  Saving model ...\n",
      "[ 33/200] train_loss: 0.11277 valid_loss: 0.12157 test_loss: 0.13708 \n",
      "[ 34/200] train_loss: 0.11388 valid_loss: 0.12121 test_loss: 0.13668 \n",
      "[ 35/200] train_loss: 0.11190 valid_loss: 0.12299 test_loss: 0.13635 \n",
      "[ 36/200] train_loss: 0.11098 valid_loss: 0.11770 test_loss: 0.13570 \n",
      "Validation loss decreased (0.119858 --> 0.117704).  Saving model ...\n",
      "[ 37/200] train_loss: 0.11105 valid_loss: 0.12137 test_loss: 0.13645 \n",
      "[ 38/200] train_loss: 0.11112 valid_loss: 0.11893 test_loss: 0.13478 \n",
      "[ 39/200] train_loss: 0.11141 valid_loss: 0.11884 test_loss: 0.13291 \n",
      "[ 40/200] train_loss: 0.10886 valid_loss: 0.11821 test_loss: 0.13320 \n",
      "[ 41/200] train_loss: 0.10578 valid_loss: 0.11762 test_loss: 0.13476 \n",
      "Validation loss decreased (0.117704 --> 0.117623).  Saving model ...\n",
      "[ 42/200] train_loss: 0.10888 valid_loss: 0.11821 test_loss: 0.13218 \n",
      "[ 43/200] train_loss: 0.10814 valid_loss: 0.11679 test_loss: 0.13407 \n",
      "Validation loss decreased (0.117623 --> 0.116793).  Saving model ...\n",
      "[ 44/200] train_loss: 0.10964 valid_loss: 0.11185 test_loss: 0.13043 \n",
      "Validation loss decreased (0.116793 --> 0.111845).  Saving model ...\n",
      "[ 45/200] train_loss: 0.10445 valid_loss: 0.11600 test_loss: 0.13087 \n",
      "[ 46/200] train_loss: 0.10497 valid_loss: 0.11756 test_loss: 0.13115 \n",
      "[ 47/200] train_loss: 0.10542 valid_loss: 0.11401 test_loss: 0.12978 \n",
      "[ 48/200] train_loss: 0.10484 valid_loss: 0.11187 test_loss: 0.12796 \n",
      "[ 49/200] train_loss: 0.10461 valid_loss: 0.11265 test_loss: 0.12751 \n",
      "[ 50/200] train_loss: 0.10442 valid_loss: 0.11020 test_loss: 0.12637 \n",
      "Validation loss decreased (0.111845 --> 0.110197).  Saving model ...\n",
      "[ 51/200] train_loss: 0.09869 valid_loss: 0.11439 test_loss: 0.13013 \n",
      "[ 52/200] train_loss: 0.10121 valid_loss: 0.11244 test_loss: 0.12792 \n",
      "[ 53/200] train_loss: 0.10164 valid_loss: 0.11049 test_loss: 0.12744 \n",
      "[ 54/200] train_loss: 0.10210 valid_loss: 0.10932 test_loss: 0.12557 \n",
      "Validation loss decreased (0.110197 --> 0.109321).  Saving model ...\n",
      "[ 55/200] train_loss: 0.10216 valid_loss: 0.10932 test_loss: 0.12544 \n",
      "Validation loss decreased (0.109321 --> 0.109317).  Saving model ...\n",
      "[ 56/200] train_loss: 0.10328 valid_loss: 0.10830 test_loss: 0.12745 \n",
      "Validation loss decreased (0.109317 --> 0.108300).  Saving model ...\n",
      "[ 57/200] train_loss: 0.10048 valid_loss: 0.10842 test_loss: 0.12503 \n",
      "[ 58/200] train_loss: 0.09779 valid_loss: 0.10880 test_loss: 0.12439 \n",
      "[ 59/200] train_loss: 0.10153 valid_loss: 0.11260 test_loss: 0.12317 \n",
      "[ 60/200] train_loss: 0.10055 valid_loss: 0.11451 test_loss: 0.12672 \n",
      "[ 61/200] train_loss: 0.09879 valid_loss: 0.10748 test_loss: 0.12234 \n",
      "Validation loss decreased (0.108300 --> 0.107484).  Saving model ...\n",
      "[ 62/200] train_loss: 0.09924 valid_loss: 0.10943 test_loss: 0.12297 \n",
      "[ 63/200] train_loss: 0.09651 valid_loss: 0.11378 test_loss: 0.12336 \n",
      "[ 64/200] train_loss: 0.09875 valid_loss: 0.10634 test_loss: 0.11962 \n",
      "Validation loss decreased (0.107484 --> 0.106340).  Saving model ...\n",
      "[ 65/200] train_loss: 0.09720 valid_loss: 0.10680 test_loss: 0.11939 \n",
      "[ 66/200] train_loss: 0.09623 valid_loss: 0.10912 test_loss: 0.12133 \n",
      "[ 67/200] train_loss: 0.09639 valid_loss: 0.10683 test_loss: 0.12065 \n",
      "[ 68/200] train_loss: 0.09614 valid_loss: 0.10899 test_loss: 0.12198 \n",
      "[ 69/200] train_loss: 0.09720 valid_loss: 0.10608 test_loss: 0.11919 \n",
      "Validation loss decreased (0.106340 --> 0.106078).  Saving model ...\n",
      "[ 70/200] train_loss: 0.09659 valid_loss: 0.10456 test_loss: 0.12005 \n",
      "Validation loss decreased (0.106078 --> 0.104565).  Saving model ...\n",
      "[ 71/200] train_loss: 0.09660 valid_loss: 0.10506 test_loss: 0.11788 \n",
      "[ 72/200] train_loss: 0.09643 valid_loss: 0.10394 test_loss: 0.11842 \n",
      "Validation loss decreased (0.104565 --> 0.103944).  Saving model ...\n",
      "[ 73/200] train_loss: 0.09230 valid_loss: 0.10449 test_loss: 0.11799 \n",
      "[ 74/200] train_loss: 0.09449 valid_loss: 0.10496 test_loss: 0.11743 \n",
      "[ 75/200] train_loss: 0.09391 valid_loss: 0.10519 test_loss: 0.11788 \n",
      "[ 76/200] train_loss: 0.09475 valid_loss: 0.10241 test_loss: 0.11921 \n",
      "Validation loss decreased (0.103944 --> 0.102413).  Saving model ...\n",
      "[ 77/200] train_loss: 0.09358 valid_loss: 0.10574 test_loss: 0.11915 \n",
      "[ 78/200] train_loss: 0.09376 valid_loss: 0.10795 test_loss: 0.11897 \n",
      "[ 79/200] train_loss: 0.09058 valid_loss: 0.10254 test_loss: 0.11741 \n",
      "[ 80/200] train_loss: 0.09227 valid_loss: 0.10142 test_loss: 0.11529 \n",
      "Validation loss decreased (0.102413 --> 0.101416).  Saving model ...\n",
      "[ 81/200] train_loss: 0.09329 valid_loss: 0.10258 test_loss: 0.11581 \n",
      "[ 82/200] train_loss: 0.09073 valid_loss: 0.10402 test_loss: 0.11624 \n",
      "[ 83/200] train_loss: 0.09177 valid_loss: 0.10316 test_loss: 0.11380 \n",
      "[ 84/200] train_loss: 0.09274 valid_loss: 0.10473 test_loss: 0.11508 \n",
      "[ 85/200] train_loss: 0.08991 valid_loss: 0.10041 test_loss: 0.11509 \n",
      "Validation loss decreased (0.101416 --> 0.100406).  Saving model ...\n",
      "[ 86/200] train_loss: 0.09146 valid_loss: 0.09796 test_loss: 0.11353 \n",
      "Validation loss decreased (0.100406 --> 0.097959).  Saving model ...\n",
      "[ 87/200] train_loss: 0.08910 valid_loss: 0.09733 test_loss: 0.11387 \n",
      "Validation loss decreased (0.097959 --> 0.097334).  Saving model ...\n",
      "[ 88/200] train_loss: 0.08962 valid_loss: 0.10464 test_loss: 0.11283 \n",
      "[ 89/200] train_loss: 0.09099 valid_loss: 0.09759 test_loss: 0.11191 \n",
      "[ 90/200] train_loss: 0.08987 valid_loss: 0.09769 test_loss: 0.11108 \n",
      "[ 91/200] train_loss: 0.08921 valid_loss: 0.10124 test_loss: 0.11297 \n",
      "[ 92/200] train_loss: 0.09144 valid_loss: 0.09967 test_loss: 0.11209 \n",
      "[ 93/200] train_loss: 0.09046 valid_loss: 0.10104 test_loss: 0.11138 \n",
      "[ 94/200] train_loss: 0.08709 valid_loss: 0.10075 test_loss: 0.11316 \n",
      "[ 95/200] train_loss: 0.08798 valid_loss: 0.09700 test_loss: 0.11124 \n",
      "Validation loss decreased (0.097334 --> 0.097002).  Saving model ...\n",
      "[ 96/200] train_loss: 0.09117 valid_loss: 0.09792 test_loss: 0.11128 \n",
      "[ 97/200] train_loss: 0.08942 valid_loss: 0.09995 test_loss: 0.11159 \n",
      "[ 98/200] train_loss: 0.09012 valid_loss: 0.10163 test_loss: 0.11224 \n",
      "[ 99/200] train_loss: 0.08744 valid_loss: 0.09828 test_loss: 0.11116 \n",
      "[100/200] train_loss: 0.08735 valid_loss: 0.09831 test_loss: 0.11140 \n",
      "[101/200] train_loss: 0.08757 valid_loss: 0.09835 test_loss: 0.11167 \n",
      "[102/200] train_loss: 0.08860 valid_loss: 0.09950 test_loss: 0.11077 \n",
      "[103/200] train_loss: 0.08748 valid_loss: 0.09633 test_loss: 0.11136 \n",
      "Validation loss decreased (0.097002 --> 0.096326).  Saving model ...\n",
      "[104/200] train_loss: 0.08640 valid_loss: 0.09540 test_loss: 0.10921 \n",
      "Validation loss decreased (0.096326 --> 0.095395).  Saving model ...\n",
      "[105/200] train_loss: 0.08725 valid_loss: 0.09486 test_loss: 0.10913 \n",
      "Validation loss decreased (0.095395 --> 0.094865).  Saving model ...\n",
      "[106/200] train_loss: 0.08704 valid_loss: 0.09563 test_loss: 0.10997 \n",
      "[107/200] train_loss: 0.08674 valid_loss: 0.09688 test_loss: 0.10988 \n",
      "[108/200] train_loss: 0.08689 valid_loss: 0.09569 test_loss: 0.10878 \n",
      "[109/200] train_loss: 0.08536 valid_loss: 0.09937 test_loss: 0.10856 \n",
      "[110/200] train_loss: 0.08634 valid_loss: 0.09733 test_loss: 0.10887 \n",
      "[111/200] train_loss: 0.08685 valid_loss: 0.09472 test_loss: 0.10837 \n",
      "Validation loss decreased (0.094865 --> 0.094724).  Saving model ...\n",
      "[112/200] train_loss: 0.08540 valid_loss: 0.09701 test_loss: 0.11165 \n",
      "[113/200] train_loss: 0.08838 valid_loss: 0.09620 test_loss: 0.10784 \n",
      "[114/200] train_loss: 0.08688 valid_loss: 0.09468 test_loss: 0.10845 \n",
      "Validation loss decreased (0.094724 --> 0.094679).  Saving model ...\n",
      "[115/200] train_loss: 0.08556 valid_loss: 0.09514 test_loss: 0.10800 \n",
      "[116/200] train_loss: 0.08400 valid_loss: 0.09753 test_loss: 0.10792 \n",
      "[117/200] train_loss: 0.08687 valid_loss: 0.09637 test_loss: 0.10753 \n",
      "[118/200] train_loss: 0.08589 valid_loss: 0.09618 test_loss: 0.10712 \n",
      "[119/200] train_loss: 0.08375 valid_loss: 0.09552 test_loss: 0.10719 \n",
      "[120/200] train_loss: 0.08335 valid_loss: 0.09434 test_loss: 0.10689 \n",
      "Validation loss decreased (0.094679 --> 0.094339).  Saving model ...\n",
      "[121/200] train_loss: 0.08244 valid_loss: 0.09516 test_loss: 0.10743 \n",
      "[122/200] train_loss: 0.08314 valid_loss: 0.09325 test_loss: 0.10668 \n",
      "Validation loss decreased (0.094339 --> 0.093249).  Saving model ...\n",
      "[123/200] train_loss: 0.08512 valid_loss: 0.10475 test_loss: 0.11034 \n",
      "[124/200] train_loss: 0.08460 valid_loss: 0.09560 test_loss: 0.10745 \n",
      "[125/200] train_loss: 0.08507 valid_loss: 0.09741 test_loss: 0.10861 \n",
      "[126/200] train_loss: 0.08349 valid_loss: 0.09168 test_loss: 0.10562 \n",
      "Validation loss decreased (0.093249 --> 0.091681).  Saving model ...\n",
      "[127/200] train_loss: 0.08530 valid_loss: 0.09106 test_loss: 0.10494 \n",
      "Validation loss decreased (0.091681 --> 0.091059).  Saving model ...\n",
      "[128/200] train_loss: 0.08376 valid_loss: 0.09384 test_loss: 0.10619 \n",
      "[129/200] train_loss: 0.08202 valid_loss: 0.09283 test_loss: 0.10589 \n",
      "[130/200] train_loss: 0.08158 valid_loss: 0.09331 test_loss: 0.10706 \n",
      "[131/200] train_loss: 0.08280 valid_loss: 0.09226 test_loss: 0.10558 \n",
      "[132/200] train_loss: 0.08236 valid_loss: 0.09321 test_loss: 0.10492 \n",
      "[133/200] train_loss: 0.08104 valid_loss: 0.09290 test_loss: 0.10513 \n",
      "[134/200] train_loss: 0.08096 valid_loss: 0.09326 test_loss: 0.10467 \n",
      "[135/200] train_loss: 0.08163 valid_loss: 0.09262 test_loss: 0.10538 \n",
      "[136/200] train_loss: 0.08334 valid_loss: 0.09433 test_loss: 0.10575 \n",
      "[137/200] train_loss: 0.08208 valid_loss: 0.09235 test_loss: 0.10382 \n",
      "[138/200] train_loss: 0.08175 valid_loss: 0.09382 test_loss: 0.10544 \n",
      "[139/200] train_loss: 0.08122 valid_loss: 0.09514 test_loss: 0.10601 \n",
      "[140/200] train_loss: 0.08209 valid_loss: 0.09443 test_loss: 0.10512 \n",
      "[141/200] train_loss: 0.08027 valid_loss: 0.09240 test_loss: 0.10601 \n",
      "[142/200] train_loss: 0.07846 valid_loss: 0.09471 test_loss: 0.10498 \n",
      "[143/200] train_loss: 0.08133 valid_loss: 0.09161 test_loss: 0.10474 \n",
      "[144/200] train_loss: 0.07994 valid_loss: 0.09431 test_loss: 0.10553 \n",
      "[145/200] train_loss: 0.08070 valid_loss: 0.09015 test_loss: 0.10308 \n",
      "Validation loss decreased (0.091059 --> 0.090147).  Saving model ...\n",
      "[146/200] train_loss: 0.07870 valid_loss: 0.09155 test_loss: 0.10561 \n",
      "[147/200] train_loss: 0.08057 valid_loss: 0.09216 test_loss: 0.10468 \n",
      "[148/200] train_loss: 0.07933 valid_loss: 0.09313 test_loss: 0.10473 \n",
      "[149/200] train_loss: 0.07915 valid_loss: 0.09162 test_loss: 0.10564 \n",
      "[150/200] train_loss: 0.08060 valid_loss: 0.09197 test_loss: 0.10450 \n",
      "[151/200] train_loss: 0.08178 valid_loss: 0.09154 test_loss: 0.10501 \n",
      "[152/200] train_loss: 0.07852 valid_loss: 0.09214 test_loss: 0.10402 \n",
      "[153/200] train_loss: 0.07731 valid_loss: 0.09129 test_loss: 0.10426 \n",
      "[154/200] train_loss: 0.07818 valid_loss: 0.09213 test_loss: 0.10325 \n",
      "[155/200] train_loss: 0.07904 valid_loss: 0.09174 test_loss: 0.10561 \n",
      "[156/200] train_loss: 0.07928 valid_loss: 0.09065 test_loss: 0.10374 \n",
      "[157/200] train_loss: 0.08044 valid_loss: 0.08960 test_loss: 0.10244 \n",
      "Validation loss decreased (0.090147 --> 0.089600).  Saving model ...\n",
      "[158/200] train_loss: 0.07672 valid_loss: 0.09105 test_loss: 0.10235 \n",
      "[159/200] train_loss: 0.07809 valid_loss: 0.09070 test_loss: 0.10333 \n",
      "[160/200] train_loss: 0.08105 valid_loss: 0.09528 test_loss: 0.10274 \n",
      "[161/200] train_loss: 0.07929 valid_loss: 0.09232 test_loss: 0.10318 \n",
      "[162/200] train_loss: 0.08029 valid_loss: 0.09179 test_loss: 0.10302 \n",
      "[163/200] train_loss: 0.07957 valid_loss: 0.09098 test_loss: 0.10179 \n",
      "[164/200] train_loss: 0.07898 valid_loss: 0.09156 test_loss: 0.10164 \n",
      "[165/200] train_loss: 0.08143 valid_loss: 0.09157 test_loss: 0.10308 \n",
      "[166/200] train_loss: 0.07951 valid_loss: 0.09020 test_loss: 0.10199 \n",
      "[167/200] train_loss: 0.07848 valid_loss: 0.08902 test_loss: 0.10074 \n",
      "Validation loss decreased (0.089600 --> 0.089017).  Saving model ...\n",
      "[168/200] train_loss: 0.07949 valid_loss: 0.08789 test_loss: 0.10013 \n",
      "Validation loss decreased (0.089017 --> 0.087891).  Saving model ...\n",
      "[169/200] train_loss: 0.07937 valid_loss: 0.09170 test_loss: 0.10354 \n",
      "[170/200] train_loss: 0.07692 valid_loss: 0.09663 test_loss: 0.10070 \n",
      "[171/200] train_loss: 0.07698 valid_loss: 0.08999 test_loss: 0.10193 \n",
      "[172/200] train_loss: 0.07815 valid_loss: 0.08937 test_loss: 0.10190 \n",
      "[173/200] train_loss: 0.07917 valid_loss: 0.08814 test_loss: 0.10003 \n",
      "[174/200] train_loss: 0.07681 valid_loss: 0.09090 test_loss: 0.10264 \n",
      "[175/200] train_loss: 0.07890 valid_loss: 0.09026 test_loss: 0.10082 \n",
      "[176/200] train_loss: 0.08113 valid_loss: 0.08923 test_loss: 0.10115 \n",
      "[177/200] train_loss: 0.07825 valid_loss: 0.08887 test_loss: 0.10033 \n",
      "[178/200] train_loss: 0.07752 valid_loss: 0.09097 test_loss: 0.10208 \n",
      "[179/200] train_loss: 0.07769 valid_loss: 0.08920 test_loss: 0.09993 \n",
      "[180/200] train_loss: 0.07804 valid_loss: 0.08913 test_loss: 0.10084 \n",
      "[181/200] train_loss: 0.07850 valid_loss: 0.08693 test_loss: 0.09967 \n",
      "Validation loss decreased (0.087891 --> 0.086929).  Saving model ...\n",
      "[182/200] train_loss: 0.07796 valid_loss: 0.08849 test_loss: 0.10039 \n",
      "[183/200] train_loss: 0.07662 valid_loss: 0.09128 test_loss: 0.10023 \n",
      "[184/200] train_loss: 0.07746 valid_loss: 0.08688 test_loss: 0.09995 \n",
      "Validation loss decreased (0.086929 --> 0.086885).  Saving model ...\n",
      "[185/200] train_loss: 0.07795 valid_loss: 0.08972 test_loss: 0.10095 \n",
      "[186/200] train_loss: 0.07614 valid_loss: 0.09048 test_loss: 0.10075 \n",
      "[187/200] train_loss: 0.07600 valid_loss: 0.08697 test_loss: 0.09903 \n",
      "[188/200] train_loss: 0.07622 valid_loss: 0.08704 test_loss: 0.10074 \n",
      "[189/200] train_loss: 0.07534 valid_loss: 0.08758 test_loss: 0.10049 \n",
      "[190/200] train_loss: 0.07708 valid_loss: 0.08695 test_loss: 0.09958 \n",
      "[191/200] train_loss: 0.07765 valid_loss: 0.08602 test_loss: 0.09936 \n",
      "Validation loss decreased (0.086885 --> 0.086015).  Saving model ...\n",
      "[192/200] train_loss: 0.07512 valid_loss: 0.08695 test_loss: 0.09963 \n",
      "[193/200] train_loss: 0.07403 valid_loss: 0.09269 test_loss: 0.09952 \n",
      "[194/200] train_loss: 0.07637 valid_loss: 0.08831 test_loss: 0.10022 \n",
      "[195/200] train_loss: 0.07506 valid_loss: 0.08807 test_loss: 0.10045 \n",
      "[196/200] train_loss: 0.07591 valid_loss: 0.08616 test_loss: 0.09797 \n",
      "[197/200] train_loss: 0.07614 valid_loss: 0.08833 test_loss: 0.09864 \n",
      "[198/200] train_loss: 0.07255 valid_loss: 0.08702 test_loss: 0.09830 \n",
      "[199/200] train_loss: 0.07333 valid_loss: 0.08559 test_loss: 0.09815 \n",
      "Validation loss decreased (0.086015 --> 0.085594).  Saving model ...\n",
      "[200/200] train_loss: 0.07422 valid_loss: 0.08736 test_loss: 0.09939 \n",
      "Model 5 trained for seen data.\n",
      "TRAINING MODEL for seen house 6\n",
      "[  1/200] train_loss: 0.50959 valid_loss: 0.39904 test_loss: 0.40252 \n",
      "Validation loss decreased (inf --> 0.399040).  Saving model ...\n",
      "[  2/200] train_loss: 0.33156 valid_loss: 0.30587 test_loss: 0.32208 \n",
      "Validation loss decreased (0.399040 --> 0.305871).  Saving model ...\n",
      "[  3/200] train_loss: 0.26097 valid_loss: 0.25575 test_loss: 0.27592 \n",
      "Validation loss decreased (0.305871 --> 0.255753).  Saving model ...\n",
      "[  4/200] train_loss: 0.22594 valid_loss: 0.22616 test_loss: 0.24549 \n",
      "Validation loss decreased (0.255753 --> 0.226157).  Saving model ...\n",
      "[  5/200] train_loss: 0.20422 valid_loss: 0.20418 test_loss: 0.22275 \n",
      "Validation loss decreased (0.226157 --> 0.204177).  Saving model ...\n",
      "[  6/200] train_loss: 0.18542 valid_loss: 0.19017 test_loss: 0.20567 \n",
      "Validation loss decreased (0.204177 --> 0.190174).  Saving model ...\n",
      "[  7/200] train_loss: 0.17825 valid_loss: 0.17797 test_loss: 0.19284 \n",
      "Validation loss decreased (0.190174 --> 0.177966).  Saving model ...\n",
      "[  8/200] train_loss: 0.16496 valid_loss: 0.17404 test_loss: 0.18838 \n",
      "Validation loss decreased (0.177966 --> 0.174038).  Saving model ...\n",
      "[  9/200] train_loss: 0.15792 valid_loss: 0.16690 test_loss: 0.18029 \n",
      "Validation loss decreased (0.174038 --> 0.166899).  Saving model ...\n",
      "[ 10/200] train_loss: 0.15302 valid_loss: 0.16450 test_loss: 0.17811 \n",
      "Validation loss decreased (0.166899 --> 0.164502).  Saving model ...\n",
      "[ 11/200] train_loss: 0.14876 valid_loss: 0.15753 test_loss: 0.16979 \n",
      "Validation loss decreased (0.164502 --> 0.157530).  Saving model ...\n",
      "[ 12/200] train_loss: 0.14423 valid_loss: 0.15494 test_loss: 0.16565 \n",
      "Validation loss decreased (0.157530 --> 0.154943).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14160 valid_loss: 0.15868 test_loss: 0.16901 \n",
      "[ 14/200] train_loss: 0.13942 valid_loss: 0.14734 test_loss: 0.16049 \n",
      "Validation loss decreased (0.154943 --> 0.147344).  Saving model ...\n",
      "[ 15/200] train_loss: 0.14068 valid_loss: 0.14611 test_loss: 0.15809 \n",
      "Validation loss decreased (0.147344 --> 0.146111).  Saving model ...\n",
      "[ 16/200] train_loss: 0.13450 valid_loss: 0.14333 test_loss: 0.15681 \n",
      "Validation loss decreased (0.146111 --> 0.143333).  Saving model ...\n",
      "[ 17/200] train_loss: 0.13588 valid_loss: 0.14095 test_loss: 0.15673 \n",
      "Validation loss decreased (0.143333 --> 0.140952).  Saving model ...\n",
      "[ 18/200] train_loss: 0.12752 valid_loss: 0.14103 test_loss: 0.15219 \n",
      "[ 19/200] train_loss: 0.12978 valid_loss: 0.13733 test_loss: 0.15422 \n",
      "Validation loss decreased (0.140952 --> 0.137326).  Saving model ...\n",
      "[ 20/200] train_loss: 0.12618 valid_loss: 0.13346 test_loss: 0.14886 \n",
      "Validation loss decreased (0.137326 --> 0.133461).  Saving model ...\n",
      "[ 21/200] train_loss: 0.12643 valid_loss: 0.13295 test_loss: 0.14830 \n",
      "Validation loss decreased (0.133461 --> 0.132952).  Saving model ...\n",
      "[ 22/200] train_loss: 0.12390 valid_loss: 0.12967 test_loss: 0.14617 \n",
      "Validation loss decreased (0.132952 --> 0.129668).  Saving model ...\n",
      "[ 23/200] train_loss: 0.12070 valid_loss: 0.13172 test_loss: 0.14701 \n",
      "[ 24/200] train_loss: 0.11897 valid_loss: 0.13053 test_loss: 0.14371 \n",
      "[ 25/200] train_loss: 0.12059 valid_loss: 0.12689 test_loss: 0.14238 \n",
      "Validation loss decreased (0.129668 --> 0.126887).  Saving model ...\n",
      "[ 26/200] train_loss: 0.12159 valid_loss: 0.12766 test_loss: 0.14323 \n",
      "[ 27/200] train_loss: 0.11920 valid_loss: 0.12729 test_loss: 0.14075 \n",
      "[ 28/200] train_loss: 0.11699 valid_loss: 0.12393 test_loss: 0.14042 \n",
      "Validation loss decreased (0.126887 --> 0.123926).  Saving model ...\n",
      "[ 29/200] train_loss: 0.11497 valid_loss: 0.12338 test_loss: 0.13916 \n",
      "Validation loss decreased (0.123926 --> 0.123379).  Saving model ...\n",
      "[ 30/200] train_loss: 0.11639 valid_loss: 0.12532 test_loss: 0.13952 \n",
      "[ 31/200] train_loss: 0.11724 valid_loss: 0.12270 test_loss: 0.13787 \n",
      "Validation loss decreased (0.123379 --> 0.122704).  Saving model ...\n",
      "[ 32/200] train_loss: 0.11513 valid_loss: 0.12086 test_loss: 0.13725 \n",
      "Validation loss decreased (0.122704 --> 0.120861).  Saving model ...\n",
      "[ 33/200] train_loss: 0.11459 valid_loss: 0.12272 test_loss: 0.13810 \n",
      "[ 34/200] train_loss: 0.11270 valid_loss: 0.12375 test_loss: 0.13764 \n",
      "[ 35/200] train_loss: 0.11222 valid_loss: 0.11988 test_loss: 0.13340 \n",
      "Validation loss decreased (0.120861 --> 0.119879).  Saving model ...\n",
      "[ 36/200] train_loss: 0.11013 valid_loss: 0.11816 test_loss: 0.13374 \n",
      "Validation loss decreased (0.119879 --> 0.118159).  Saving model ...\n",
      "[ 37/200] train_loss: 0.10979 valid_loss: 0.11781 test_loss: 0.13465 \n",
      "Validation loss decreased (0.118159 --> 0.117813).  Saving model ...\n",
      "[ 38/200] train_loss: 0.10961 valid_loss: 0.11953 test_loss: 0.13470 \n",
      "[ 39/200] train_loss: 0.10887 valid_loss: 0.11631 test_loss: 0.13315 \n",
      "Validation loss decreased (0.117813 --> 0.116309).  Saving model ...\n",
      "[ 40/200] train_loss: 0.10935 valid_loss: 0.11861 test_loss: 0.13622 \n",
      "[ 41/200] train_loss: 0.10835 valid_loss: 0.11854 test_loss: 0.13316 \n",
      "[ 42/200] train_loss: 0.10849 valid_loss: 0.11834 test_loss: 0.13465 \n",
      "[ 43/200] train_loss: 0.10652 valid_loss: 0.11288 test_loss: 0.13048 \n",
      "Validation loss decreased (0.116309 --> 0.112877).  Saving model ...\n",
      "[ 44/200] train_loss: 0.10693 valid_loss: 0.11382 test_loss: 0.12985 \n",
      "[ 45/200] train_loss: 0.10693 valid_loss: 0.11820 test_loss: 0.12988 \n",
      "[ 46/200] train_loss: 0.10247 valid_loss: 0.11427 test_loss: 0.12722 \n",
      "[ 47/200] train_loss: 0.10458 valid_loss: 0.11279 test_loss: 0.12656 \n",
      "Validation loss decreased (0.112877 --> 0.112794).  Saving model ...\n",
      "[ 48/200] train_loss: 0.10558 valid_loss: 0.11459 test_loss: 0.12674 \n",
      "[ 49/200] train_loss: 0.10417 valid_loss: 0.11094 test_loss: 0.12659 \n",
      "Validation loss decreased (0.112794 --> 0.110937).  Saving model ...\n",
      "[ 50/200] train_loss: 0.10221 valid_loss: 0.11512 test_loss: 0.12686 \n",
      "[ 51/200] train_loss: 0.10260 valid_loss: 0.11241 test_loss: 0.12472 \n",
      "[ 52/200] train_loss: 0.10214 valid_loss: 0.10677 test_loss: 0.12289 \n",
      "Validation loss decreased (0.110937 --> 0.106769).  Saving model ...\n",
      "[ 53/200] train_loss: 0.10118 valid_loss: 0.11094 test_loss: 0.12454 \n",
      "[ 54/200] train_loss: 0.10246 valid_loss: 0.11465 test_loss: 0.12666 \n",
      "[ 55/200] train_loss: 0.10112 valid_loss: 0.11128 test_loss: 0.12342 \n",
      "[ 56/200] train_loss: 0.09760 valid_loss: 0.11009 test_loss: 0.12216 \n",
      "[ 57/200] train_loss: 0.10069 valid_loss: 0.11059 test_loss: 0.12329 \n",
      "[ 58/200] train_loss: 0.09989 valid_loss: 0.10851 test_loss: 0.12385 \n",
      "[ 59/200] train_loss: 0.10267 valid_loss: 0.10937 test_loss: 0.12232 \n",
      "[ 60/200] train_loss: 0.09876 valid_loss: 0.10939 test_loss: 0.12212 \n",
      "[ 61/200] train_loss: 0.09897 valid_loss: 0.10967 test_loss: 0.12295 \n",
      "[ 62/200] train_loss: 0.09601 valid_loss: 0.10770 test_loss: 0.12235 \n",
      "[ 63/200] train_loss: 0.09654 valid_loss: 0.11009 test_loss: 0.11895 \n",
      "[ 64/200] train_loss: 0.09972 valid_loss: 0.10939 test_loss: 0.11981 \n",
      "[ 65/200] train_loss: 0.09787 valid_loss: 0.10625 test_loss: 0.11915 \n",
      "Validation loss decreased (0.106769 --> 0.106245).  Saving model ...\n",
      "[ 66/200] train_loss: 0.09683 valid_loss: 0.10233 test_loss: 0.11749 \n",
      "Validation loss decreased (0.106245 --> 0.102326).  Saving model ...\n",
      "[ 67/200] train_loss: 0.09791 valid_loss: 0.10584 test_loss: 0.11970 \n",
      "[ 68/200] train_loss: 0.09669 valid_loss: 0.10619 test_loss: 0.11923 \n",
      "[ 69/200] train_loss: 0.09613 valid_loss: 0.10193 test_loss: 0.11953 \n",
      "Validation loss decreased (0.102326 --> 0.101929).  Saving model ...\n",
      "[ 70/200] train_loss: 0.09535 valid_loss: 0.10214 test_loss: 0.11849 \n",
      "[ 71/200] train_loss: 0.09464 valid_loss: 0.10367 test_loss: 0.11962 \n",
      "[ 72/200] train_loss: 0.09454 valid_loss: 0.10339 test_loss: 0.11899 \n",
      "[ 73/200] train_loss: 0.09623 valid_loss: 0.10415 test_loss: 0.11721 \n",
      "[ 74/200] train_loss: 0.09541 valid_loss: 0.10090 test_loss: 0.11664 \n",
      "Validation loss decreased (0.101929 --> 0.100903).  Saving model ...\n",
      "[ 75/200] train_loss: 0.09595 valid_loss: 0.10056 test_loss: 0.11491 \n",
      "Validation loss decreased (0.100903 --> 0.100556).  Saving model ...\n",
      "[ 76/200] train_loss: 0.09437 valid_loss: 0.10759 test_loss: 0.11936 \n",
      "[ 77/200] train_loss: 0.09329 valid_loss: 0.10650 test_loss: 0.11704 \n",
      "[ 78/200] train_loss: 0.09183 valid_loss: 0.10000 test_loss: 0.11540 \n",
      "Validation loss decreased (0.100556 --> 0.100003).  Saving model ...\n",
      "[ 79/200] train_loss: 0.09265 valid_loss: 0.10080 test_loss: 0.11434 \n",
      "[ 80/200] train_loss: 0.09201 valid_loss: 0.10074 test_loss: 0.11519 \n",
      "[ 81/200] train_loss: 0.09232 valid_loss: 0.10097 test_loss: 0.11358 \n",
      "[ 82/200] train_loss: 0.09406 valid_loss: 0.10073 test_loss: 0.11283 \n",
      "[ 83/200] train_loss: 0.09102 valid_loss: 0.10314 test_loss: 0.11314 \n",
      "[ 84/200] train_loss: 0.09476 valid_loss: 0.10478 test_loss: 0.11448 \n",
      "[ 85/200] train_loss: 0.09222 valid_loss: 0.09916 test_loss: 0.11339 \n",
      "Validation loss decreased (0.100003 --> 0.099160).  Saving model ...\n",
      "[ 86/200] train_loss: 0.08975 valid_loss: 0.09777 test_loss: 0.11355 \n",
      "Validation loss decreased (0.099160 --> 0.097769).  Saving model ...\n",
      "[ 87/200] train_loss: 0.08926 valid_loss: 0.09803 test_loss: 0.11337 \n",
      "[ 88/200] train_loss: 0.09322 valid_loss: 0.09796 test_loss: 0.11409 \n",
      "[ 89/200] train_loss: 0.09072 valid_loss: 0.09825 test_loss: 0.11302 \n",
      "[ 90/200] train_loss: 0.09251 valid_loss: 0.09680 test_loss: 0.11148 \n",
      "Validation loss decreased (0.097769 --> 0.096796).  Saving model ...\n",
      "[ 91/200] train_loss: 0.09121 valid_loss: 0.09697 test_loss: 0.11185 \n",
      "[ 92/200] train_loss: 0.09153 valid_loss: 0.09604 test_loss: 0.11010 \n",
      "Validation loss decreased (0.096796 --> 0.096036).  Saving model ...\n",
      "[ 93/200] train_loss: 0.08730 valid_loss: 0.09679 test_loss: 0.11223 \n",
      "[ 94/200] train_loss: 0.09048 valid_loss: 0.09724 test_loss: 0.11065 \n",
      "[ 95/200] train_loss: 0.09027 valid_loss: 0.09676 test_loss: 0.11006 \n",
      "[ 96/200] train_loss: 0.08818 valid_loss: 0.09885 test_loss: 0.10999 \n",
      "[ 97/200] train_loss: 0.08752 valid_loss: 0.09393 test_loss: 0.10953 \n",
      "Validation loss decreased (0.096036 --> 0.093929).  Saving model ...\n",
      "[ 98/200] train_loss: 0.08718 valid_loss: 0.09611 test_loss: 0.11202 \n",
      "[ 99/200] train_loss: 0.08626 valid_loss: 0.09578 test_loss: 0.11067 \n",
      "[100/200] train_loss: 0.08723 valid_loss: 0.09502 test_loss: 0.10935 \n",
      "[101/200] train_loss: 0.08859 valid_loss: 0.09521 test_loss: 0.11010 \n",
      "[102/200] train_loss: 0.08773 valid_loss: 0.09265 test_loss: 0.10799 \n",
      "Validation loss decreased (0.093929 --> 0.092650).  Saving model ...\n",
      "[103/200] train_loss: 0.08708 valid_loss: 0.09327 test_loss: 0.10905 \n",
      "[104/200] train_loss: 0.08645 valid_loss: 0.09389 test_loss: 0.10776 \n",
      "[105/200] train_loss: 0.08610 valid_loss: 0.09324 test_loss: 0.10917 \n",
      "[106/200] train_loss: 0.08477 valid_loss: 0.09392 test_loss: 0.10943 \n",
      "[107/200] train_loss: 0.08638 valid_loss: 0.09336 test_loss: 0.10847 \n",
      "[108/200] train_loss: 0.08746 valid_loss: 0.09427 test_loss: 0.11037 \n",
      "[109/200] train_loss: 0.08611 valid_loss: 0.09173 test_loss: 0.10828 \n",
      "Validation loss decreased (0.092650 --> 0.091731).  Saving model ...\n",
      "[110/200] train_loss: 0.08422 valid_loss: 0.09428 test_loss: 0.11055 \n",
      "[111/200] train_loss: 0.08758 valid_loss: 0.09466 test_loss: 0.10912 \n",
      "[112/200] train_loss: 0.08659 valid_loss: 0.09192 test_loss: 0.10711 \n",
      "[113/200] train_loss: 0.08619 valid_loss: 0.09154 test_loss: 0.10693 \n",
      "Validation loss decreased (0.091731 --> 0.091542).  Saving model ...\n",
      "[114/200] train_loss: 0.08597 valid_loss: 0.09535 test_loss: 0.10728 \n",
      "[115/200] train_loss: 0.08394 valid_loss: 0.09442 test_loss: 0.10791 \n",
      "[116/200] train_loss: 0.08526 valid_loss: 0.09524 test_loss: 0.10631 \n",
      "[117/200] train_loss: 0.08673 valid_loss: 0.09392 test_loss: 0.10715 \n",
      "[118/200] train_loss: 0.08612 valid_loss: 0.09251 test_loss: 0.10577 \n",
      "[119/200] train_loss: 0.08397 valid_loss: 0.09150 test_loss: 0.10597 \n",
      "Validation loss decreased (0.091542 --> 0.091504).  Saving model ...\n",
      "[120/200] train_loss: 0.08655 valid_loss: 0.09229 test_loss: 0.10681 \n",
      "[121/200] train_loss: 0.08552 valid_loss: 0.09115 test_loss: 0.10583 \n",
      "Validation loss decreased (0.091504 --> 0.091148).  Saving model ...\n",
      "[122/200] train_loss: 0.08271 valid_loss: 0.09304 test_loss: 0.10672 \n",
      "[123/200] train_loss: 0.08157 valid_loss: 0.09302 test_loss: 0.10710 \n",
      "[124/200] train_loss: 0.08595 valid_loss: 0.09177 test_loss: 0.10574 \n",
      "[125/200] train_loss: 0.08244 valid_loss: 0.09184 test_loss: 0.10596 \n",
      "[126/200] train_loss: 0.08498 valid_loss: 0.10046 test_loss: 0.10970 \n",
      "[127/200] train_loss: 0.08607 valid_loss: 0.09202 test_loss: 0.10541 \n",
      "[128/200] train_loss: 0.08264 valid_loss: 0.09318 test_loss: 0.10424 \n",
      "[129/200] train_loss: 0.08435 valid_loss: 0.08922 test_loss: 0.10258 \n",
      "Validation loss decreased (0.091148 --> 0.089218).  Saving model ...\n",
      "[130/200] train_loss: 0.08317 valid_loss: 0.09182 test_loss: 0.10431 \n",
      "[131/200] train_loss: 0.08230 valid_loss: 0.09223 test_loss: 0.10489 \n",
      "[132/200] train_loss: 0.08358 valid_loss: 0.08949 test_loss: 0.10449 \n",
      "[133/200] train_loss: 0.08348 valid_loss: 0.09088 test_loss: 0.10618 \n",
      "[134/200] train_loss: 0.08343 valid_loss: 0.09047 test_loss: 0.10452 \n",
      "[135/200] train_loss: 0.08300 valid_loss: 0.09022 test_loss: 0.10323 \n",
      "[136/200] train_loss: 0.07924 valid_loss: 0.08977 test_loss: 0.10389 \n",
      "[137/200] train_loss: 0.08157 valid_loss: 0.09008 test_loss: 0.10313 \n",
      "[138/200] train_loss: 0.08109 valid_loss: 0.08914 test_loss: 0.10259 \n",
      "Validation loss decreased (0.089218 --> 0.089141).  Saving model ...\n",
      "[139/200] train_loss: 0.08277 valid_loss: 0.08793 test_loss: 0.10267 \n",
      "Validation loss decreased (0.089141 --> 0.087933).  Saving model ...\n",
      "[140/200] train_loss: 0.08301 valid_loss: 0.08802 test_loss: 0.10183 \n",
      "[141/200] train_loss: 0.08120 valid_loss: 0.09092 test_loss: 0.10307 \n",
      "[142/200] train_loss: 0.08598 valid_loss: 0.09229 test_loss: 0.10360 \n",
      "[143/200] train_loss: 0.08090 valid_loss: 0.09015 test_loss: 0.10255 \n",
      "[144/200] train_loss: 0.07902 valid_loss: 0.08833 test_loss: 0.10170 \n",
      "[145/200] train_loss: 0.08009 valid_loss: 0.08852 test_loss: 0.10309 \n",
      "[146/200] train_loss: 0.08251 valid_loss: 0.08940 test_loss: 0.10206 \n",
      "[147/200] train_loss: 0.08005 valid_loss: 0.08884 test_loss: 0.10286 \n",
      "[148/200] train_loss: 0.08007 valid_loss: 0.08821 test_loss: 0.10196 \n",
      "[149/200] train_loss: 0.07942 valid_loss: 0.08822 test_loss: 0.10208 \n",
      "[150/200] train_loss: 0.08127 valid_loss: 0.08693 test_loss: 0.10113 \n",
      "Validation loss decreased (0.087933 --> 0.086931).  Saving model ...\n",
      "[151/200] train_loss: 0.07933 valid_loss: 0.08765 test_loss: 0.10223 \n",
      "[152/200] train_loss: 0.08141 valid_loss: 0.08915 test_loss: 0.10243 \n",
      "[153/200] train_loss: 0.07871 valid_loss: 0.08942 test_loss: 0.10280 \n",
      "[154/200] train_loss: 0.08109 valid_loss: 0.09232 test_loss: 0.10135 \n",
      "[155/200] train_loss: 0.08069 valid_loss: 0.08971 test_loss: 0.10156 \n",
      "[156/200] train_loss: 0.08096 valid_loss: 0.09122 test_loss: 0.10193 \n",
      "[157/200] train_loss: 0.07978 valid_loss: 0.08761 test_loss: 0.10118 \n",
      "[158/200] train_loss: 0.07694 valid_loss: 0.08781 test_loss: 0.10155 \n",
      "[159/200] train_loss: 0.07955 valid_loss: 0.08864 test_loss: 0.10198 \n",
      "[160/200] train_loss: 0.07779 valid_loss: 0.08823 test_loss: 0.10149 \n",
      "[161/200] train_loss: 0.08022 valid_loss: 0.08736 test_loss: 0.10102 \n",
      "[162/200] train_loss: 0.07973 valid_loss: 0.08732 test_loss: 0.10040 \n",
      "[163/200] train_loss: 0.07894 valid_loss: 0.08696 test_loss: 0.10160 \n",
      "[164/200] train_loss: 0.08019 valid_loss: 0.08713 test_loss: 0.10188 \n",
      "[165/200] train_loss: 0.07867 valid_loss: 0.08688 test_loss: 0.10085 \n",
      "Validation loss decreased (0.086931 --> 0.086880).  Saving model ...\n",
      "[166/200] train_loss: 0.07936 valid_loss: 0.08591 test_loss: 0.09904 \n",
      "Validation loss decreased (0.086880 --> 0.085905).  Saving model ...\n",
      "[167/200] train_loss: 0.07611 valid_loss: 0.08598 test_loss: 0.09982 \n",
      "[168/200] train_loss: 0.07434 valid_loss: 0.08744 test_loss: 0.10032 \n",
      "[169/200] train_loss: 0.07830 valid_loss: 0.09077 test_loss: 0.10078 \n",
      "[170/200] train_loss: 0.07835 valid_loss: 0.08806 test_loss: 0.09994 \n",
      "[171/200] train_loss: 0.07591 valid_loss: 0.08579 test_loss: 0.10024 \n",
      "Validation loss decreased (0.085905 --> 0.085794).  Saving model ...\n",
      "[172/200] train_loss: 0.07873 valid_loss: 0.08681 test_loss: 0.10022 \n",
      "[173/200] train_loss: 0.07855 valid_loss: 0.08673 test_loss: 0.10090 \n",
      "[174/200] train_loss: 0.07606 valid_loss: 0.08640 test_loss: 0.10077 \n",
      "[175/200] train_loss: 0.07833 valid_loss: 0.08635 test_loss: 0.09875 \n",
      "[176/200] train_loss: 0.07806 valid_loss: 0.08797 test_loss: 0.10053 \n",
      "[177/200] train_loss: 0.07637 valid_loss: 0.08471 test_loss: 0.09879 \n",
      "Validation loss decreased (0.085794 --> 0.084713).  Saving model ...\n",
      "[178/200] train_loss: 0.07681 valid_loss: 0.08912 test_loss: 0.09929 \n",
      "[179/200] train_loss: 0.07730 valid_loss: 0.08671 test_loss: 0.09847 \n",
      "[180/200] train_loss: 0.07816 valid_loss: 0.08706 test_loss: 0.09995 \n",
      "[181/200] train_loss: 0.07486 valid_loss: 0.08723 test_loss: 0.09969 \n",
      "[182/200] train_loss: 0.07775 valid_loss: 0.08624 test_loss: 0.10098 \n",
      "[183/200] train_loss: 0.07558 valid_loss: 0.08509 test_loss: 0.09981 \n",
      "[184/200] train_loss: 0.07389 valid_loss: 0.08706 test_loss: 0.10003 \n",
      "[185/200] train_loss: 0.07556 valid_loss: 0.08392 test_loss: 0.09813 \n",
      "Validation loss decreased (0.084713 --> 0.083923).  Saving model ...\n",
      "[186/200] train_loss: 0.07630 valid_loss: 0.08735 test_loss: 0.10029 \n",
      "[187/200] train_loss: 0.07627 valid_loss: 0.08585 test_loss: 0.09899 \n",
      "[188/200] train_loss: 0.07499 valid_loss: 0.08531 test_loss: 0.09835 \n",
      "[189/200] train_loss: 0.07491 valid_loss: 0.08527 test_loss: 0.09925 \n",
      "[190/200] train_loss: 0.07515 valid_loss: 0.08943 test_loss: 0.09996 \n",
      "[191/200] train_loss: 0.07935 valid_loss: 0.08678 test_loss: 0.09992 \n",
      "[192/200] train_loss: 0.07577 valid_loss: 0.08462 test_loss: 0.09823 \n",
      "[193/200] train_loss: 0.07321 valid_loss: 0.08538 test_loss: 0.09987 \n",
      "[194/200] train_loss: 0.07730 valid_loss: 0.08664 test_loss: 0.10022 \n",
      "[195/200] train_loss: 0.07308 valid_loss: 0.08396 test_loss: 0.09847 \n",
      "[196/200] train_loss: 0.07474 valid_loss: 0.08561 test_loss: 0.09827 \n",
      "[197/200] train_loss: 0.07322 valid_loss: 0.08598 test_loss: 0.09792 \n",
      "[198/200] train_loss: 0.07592 valid_loss: 0.08323 test_loss: 0.09749 \n",
      "Validation loss decreased (0.083923 --> 0.083234).  Saving model ...\n",
      "[199/200] train_loss: 0.07595 valid_loss: 0.08376 test_loss: 0.09735 \n",
      "[200/200] train_loss: 0.07370 valid_loss: 0.08607 test_loss: 0.09810 \n",
      "Model 6 trained for seen data.\n",
      "TRAINING MODEL for seen house 7\n",
      "[  1/200] train_loss: 0.61120 valid_loss: 0.51279 test_loss: 0.50123 \n",
      "Validation loss decreased (inf --> 0.512791).  Saving model ...\n",
      "[  2/200] train_loss: 0.41836 valid_loss: 0.36631 test_loss: 0.36644 \n",
      "Validation loss decreased (0.512791 --> 0.366315).  Saving model ...\n",
      "[  3/200] train_loss: 0.31602 valid_loss: 0.30269 test_loss: 0.30877 \n",
      "Validation loss decreased (0.366315 --> 0.302686).  Saving model ...\n",
      "[  4/200] train_loss: 0.26185 valid_loss: 0.25659 test_loss: 0.26522 \n",
      "Validation loss decreased (0.302686 --> 0.256593).  Saving model ...\n",
      "[  5/200] train_loss: 0.22661 valid_loss: 0.22974 test_loss: 0.24069 \n",
      "Validation loss decreased (0.256593 --> 0.229738).  Saving model ...\n",
      "[  6/200] train_loss: 0.20361 valid_loss: 0.20617 test_loss: 0.21692 \n",
      "Validation loss decreased (0.229738 --> 0.206167).  Saving model ...\n",
      "[  7/200] train_loss: 0.18820 valid_loss: 0.19010 test_loss: 0.20057 \n",
      "Validation loss decreased (0.206167 --> 0.190101).  Saving model ...\n",
      "[  8/200] train_loss: 0.17520 valid_loss: 0.18151 test_loss: 0.19169 \n",
      "Validation loss decreased (0.190101 --> 0.181506).  Saving model ...\n",
      "[  9/200] train_loss: 0.16762 valid_loss: 0.17741 test_loss: 0.18839 \n",
      "Validation loss decreased (0.181506 --> 0.177413).  Saving model ...\n",
      "[ 10/200] train_loss: 0.16444 valid_loss: 0.16750 test_loss: 0.18089 \n",
      "Validation loss decreased (0.177413 --> 0.167497).  Saving model ...\n",
      "[ 11/200] train_loss: 0.15410 valid_loss: 0.16757 test_loss: 0.17661 \n",
      "[ 12/200] train_loss: 0.15016 valid_loss: 0.16269 test_loss: 0.17189 \n",
      "Validation loss decreased (0.167497 --> 0.162691).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14416 valid_loss: 0.16021 test_loss: 0.17095 \n",
      "Validation loss decreased (0.162691 --> 0.160208).  Saving model ...\n",
      "[ 14/200] train_loss: 0.14341 valid_loss: 0.15733 test_loss: 0.16668 \n",
      "Validation loss decreased (0.160208 --> 0.157329).  Saving model ...\n",
      "[ 15/200] train_loss: 0.14505 valid_loss: 0.15395 test_loss: 0.16173 \n",
      "Validation loss decreased (0.157329 --> 0.153948).  Saving model ...\n",
      "[ 16/200] train_loss: 0.13712 valid_loss: 0.14693 test_loss: 0.15883 \n",
      "Validation loss decreased (0.153948 --> 0.146927).  Saving model ...\n",
      "[ 17/200] train_loss: 0.13520 valid_loss: 0.14756 test_loss: 0.16080 \n",
      "[ 18/200] train_loss: 0.13245 valid_loss: 0.14278 test_loss: 0.15624 \n",
      "Validation loss decreased (0.146927 --> 0.142782).  Saving model ...\n",
      "[ 19/200] train_loss: 0.13139 valid_loss: 0.14574 test_loss: 0.15466 \n",
      "[ 20/200] train_loss: 0.12996 valid_loss: 0.13917 test_loss: 0.15081 \n",
      "Validation loss decreased (0.142782 --> 0.139169).  Saving model ...\n",
      "[ 21/200] train_loss: 0.12879 valid_loss: 0.13671 test_loss: 0.15026 \n",
      "Validation loss decreased (0.139169 --> 0.136712).  Saving model ...\n",
      "[ 22/200] train_loss: 0.12757 valid_loss: 0.13834 test_loss: 0.15032 \n",
      "[ 23/200] train_loss: 0.12840 valid_loss: 0.13559 test_loss: 0.14803 \n",
      "Validation loss decreased (0.136712 --> 0.135595).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12174 valid_loss: 0.13408 test_loss: 0.14986 \n",
      "Validation loss decreased (0.135595 --> 0.134076).  Saving model ...\n",
      "[ 25/200] train_loss: 0.12019 valid_loss: 0.13152 test_loss: 0.14616 \n",
      "Validation loss decreased (0.134076 --> 0.131517).  Saving model ...\n",
      "[ 26/200] train_loss: 0.12179 valid_loss: 0.12763 test_loss: 0.14524 \n",
      "Validation loss decreased (0.131517 --> 0.127632).  Saving model ...\n",
      "[ 27/200] train_loss: 0.11673 valid_loss: 0.13042 test_loss: 0.14417 \n",
      "[ 28/200] train_loss: 0.11740 valid_loss: 0.12641 test_loss: 0.14130 \n",
      "Validation loss decreased (0.127632 --> 0.126405).  Saving model ...\n",
      "[ 29/200] train_loss: 0.11944 valid_loss: 0.13073 test_loss: 0.14270 \n",
      "[ 30/200] train_loss: 0.11602 valid_loss: 0.12868 test_loss: 0.14398 \n",
      "[ 31/200] train_loss: 0.11914 valid_loss: 0.12690 test_loss: 0.14101 \n",
      "[ 32/200] train_loss: 0.11522 valid_loss: 0.12396 test_loss: 0.13803 \n",
      "Validation loss decreased (0.126405 --> 0.123961).  Saving model ...\n",
      "[ 33/200] train_loss: 0.11476 valid_loss: 0.12161 test_loss: 0.13725 \n",
      "Validation loss decreased (0.123961 --> 0.121610).  Saving model ...\n",
      "[ 34/200] train_loss: 0.11266 valid_loss: 0.12383 test_loss: 0.13667 \n",
      "[ 35/200] train_loss: 0.11723 valid_loss: 0.12194 test_loss: 0.13797 \n",
      "[ 36/200] train_loss: 0.11574 valid_loss: 0.12525 test_loss: 0.13845 \n",
      "[ 37/200] train_loss: 0.10956 valid_loss: 0.12361 test_loss: 0.13658 \n",
      "[ 38/200] train_loss: 0.11259 valid_loss: 0.11939 test_loss: 0.13454 \n",
      "Validation loss decreased (0.121610 --> 0.119394).  Saving model ...\n",
      "[ 39/200] train_loss: 0.10839 valid_loss: 0.11885 test_loss: 0.13436 \n",
      "Validation loss decreased (0.119394 --> 0.118854).  Saving model ...\n",
      "[ 40/200] train_loss: 0.11124 valid_loss: 0.11874 test_loss: 0.13555 \n",
      "Validation loss decreased (0.118854 --> 0.118745).  Saving model ...\n",
      "[ 41/200] train_loss: 0.10752 valid_loss: 0.11987 test_loss: 0.13459 \n",
      "[ 42/200] train_loss: 0.11143 valid_loss: 0.12126 test_loss: 0.13520 \n",
      "[ 43/200] train_loss: 0.10721 valid_loss: 0.11577 test_loss: 0.13130 \n",
      "Validation loss decreased (0.118745 --> 0.115767).  Saving model ...\n",
      "[ 44/200] train_loss: 0.10641 valid_loss: 0.11544 test_loss: 0.13240 \n",
      "Validation loss decreased (0.115767 --> 0.115445).  Saving model ...\n",
      "[ 45/200] train_loss: 0.11087 valid_loss: 0.11987 test_loss: 0.13430 \n",
      "[ 46/200] train_loss: 0.10698 valid_loss: 0.11439 test_loss: 0.13029 \n",
      "Validation loss decreased (0.115445 --> 0.114392).  Saving model ...\n",
      "[ 47/200] train_loss: 0.10672 valid_loss: 0.11726 test_loss: 0.13126 \n",
      "[ 48/200] train_loss: 0.10582 valid_loss: 0.11837 test_loss: 0.13380 \n",
      "[ 49/200] train_loss: 0.10486 valid_loss: 0.11308 test_loss: 0.12870 \n",
      "Validation loss decreased (0.114392 --> 0.113081).  Saving model ...\n",
      "[ 50/200] train_loss: 0.10479 valid_loss: 0.11303 test_loss: 0.12909 \n",
      "Validation loss decreased (0.113081 --> 0.113030).  Saving model ...\n",
      "[ 51/200] train_loss: 0.10599 valid_loss: 0.11288 test_loss: 0.12909 \n",
      "Validation loss decreased (0.113030 --> 0.112878).  Saving model ...\n",
      "[ 52/200] train_loss: 0.10425 valid_loss: 0.11306 test_loss: 0.12712 \n",
      "[ 53/200] train_loss: 0.10055 valid_loss: 0.11633 test_loss: 0.13039 \n",
      "[ 54/200] train_loss: 0.10312 valid_loss: 0.11425 test_loss: 0.12745 \n",
      "[ 55/200] train_loss: 0.10215 valid_loss: 0.11415 test_loss: 0.12781 \n",
      "[ 56/200] train_loss: 0.10220 valid_loss: 0.11076 test_loss: 0.12620 \n",
      "Validation loss decreased (0.112878 --> 0.110756).  Saving model ...\n",
      "[ 57/200] train_loss: 0.10190 valid_loss: 0.10831 test_loss: 0.12427 \n",
      "Validation loss decreased (0.110756 --> 0.108310).  Saving model ...\n",
      "[ 58/200] train_loss: 0.09975 valid_loss: 0.11021 test_loss: 0.12505 \n",
      "[ 59/200] train_loss: 0.09984 valid_loss: 0.11085 test_loss: 0.12492 \n",
      "[ 60/200] train_loss: 0.09962 valid_loss: 0.11014 test_loss: 0.12496 \n",
      "[ 61/200] train_loss: 0.09905 valid_loss: 0.10890 test_loss: 0.12411 \n",
      "[ 62/200] train_loss: 0.10299 valid_loss: 0.11056 test_loss: 0.12406 \n",
      "[ 63/200] train_loss: 0.09889 valid_loss: 0.10605 test_loss: 0.12393 \n",
      "Validation loss decreased (0.108310 --> 0.106051).  Saving model ...\n",
      "[ 64/200] train_loss: 0.09936 valid_loss: 0.10963 test_loss: 0.12235 \n",
      "[ 65/200] train_loss: 0.09531 valid_loss: 0.10824 test_loss: 0.12266 \n",
      "[ 66/200] train_loss: 0.09718 valid_loss: 0.10667 test_loss: 0.12272 \n",
      "[ 67/200] train_loss: 0.10145 valid_loss: 0.10755 test_loss: 0.12167 \n",
      "[ 68/200] train_loss: 0.09577 valid_loss: 0.10788 test_loss: 0.12651 \n",
      "[ 69/200] train_loss: 0.09609 valid_loss: 0.10793 test_loss: 0.12143 \n",
      "[ 70/200] train_loss: 0.09675 valid_loss: 0.10566 test_loss: 0.12139 \n",
      "Validation loss decreased (0.106051 --> 0.105661).  Saving model ...\n",
      "[ 71/200] train_loss: 0.09568 valid_loss: 0.10352 test_loss: 0.11900 \n",
      "Validation loss decreased (0.105661 --> 0.103518).  Saving model ...\n",
      "[ 72/200] train_loss: 0.09607 valid_loss: 0.10503 test_loss: 0.12009 \n",
      "[ 73/200] train_loss: 0.09757 valid_loss: 0.10408 test_loss: 0.11882 \n",
      "[ 74/200] train_loss: 0.09452 valid_loss: 0.10444 test_loss: 0.11821 \n",
      "[ 75/200] train_loss: 0.09377 valid_loss: 0.10350 test_loss: 0.11818 \n",
      "Validation loss decreased (0.103518 --> 0.103505).  Saving model ...\n",
      "[ 76/200] train_loss: 0.09677 valid_loss: 0.10218 test_loss: 0.11761 \n",
      "Validation loss decreased (0.103505 --> 0.102177).  Saving model ...\n",
      "[ 77/200] train_loss: 0.09239 valid_loss: 0.10489 test_loss: 0.11958 \n",
      "[ 78/200] train_loss: 0.09463 valid_loss: 0.10406 test_loss: 0.11709 \n",
      "[ 79/200] train_loss: 0.09300 valid_loss: 0.10538 test_loss: 0.11631 \n",
      "[ 80/200] train_loss: 0.09573 valid_loss: 0.10298 test_loss: 0.11557 \n",
      "[ 81/200] train_loss: 0.09558 valid_loss: 0.10463 test_loss: 0.11625 \n",
      "[ 82/200] train_loss: 0.09179 valid_loss: 0.10139 test_loss: 0.11666 \n",
      "Validation loss decreased (0.102177 --> 0.101390).  Saving model ...\n",
      "[ 83/200] train_loss: 0.09201 valid_loss: 0.10261 test_loss: 0.11511 \n",
      "[ 84/200] train_loss: 0.09319 valid_loss: 0.10245 test_loss: 0.11577 \n",
      "[ 85/200] train_loss: 0.09013 valid_loss: 0.10302 test_loss: 0.11768 \n",
      "[ 86/200] train_loss: 0.09243 valid_loss: 0.10191 test_loss: 0.11449 \n",
      "[ 87/200] train_loss: 0.09194 valid_loss: 0.09874 test_loss: 0.11453 \n",
      "Validation loss decreased (0.101390 --> 0.098737).  Saving model ...\n",
      "[ 88/200] train_loss: 0.09437 valid_loss: 0.10316 test_loss: 0.11602 \n",
      "[ 89/200] train_loss: 0.09352 valid_loss: 0.09762 test_loss: 0.11194 \n",
      "Validation loss decreased (0.098737 --> 0.097618).  Saving model ...\n",
      "[ 90/200] train_loss: 0.09368 valid_loss: 0.09871 test_loss: 0.11285 \n",
      "[ 91/200] train_loss: 0.09131 valid_loss: 0.10160 test_loss: 0.11281 \n",
      "[ 92/200] train_loss: 0.08862 valid_loss: 0.09896 test_loss: 0.11294 \n",
      "[ 93/200] train_loss: 0.09224 valid_loss: 0.10221 test_loss: 0.11448 \n",
      "[ 94/200] train_loss: 0.09151 valid_loss: 0.09891 test_loss: 0.11360 \n",
      "[ 95/200] train_loss: 0.08967 valid_loss: 0.09643 test_loss: 0.11036 \n",
      "Validation loss decreased (0.097618 --> 0.096430).  Saving model ...\n",
      "[ 96/200] train_loss: 0.09092 valid_loss: 0.09743 test_loss: 0.11157 \n",
      "[ 97/200] train_loss: 0.08817 valid_loss: 0.09628 test_loss: 0.11120 \n",
      "Validation loss decreased (0.096430 --> 0.096283).  Saving model ...\n",
      "[ 98/200] train_loss: 0.08824 valid_loss: 0.09877 test_loss: 0.11262 \n",
      "[ 99/200] train_loss: 0.08905 valid_loss: 0.09549 test_loss: 0.11039 \n",
      "Validation loss decreased (0.096283 --> 0.095495).  Saving model ...\n",
      "[100/200] train_loss: 0.08776 valid_loss: 0.09704 test_loss: 0.11039 \n",
      "[101/200] train_loss: 0.08771 valid_loss: 0.09724 test_loss: 0.11200 \n",
      "[102/200] train_loss: 0.08767 valid_loss: 0.09554 test_loss: 0.10983 \n",
      "[103/200] train_loss: 0.08770 valid_loss: 0.09564 test_loss: 0.10977 \n",
      "[104/200] train_loss: 0.08832 valid_loss: 0.09524 test_loss: 0.11032 \n",
      "Validation loss decreased (0.095495 --> 0.095237).  Saving model ...\n",
      "[105/200] train_loss: 0.08899 valid_loss: 0.09510 test_loss: 0.10926 \n",
      "Validation loss decreased (0.095237 --> 0.095104).  Saving model ...\n",
      "[106/200] train_loss: 0.08708 valid_loss: 0.09421 test_loss: 0.10848 \n",
      "Validation loss decreased (0.095104 --> 0.094215).  Saving model ...\n",
      "[107/200] train_loss: 0.08816 valid_loss: 0.09733 test_loss: 0.10914 \n",
      "[108/200] train_loss: 0.08605 valid_loss: 0.09522 test_loss: 0.10969 \n",
      "[109/200] train_loss: 0.08560 valid_loss: 0.09513 test_loss: 0.10882 \n",
      "[110/200] train_loss: 0.08592 valid_loss: 0.09455 test_loss: 0.10925 \n",
      "[111/200] train_loss: 0.08476 valid_loss: 0.09441 test_loss: 0.10753 \n",
      "[112/200] train_loss: 0.08925 valid_loss: 0.09258 test_loss: 0.10718 \n",
      "Validation loss decreased (0.094215 --> 0.092576).  Saving model ...\n",
      "[113/200] train_loss: 0.08422 valid_loss: 0.09498 test_loss: 0.10902 \n",
      "[114/200] train_loss: 0.08408 valid_loss: 0.09468 test_loss: 0.10902 \n",
      "[115/200] train_loss: 0.08424 valid_loss: 0.09476 test_loss: 0.10758 \n",
      "[116/200] train_loss: 0.08705 valid_loss: 0.09398 test_loss: 0.10696 \n",
      "[117/200] train_loss: 0.08798 valid_loss: 0.09225 test_loss: 0.10768 \n",
      "Validation loss decreased (0.092576 --> 0.092251).  Saving model ...\n",
      "[118/200] train_loss: 0.08429 valid_loss: 0.09602 test_loss: 0.10671 \n",
      "[119/200] train_loss: 0.08312 valid_loss: 0.09315 test_loss: 0.10654 \n",
      "[120/200] train_loss: 0.08517 valid_loss: 0.09271 test_loss: 0.10579 \n",
      "[121/200] train_loss: 0.08618 valid_loss: 0.09209 test_loss: 0.10646 \n",
      "Validation loss decreased (0.092251 --> 0.092089).  Saving model ...\n",
      "[122/200] train_loss: 0.08287 valid_loss: 0.09288 test_loss: 0.10536 \n",
      "[123/200] train_loss: 0.08478 valid_loss: 0.08989 test_loss: 0.10450 \n",
      "Validation loss decreased (0.092089 --> 0.089894).  Saving model ...\n",
      "[124/200] train_loss: 0.08152 valid_loss: 0.09096 test_loss: 0.10474 \n",
      "[125/200] train_loss: 0.08775 valid_loss: 0.09164 test_loss: 0.10577 \n",
      "[126/200] train_loss: 0.08287 valid_loss: 0.09132 test_loss: 0.10528 \n",
      "[127/200] train_loss: 0.08463 valid_loss: 0.09292 test_loss: 0.10456 \n",
      "[128/200] train_loss: 0.08242 valid_loss: 0.09344 test_loss: 0.10498 \n",
      "[129/200] train_loss: 0.08281 valid_loss: 0.09016 test_loss: 0.10319 \n",
      "[130/200] train_loss: 0.08409 valid_loss: 0.09139 test_loss: 0.10451 \n",
      "[131/200] train_loss: 0.08542 valid_loss: 0.09092 test_loss: 0.10544 \n",
      "[132/200] train_loss: 0.08303 valid_loss: 0.09307 test_loss: 0.10462 \n",
      "[133/200] train_loss: 0.08250 valid_loss: 0.09146 test_loss: 0.10509 \n",
      "[134/200] train_loss: 0.08333 valid_loss: 0.09098 test_loss: 0.10449 \n",
      "[135/200] train_loss: 0.08521 valid_loss: 0.09233 test_loss: 0.10362 \n",
      "[136/200] train_loss: 0.08511 valid_loss: 0.09336 test_loss: 0.10567 \n",
      "[137/200] train_loss: 0.08499 valid_loss: 0.08946 test_loss: 0.10471 \n",
      "Validation loss decreased (0.089894 --> 0.089463).  Saving model ...\n",
      "[138/200] train_loss: 0.08184 valid_loss: 0.08982 test_loss: 0.10397 \n",
      "[139/200] train_loss: 0.07967 valid_loss: 0.09006 test_loss: 0.10498 \n",
      "[140/200] train_loss: 0.08116 valid_loss: 0.08938 test_loss: 0.10364 \n",
      "Validation loss decreased (0.089463 --> 0.089378).  Saving model ...\n",
      "[141/200] train_loss: 0.08085 valid_loss: 0.08990 test_loss: 0.10379 \n",
      "[142/200] train_loss: 0.07950 valid_loss: 0.08967 test_loss: 0.10242 \n",
      "[143/200] train_loss: 0.08225 valid_loss: 0.09070 test_loss: 0.10399 \n",
      "[144/200] train_loss: 0.08614 valid_loss: 0.08889 test_loss: 0.10321 \n",
      "Validation loss decreased (0.089378 --> 0.088892).  Saving model ...\n",
      "[145/200] train_loss: 0.08173 valid_loss: 0.09019 test_loss: 0.10406 \n",
      "[146/200] train_loss: 0.08024 valid_loss: 0.09151 test_loss: 0.10437 \n",
      "[147/200] train_loss: 0.08252 valid_loss: 0.09032 test_loss: 0.10220 \n",
      "[148/200] train_loss: 0.08034 valid_loss: 0.08837 test_loss: 0.10209 \n",
      "Validation loss decreased (0.088892 --> 0.088367).  Saving model ...\n",
      "[149/200] train_loss: 0.08085 valid_loss: 0.08812 test_loss: 0.10192 \n",
      "Validation loss decreased (0.088367 --> 0.088121).  Saving model ...\n",
      "[150/200] train_loss: 0.08091 valid_loss: 0.08933 test_loss: 0.10290 \n",
      "[151/200] train_loss: 0.07991 valid_loss: 0.08915 test_loss: 0.10109 \n",
      "[152/200] train_loss: 0.07984 valid_loss: 0.08812 test_loss: 0.10190 \n",
      "Validation loss decreased (0.088121 --> 0.088116).  Saving model ...\n",
      "[153/200] train_loss: 0.08107 valid_loss: 0.08808 test_loss: 0.10126 \n",
      "Validation loss decreased (0.088116 --> 0.088077).  Saving model ...\n",
      "[154/200] train_loss: 0.08220 valid_loss: 0.08828 test_loss: 0.10250 \n",
      "[155/200] train_loss: 0.08033 valid_loss: 0.08986 test_loss: 0.10217 \n",
      "[156/200] train_loss: 0.07948 valid_loss: 0.08839 test_loss: 0.10019 \n",
      "[157/200] train_loss: 0.07936 valid_loss: 0.09020 test_loss: 0.10053 \n",
      "[158/200] train_loss: 0.07987 valid_loss: 0.08947 test_loss: 0.10216 \n",
      "[159/200] train_loss: 0.08091 valid_loss: 0.08866 test_loss: 0.10095 \n",
      "[160/200] train_loss: 0.08303 valid_loss: 0.08802 test_loss: 0.10032 \n",
      "Validation loss decreased (0.088077 --> 0.088024).  Saving model ...\n",
      "[161/200] train_loss: 0.07946 valid_loss: 0.08716 test_loss: 0.09987 \n",
      "Validation loss decreased (0.088024 --> 0.087164).  Saving model ...\n",
      "[162/200] train_loss: 0.08082 valid_loss: 0.09268 test_loss: 0.09993 \n",
      "[163/200] train_loss: 0.07856 valid_loss: 0.09059 test_loss: 0.10148 \n",
      "[164/200] train_loss: 0.07943 valid_loss: 0.08896 test_loss: 0.10080 \n",
      "[165/200] train_loss: 0.07978 valid_loss: 0.08831 test_loss: 0.09965 \n",
      "[166/200] train_loss: 0.07945 valid_loss: 0.08799 test_loss: 0.10197 \n",
      "[167/200] train_loss: 0.07937 valid_loss: 0.08733 test_loss: 0.10068 \n",
      "[168/200] train_loss: 0.07825 valid_loss: 0.08907 test_loss: 0.10016 \n",
      "[169/200] train_loss: 0.07916 valid_loss: 0.08692 test_loss: 0.10035 \n",
      "Validation loss decreased (0.087164 --> 0.086919).  Saving model ...\n",
      "[170/200] train_loss: 0.07891 valid_loss: 0.08682 test_loss: 0.10008 \n",
      "Validation loss decreased (0.086919 --> 0.086821).  Saving model ...\n",
      "[171/200] train_loss: 0.07612 valid_loss: 0.08767 test_loss: 0.09994 \n",
      "[172/200] train_loss: 0.07806 valid_loss: 0.08635 test_loss: 0.10000 \n",
      "Validation loss decreased (0.086821 --> 0.086349).  Saving model ...\n",
      "[173/200] train_loss: 0.07809 valid_loss: 0.08776 test_loss: 0.09996 \n",
      "[174/200] train_loss: 0.07822 valid_loss: 0.08543 test_loss: 0.09969 \n",
      "Validation loss decreased (0.086349 --> 0.085429).  Saving model ...\n",
      "[175/200] train_loss: 0.07905 valid_loss: 0.08845 test_loss: 0.09872 \n",
      "[176/200] train_loss: 0.08054 valid_loss: 0.08667 test_loss: 0.09862 \n",
      "[177/200] train_loss: 0.07982 valid_loss: 0.08735 test_loss: 0.09834 \n",
      "[178/200] train_loss: 0.07806 valid_loss: 0.08748 test_loss: 0.09913 \n",
      "[179/200] train_loss: 0.07863 valid_loss: 0.08847 test_loss: 0.10005 \n",
      "[180/200] train_loss: 0.07689 valid_loss: 0.08695 test_loss: 0.09869 \n",
      "[181/200] train_loss: 0.07875 valid_loss: 0.08617 test_loss: 0.09984 \n",
      "[182/200] train_loss: 0.07651 valid_loss: 0.08594 test_loss: 0.09866 \n",
      "[183/200] train_loss: 0.07668 valid_loss: 0.08587 test_loss: 0.09836 \n",
      "[184/200] train_loss: 0.07629 valid_loss: 0.08592 test_loss: 0.09903 \n",
      "[185/200] train_loss: 0.07582 valid_loss: 0.08552 test_loss: 0.09799 \n",
      "[186/200] train_loss: 0.07670 valid_loss: 0.08625 test_loss: 0.09815 \n",
      "[187/200] train_loss: 0.07709 valid_loss: 0.08679 test_loss: 0.09805 \n",
      "[188/200] train_loss: 0.07603 valid_loss: 0.08795 test_loss: 0.10000 \n",
      "[189/200] train_loss: 0.07895 valid_loss: 0.08523 test_loss: 0.09932 \n",
      "Validation loss decreased (0.085429 --> 0.085226).  Saving model ...\n",
      "[190/200] train_loss: 0.07689 valid_loss: 0.08679 test_loss: 0.09840 \n",
      "[191/200] train_loss: 0.07686 valid_loss: 0.08553 test_loss: 0.09907 \n",
      "[192/200] train_loss: 0.07578 valid_loss: 0.08474 test_loss: 0.09764 \n",
      "Validation loss decreased (0.085226 --> 0.084744).  Saving model ...\n",
      "[193/200] train_loss: 0.07751 valid_loss: 0.08532 test_loss: 0.09716 \n",
      "[194/200] train_loss: 0.07560 valid_loss: 0.08628 test_loss: 0.09857 \n",
      "[195/200] train_loss: 0.07704 valid_loss: 0.08577 test_loss: 0.09776 \n",
      "[196/200] train_loss: 0.07669 valid_loss: 0.08492 test_loss: 0.09741 \n",
      "[197/200] train_loss: 0.07482 valid_loss: 0.08581 test_loss: 0.09819 \n",
      "[198/200] train_loss: 0.07700 valid_loss: 0.08607 test_loss: 0.09834 \n",
      "[199/200] train_loss: 0.07540 valid_loss: 0.08975 test_loss: 0.09922 \n",
      "[200/200] train_loss: 0.07726 valid_loss: 0.08456 test_loss: 0.09685 \n",
      "Validation loss decreased (0.084744 --> 0.084560).  Saving model ...\n",
      "Model 7 trained for seen data.\n",
      "TRAINING MODEL for seen house 8\n",
      "[  1/200] train_loss: 0.63133 valid_loss: 0.50424 test_loss: 0.50626 \n",
      "Validation loss decreased (inf --> 0.504245).  Saving model ...\n",
      "[  2/200] train_loss: 0.41114 valid_loss: 0.35560 test_loss: 0.36185 \n",
      "Validation loss decreased (0.504245 --> 0.355600).  Saving model ...\n",
      "[  3/200] train_loss: 0.31318 valid_loss: 0.30014 test_loss: 0.31121 \n",
      "Validation loss decreased (0.355600 --> 0.300140).  Saving model ...\n",
      "[  4/200] train_loss: 0.26435 valid_loss: 0.25948 test_loss: 0.27538 \n",
      "Validation loss decreased (0.300140 --> 0.259479).  Saving model ...\n",
      "[  5/200] train_loss: 0.23136 valid_loss: 0.23286 test_loss: 0.24789 \n",
      "Validation loss decreased (0.259479 --> 0.232856).  Saving model ...\n",
      "[  6/200] train_loss: 0.21057 valid_loss: 0.21042 test_loss: 0.22640 \n",
      "Validation loss decreased (0.232856 --> 0.210416).  Saving model ...\n",
      "[  7/200] train_loss: 0.19420 valid_loss: 0.19239 test_loss: 0.20733 \n",
      "Validation loss decreased (0.210416 --> 0.192392).  Saving model ...\n",
      "[  8/200] train_loss: 0.18190 valid_loss: 0.18181 test_loss: 0.19653 \n",
      "Validation loss decreased (0.192392 --> 0.181811).  Saving model ...\n",
      "[  9/200] train_loss: 0.17267 valid_loss: 0.17376 test_loss: 0.18876 \n",
      "Validation loss decreased (0.181811 --> 0.173758).  Saving model ...\n",
      "[ 10/200] train_loss: 0.16266 valid_loss: 0.16656 test_loss: 0.18066 \n",
      "Validation loss decreased (0.173758 --> 0.166560).  Saving model ...\n",
      "[ 11/200] train_loss: 0.15717 valid_loss: 0.16540 test_loss: 0.17926 \n",
      "Validation loss decreased (0.166560 --> 0.165396).  Saving model ...\n",
      "[ 12/200] train_loss: 0.15393 valid_loss: 0.15948 test_loss: 0.17217 \n",
      "Validation loss decreased (0.165396 --> 0.159480).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14799 valid_loss: 0.15394 test_loss: 0.16747 \n",
      "Validation loss decreased (0.159480 --> 0.153943).  Saving model ...\n",
      "[ 14/200] train_loss: 0.14785 valid_loss: 0.15226 test_loss: 0.16484 \n",
      "Validation loss decreased (0.153943 --> 0.152257).  Saving model ...\n",
      "[ 15/200] train_loss: 0.14354 valid_loss: 0.15172 test_loss: 0.16371 \n",
      "Validation loss decreased (0.152257 --> 0.151725).  Saving model ...\n",
      "[ 16/200] train_loss: 0.14013 valid_loss: 0.15050 test_loss: 0.16246 \n",
      "Validation loss decreased (0.151725 --> 0.150503).  Saving model ...\n",
      "[ 17/200] train_loss: 0.13898 valid_loss: 0.15143 test_loss: 0.15997 \n",
      "[ 18/200] train_loss: 0.13765 valid_loss: 0.14426 test_loss: 0.15702 \n",
      "Validation loss decreased (0.150503 --> 0.144258).  Saving model ...\n",
      "[ 19/200] train_loss: 0.13745 valid_loss: 0.14641 test_loss: 0.15489 \n",
      "[ 20/200] train_loss: 0.13389 valid_loss: 0.14422 test_loss: 0.15358 \n",
      "Validation loss decreased (0.144258 --> 0.144219).  Saving model ...\n",
      "[ 21/200] train_loss: 0.13049 valid_loss: 0.14417 test_loss: 0.15379 \n",
      "Validation loss decreased (0.144219 --> 0.144169).  Saving model ...\n",
      "[ 22/200] train_loss: 0.12826 valid_loss: 0.14285 test_loss: 0.15202 \n",
      "Validation loss decreased (0.144169 --> 0.142850).  Saving model ...\n",
      "[ 23/200] train_loss: 0.12876 valid_loss: 0.13777 test_loss: 0.14959 \n",
      "Validation loss decreased (0.142850 --> 0.137773).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12725 valid_loss: 0.13269 test_loss: 0.14934 \n",
      "Validation loss decreased (0.137773 --> 0.132691).  Saving model ...\n",
      "[ 25/200] train_loss: 0.12635 valid_loss: 0.14067 test_loss: 0.14828 \n",
      "[ 26/200] train_loss: 0.12395 valid_loss: 0.13348 test_loss: 0.14715 \n",
      "[ 27/200] train_loss: 0.12382 valid_loss: 0.13913 test_loss: 0.14834 \n",
      "[ 28/200] train_loss: 0.12235 valid_loss: 0.13186 test_loss: 0.14411 \n",
      "Validation loss decreased (0.132691 --> 0.131857).  Saving model ...\n",
      "[ 29/200] train_loss: 0.12419 valid_loss: 0.12951 test_loss: 0.14206 \n",
      "Validation loss decreased (0.131857 --> 0.129508).  Saving model ...\n",
      "[ 30/200] train_loss: 0.12134 valid_loss: 0.13189 test_loss: 0.14341 \n",
      "[ 31/200] train_loss: 0.12038 valid_loss: 0.12873 test_loss: 0.14323 \n",
      "Validation loss decreased (0.129508 --> 0.128727).  Saving model ...\n",
      "[ 32/200] train_loss: 0.11810 valid_loss: 0.12714 test_loss: 0.14254 \n",
      "Validation loss decreased (0.128727 --> 0.127137).  Saving model ...\n",
      "[ 33/200] train_loss: 0.11859 valid_loss: 0.12503 test_loss: 0.14294 \n",
      "Validation loss decreased (0.127137 --> 0.125033).  Saving model ...\n",
      "[ 34/200] train_loss: 0.11708 valid_loss: 0.12378 test_loss: 0.14115 \n",
      "Validation loss decreased (0.125033 --> 0.123781).  Saving model ...\n",
      "[ 35/200] train_loss: 0.11842 valid_loss: 0.12561 test_loss: 0.14132 \n",
      "[ 36/200] train_loss: 0.11623 valid_loss: 0.12941 test_loss: 0.14086 \n",
      "[ 37/200] train_loss: 0.11829 valid_loss: 0.12545 test_loss: 0.13835 \n",
      "[ 38/200] train_loss: 0.11296 valid_loss: 0.12390 test_loss: 0.13864 \n",
      "[ 39/200] train_loss: 0.11246 valid_loss: 0.12160 test_loss: 0.13772 \n",
      "Validation loss decreased (0.123781 --> 0.121596).  Saving model ...\n",
      "[ 40/200] train_loss: 0.11325 valid_loss: 0.12304 test_loss: 0.13842 \n",
      "[ 41/200] train_loss: 0.11039 valid_loss: 0.12361 test_loss: 0.13676 \n",
      "[ 42/200] train_loss: 0.11262 valid_loss: 0.11793 test_loss: 0.13569 \n",
      "Validation loss decreased (0.121596 --> 0.117926).  Saving model ...\n",
      "[ 43/200] train_loss: 0.11161 valid_loss: 0.11828 test_loss: 0.13607 \n",
      "[ 44/200] train_loss: 0.11257 valid_loss: 0.12054 test_loss: 0.13508 \n",
      "[ 45/200] train_loss: 0.10861 valid_loss: 0.12683 test_loss: 0.13574 \n",
      "[ 46/200] train_loss: 0.10975 valid_loss: 0.11832 test_loss: 0.13419 \n",
      "[ 47/200] train_loss: 0.10980 valid_loss: 0.12058 test_loss: 0.13385 \n",
      "[ 48/200] train_loss: 0.11078 valid_loss: 0.12012 test_loss: 0.13488 \n",
      "[ 49/200] train_loss: 0.10689 valid_loss: 0.11666 test_loss: 0.13384 \n",
      "Validation loss decreased (0.117926 --> 0.116656).  Saving model ...\n",
      "[ 50/200] train_loss: 0.10773 valid_loss: 0.12026 test_loss: 0.13185 \n",
      "[ 51/200] train_loss: 0.10326 valid_loss: 0.11656 test_loss: 0.13088 \n",
      "Validation loss decreased (0.116656 --> 0.116559).  Saving model ...\n",
      "[ 52/200] train_loss: 0.10698 valid_loss: 0.11605 test_loss: 0.13014 \n",
      "Validation loss decreased (0.116559 --> 0.116048).  Saving model ...\n",
      "[ 53/200] train_loss: 0.10553 valid_loss: 0.11830 test_loss: 0.13013 \n",
      "[ 54/200] train_loss: 0.10435 valid_loss: 0.12026 test_loss: 0.12887 \n",
      "[ 55/200] train_loss: 0.10451 valid_loss: 0.11702 test_loss: 0.12886 \n",
      "[ 56/200] train_loss: 0.10458 valid_loss: 0.11304 test_loss: 0.12688 \n",
      "Validation loss decreased (0.116048 --> 0.113038).  Saving model ...\n",
      "[ 57/200] train_loss: 0.10541 valid_loss: 0.10983 test_loss: 0.12665 \n",
      "Validation loss decreased (0.113038 --> 0.109827).  Saving model ...\n",
      "[ 58/200] train_loss: 0.10738 valid_loss: 0.10990 test_loss: 0.12636 \n",
      "[ 59/200] train_loss: 0.10657 valid_loss: 0.11332 test_loss: 0.12692 \n",
      "[ 60/200] train_loss: 0.10329 valid_loss: 0.11163 test_loss: 0.12693 \n",
      "[ 61/200] train_loss: 0.10472 valid_loss: 0.11264 test_loss: 0.12654 \n",
      "[ 62/200] train_loss: 0.10205 valid_loss: 0.11101 test_loss: 0.12456 \n",
      "[ 63/200] train_loss: 0.10425 valid_loss: 0.11140 test_loss: 0.12588 \n",
      "[ 64/200] train_loss: 0.10470 valid_loss: 0.11473 test_loss: 0.12576 \n",
      "[ 65/200] train_loss: 0.10139 valid_loss: 0.10921 test_loss: 0.12499 \n",
      "Validation loss decreased (0.109827 --> 0.109207).  Saving model ...\n",
      "[ 66/200] train_loss: 0.10081 valid_loss: 0.10850 test_loss: 0.12499 \n",
      "Validation loss decreased (0.109207 --> 0.108499).  Saving model ...\n",
      "[ 67/200] train_loss: 0.10240 valid_loss: 0.11011 test_loss: 0.12402 \n",
      "[ 68/200] train_loss: 0.09879 valid_loss: 0.10679 test_loss: 0.12207 \n",
      "Validation loss decreased (0.108499 --> 0.106789).  Saving model ...\n",
      "[ 69/200] train_loss: 0.09794 valid_loss: 0.11262 test_loss: 0.12360 \n",
      "[ 70/200] train_loss: 0.10037 valid_loss: 0.10679 test_loss: 0.12328 \n",
      "[ 71/200] train_loss: 0.09947 valid_loss: 0.10992 test_loss: 0.12326 \n",
      "[ 72/200] train_loss: 0.10048 valid_loss: 0.11313 test_loss: 0.12635 \n",
      "[ 73/200] train_loss: 0.10163 valid_loss: 0.11539 test_loss: 0.12541 \n",
      "[ 74/200] train_loss: 0.09807 valid_loss: 0.11332 test_loss: 0.12494 \n",
      "[ 75/200] train_loss: 0.09781 valid_loss: 0.10738 test_loss: 0.12190 \n",
      "[ 76/200] train_loss: 0.09770 valid_loss: 0.10551 test_loss: 0.12156 \n",
      "Validation loss decreased (0.106789 --> 0.105508).  Saving model ...\n",
      "[ 77/200] train_loss: 0.09553 valid_loss: 0.10860 test_loss: 0.12067 \n",
      "[ 78/200] train_loss: 0.09834 valid_loss: 0.10512 test_loss: 0.12012 \n",
      "Validation loss decreased (0.105508 --> 0.105118).  Saving model ...\n",
      "[ 79/200] train_loss: 0.09718 valid_loss: 0.10461 test_loss: 0.11992 \n",
      "Validation loss decreased (0.105118 --> 0.104611).  Saving model ...\n",
      "[ 80/200] train_loss: 0.09882 valid_loss: 0.10785 test_loss: 0.12046 \n",
      "[ 81/200] train_loss: 0.09720 valid_loss: 0.10472 test_loss: 0.11973 \n",
      "[ 82/200] train_loss: 0.09512 valid_loss: 0.10413 test_loss: 0.11884 \n",
      "Validation loss decreased (0.104611 --> 0.104132).  Saving model ...\n",
      "[ 83/200] train_loss: 0.09320 valid_loss: 0.10491 test_loss: 0.11958 \n",
      "[ 84/200] train_loss: 0.09673 valid_loss: 0.10531 test_loss: 0.11970 \n",
      "[ 85/200] train_loss: 0.09467 valid_loss: 0.10514 test_loss: 0.11859 \n",
      "[ 86/200] train_loss: 0.09367 valid_loss: 0.10632 test_loss: 0.11713 \n",
      "[ 87/200] train_loss: 0.09390 valid_loss: 0.10632 test_loss: 0.11826 \n",
      "[ 88/200] train_loss: 0.09462 valid_loss: 0.10523 test_loss: 0.11885 \n",
      "[ 89/200] train_loss: 0.09042 valid_loss: 0.10572 test_loss: 0.11649 \n",
      "[ 90/200] train_loss: 0.09464 valid_loss: 0.10216 test_loss: 0.11720 \n",
      "Validation loss decreased (0.104132 --> 0.102159).  Saving model ...\n",
      "[ 91/200] train_loss: 0.09495 valid_loss: 0.10208 test_loss: 0.11733 \n",
      "Validation loss decreased (0.102159 --> 0.102081).  Saving model ...\n",
      "[ 92/200] train_loss: 0.09353 valid_loss: 0.10120 test_loss: 0.11526 \n",
      "Validation loss decreased (0.102081 --> 0.101195).  Saving model ...\n",
      "[ 93/200] train_loss: 0.09408 valid_loss: 0.10262 test_loss: 0.11853 \n",
      "[ 94/200] train_loss: 0.09163 valid_loss: 0.10793 test_loss: 0.11678 \n",
      "[ 95/200] train_loss: 0.09227 valid_loss: 0.09992 test_loss: 0.11522 \n",
      "Validation loss decreased (0.101195 --> 0.099920).  Saving model ...\n",
      "[ 96/200] train_loss: 0.09201 valid_loss: 0.10261 test_loss: 0.11582 \n",
      "[ 97/200] train_loss: 0.09053 valid_loss: 0.10122 test_loss: 0.11465 \n",
      "[ 98/200] train_loss: 0.09315 valid_loss: 0.10173 test_loss: 0.11571 \n",
      "[ 99/200] train_loss: 0.09147 valid_loss: 0.10214 test_loss: 0.11441 \n",
      "[100/200] train_loss: 0.09097 valid_loss: 0.10050 test_loss: 0.11560 \n",
      "[101/200] train_loss: 0.08780 valid_loss: 0.10009 test_loss: 0.11358 \n",
      "[102/200] train_loss: 0.09129 valid_loss: 0.10110 test_loss: 0.11390 \n",
      "[103/200] train_loss: 0.09129 valid_loss: 0.09853 test_loss: 0.11296 \n",
      "Validation loss decreased (0.099920 --> 0.098529).  Saving model ...\n",
      "[104/200] train_loss: 0.09062 valid_loss: 0.10050 test_loss: 0.11284 \n",
      "[105/200] train_loss: 0.09144 valid_loss: 0.10390 test_loss: 0.11399 \n",
      "[106/200] train_loss: 0.08871 valid_loss: 0.10461 test_loss: 0.11197 \n",
      "[107/200] train_loss: 0.09054 valid_loss: 0.10055 test_loss: 0.11377 \n",
      "[108/200] train_loss: 0.09138 valid_loss: 0.09927 test_loss: 0.11320 \n",
      "[109/200] train_loss: 0.08874 valid_loss: 0.10069 test_loss: 0.11258 \n",
      "[110/200] train_loss: 0.09053 valid_loss: 0.10324 test_loss: 0.11234 \n",
      "[111/200] train_loss: 0.09074 valid_loss: 0.10076 test_loss: 0.11290 \n",
      "[112/200] train_loss: 0.08974 valid_loss: 0.10433 test_loss: 0.11041 \n",
      "[113/200] train_loss: 0.08836 valid_loss: 0.10525 test_loss: 0.11905 \n",
      "[114/200] train_loss: 0.08954 valid_loss: 0.09798 test_loss: 0.11086 \n",
      "Validation loss decreased (0.098529 --> 0.097978).  Saving model ...\n",
      "[115/200] train_loss: 0.08999 valid_loss: 0.10255 test_loss: 0.11247 \n",
      "[116/200] train_loss: 0.08728 valid_loss: 0.09872 test_loss: 0.10989 \n",
      "[117/200] train_loss: 0.08760 valid_loss: 0.10182 test_loss: 0.11076 \n",
      "[118/200] train_loss: 0.08791 valid_loss: 0.10428 test_loss: 0.11207 \n",
      "[119/200] train_loss: 0.08651 valid_loss: 0.10158 test_loss: 0.11146 \n",
      "[120/200] train_loss: 0.08556 valid_loss: 0.09845 test_loss: 0.11077 \n",
      "[121/200] train_loss: 0.08742 valid_loss: 0.09742 test_loss: 0.11017 \n",
      "Validation loss decreased (0.097978 --> 0.097422).  Saving model ...\n",
      "[122/200] train_loss: 0.08516 valid_loss: 0.09627 test_loss: 0.10887 \n",
      "Validation loss decreased (0.097422 --> 0.096267).  Saving model ...\n",
      "[123/200] train_loss: 0.08715 valid_loss: 0.09679 test_loss: 0.10865 \n",
      "[124/200] train_loss: 0.08869 valid_loss: 0.09663 test_loss: 0.10761 \n",
      "[125/200] train_loss: 0.08749 valid_loss: 0.10121 test_loss: 0.10839 \n",
      "[126/200] train_loss: 0.08471 valid_loss: 0.10099 test_loss: 0.10973 \n",
      "[127/200] train_loss: 0.08509 valid_loss: 0.09653 test_loss: 0.10954 \n",
      "[128/200] train_loss: 0.08469 valid_loss: 0.09542 test_loss: 0.10755 \n",
      "Validation loss decreased (0.096267 --> 0.095420).  Saving model ...\n",
      "[129/200] train_loss: 0.08838 valid_loss: 0.09630 test_loss: 0.10925 \n",
      "[130/200] train_loss: 0.08618 valid_loss: 0.10384 test_loss: 0.10880 \n",
      "[131/200] train_loss: 0.08455 valid_loss: 0.09891 test_loss: 0.10932 \n",
      "[132/200] train_loss: 0.08505 valid_loss: 0.09776 test_loss: 0.10909 \n",
      "[133/200] train_loss: 0.08456 valid_loss: 0.09568 test_loss: 0.10789 \n",
      "[134/200] train_loss: 0.08403 valid_loss: 0.09703 test_loss: 0.10742 \n",
      "[135/200] train_loss: 0.08690 valid_loss: 0.09397 test_loss: 0.10695 \n",
      "Validation loss decreased (0.095420 --> 0.093966).  Saving model ...\n",
      "[136/200] train_loss: 0.08308 valid_loss: 0.09320 test_loss: 0.10776 \n",
      "Validation loss decreased (0.093966 --> 0.093204).  Saving model ...\n",
      "[137/200] train_loss: 0.08571 valid_loss: 0.09520 test_loss: 0.10862 \n",
      "[138/200] train_loss: 0.08364 valid_loss: 0.09543 test_loss: 0.10703 \n",
      "[139/200] train_loss: 0.08488 valid_loss: 0.09618 test_loss: 0.10926 \n",
      "[140/200] train_loss: 0.08321 valid_loss: 0.09985 test_loss: 0.10870 \n",
      "[141/200] train_loss: 0.08582 valid_loss: 0.09825 test_loss: 0.10579 \n",
      "[142/200] train_loss: 0.08475 valid_loss: 0.09854 test_loss: 0.10738 \n",
      "[143/200] train_loss: 0.08453 valid_loss: 0.09798 test_loss: 0.10602 \n",
      "[144/200] train_loss: 0.08242 valid_loss: 0.09409 test_loss: 0.10574 \n",
      "[145/200] train_loss: 0.08378 valid_loss: 0.09358 test_loss: 0.10607 \n",
      "[146/200] train_loss: 0.08112 valid_loss: 0.09271 test_loss: 0.10557 \n",
      "Validation loss decreased (0.093204 --> 0.092710).  Saving model ...\n",
      "[147/200] train_loss: 0.08378 valid_loss: 0.09422 test_loss: 0.10597 \n",
      "[148/200] train_loss: 0.08411 valid_loss: 0.09244 test_loss: 0.10629 \n",
      "Validation loss decreased (0.092710 --> 0.092441).  Saving model ...\n",
      "[149/200] train_loss: 0.08363 valid_loss: 0.09263 test_loss: 0.10509 \n",
      "[150/200] train_loss: 0.08536 valid_loss: 0.09455 test_loss: 0.10543 \n",
      "[151/200] train_loss: 0.08181 valid_loss: 0.09469 test_loss: 0.10570 \n",
      "[152/200] train_loss: 0.08359 valid_loss: 0.09575 test_loss: 0.10581 \n",
      "[153/200] train_loss: 0.08312 valid_loss: 0.09601 test_loss: 0.10409 \n",
      "[154/200] train_loss: 0.08341 valid_loss: 0.09140 test_loss: 0.10508 \n",
      "Validation loss decreased (0.092441 --> 0.091397).  Saving model ...\n",
      "[155/200] train_loss: 0.08252 valid_loss: 0.10344 test_loss: 0.10476 \n",
      "[156/200] train_loss: 0.07974 valid_loss: 0.09647 test_loss: 0.10500 \n",
      "[157/200] train_loss: 0.08183 valid_loss: 0.09536 test_loss: 0.10655 \n",
      "[158/200] train_loss: 0.08060 valid_loss: 0.09196 test_loss: 0.10475 \n",
      "[159/200] train_loss: 0.08047 valid_loss: 0.09142 test_loss: 0.10372 \n",
      "[160/200] train_loss: 0.08266 valid_loss: 0.09464 test_loss: 0.10399 \n",
      "[161/200] train_loss: 0.08203 valid_loss: 0.09808 test_loss: 0.10462 \n",
      "[162/200] train_loss: 0.07939 valid_loss: 0.09508 test_loss: 0.10338 \n",
      "[163/200] train_loss: 0.07982 valid_loss: 0.09542 test_loss: 0.10495 \n",
      "[164/200] train_loss: 0.08236 valid_loss: 0.09350 test_loss: 0.10319 \n",
      "[165/200] train_loss: 0.07987 valid_loss: 0.09570 test_loss: 0.10350 \n",
      "[166/200] train_loss: 0.08151 valid_loss: 0.09138 test_loss: 0.10259 \n",
      "Validation loss decreased (0.091397 --> 0.091375).  Saving model ...\n",
      "[167/200] train_loss: 0.08350 valid_loss: 0.09240 test_loss: 0.10293 \n",
      "[168/200] train_loss: 0.07939 valid_loss: 0.09152 test_loss: 0.10286 \n",
      "[169/200] train_loss: 0.08223 valid_loss: 0.09169 test_loss: 0.10399 \n",
      "[170/200] train_loss: 0.08173 valid_loss: 0.09530 test_loss: 0.10379 \n",
      "[171/200] train_loss: 0.08206 valid_loss: 0.09202 test_loss: 0.10364 \n",
      "[172/200] train_loss: 0.08166 valid_loss: 0.09362 test_loss: 0.10256 \n",
      "[173/200] train_loss: 0.08050 valid_loss: 0.09006 test_loss: 0.10127 \n",
      "Validation loss decreased (0.091375 --> 0.090063).  Saving model ...\n",
      "[174/200] train_loss: 0.08162 valid_loss: 0.09519 test_loss: 0.10345 \n",
      "[175/200] train_loss: 0.08202 valid_loss: 0.09698 test_loss: 0.10185 \n",
      "[176/200] train_loss: 0.07914 valid_loss: 0.09138 test_loss: 0.10333 \n",
      "[177/200] train_loss: 0.08067 valid_loss: 0.09037 test_loss: 0.10176 \n",
      "[178/200] train_loss: 0.07822 valid_loss: 0.09493 test_loss: 0.10277 \n",
      "[179/200] train_loss: 0.08029 valid_loss: 0.09384 test_loss: 0.10244 \n",
      "[180/200] train_loss: 0.08157 valid_loss: 0.09241 test_loss: 0.10125 \n",
      "[181/200] train_loss: 0.07806 valid_loss: 0.08965 test_loss: 0.10094 \n",
      "Validation loss decreased (0.090063 --> 0.089648).  Saving model ...\n",
      "[182/200] train_loss: 0.08154 valid_loss: 0.08812 test_loss: 0.10084 \n",
      "Validation loss decreased (0.089648 --> 0.088118).  Saving model ...\n",
      "[183/200] train_loss: 0.07988 valid_loss: 0.09386 test_loss: 0.10113 \n",
      "[184/200] train_loss: 0.07882 valid_loss: 0.09379 test_loss: 0.10100 \n",
      "[185/200] train_loss: 0.08004 valid_loss: 0.09174 test_loss: 0.10231 \n",
      "[186/200] train_loss: 0.08044 valid_loss: 0.09011 test_loss: 0.10155 \n",
      "[187/200] train_loss: 0.07986 valid_loss: 0.09215 test_loss: 0.10147 \n",
      "[188/200] train_loss: 0.07934 valid_loss: 0.09411 test_loss: 0.10096 \n",
      "[189/200] train_loss: 0.08125 valid_loss: 0.10035 test_loss: 0.10136 \n",
      "[190/200] train_loss: 0.07815 valid_loss: 0.09181 test_loss: 0.10197 \n",
      "[191/200] train_loss: 0.07832 valid_loss: 0.09400 test_loss: 0.10175 \n",
      "[192/200] train_loss: 0.07999 valid_loss: 0.09242 test_loss: 0.10070 \n",
      "[193/200] train_loss: 0.07914 valid_loss: 0.09550 test_loss: 0.09969 \n",
      "[194/200] train_loss: 0.07844 valid_loss: 0.09012 test_loss: 0.10123 \n",
      "[195/200] train_loss: 0.07669 valid_loss: 0.09056 test_loss: 0.10285 \n",
      "[196/200] train_loss: 0.07779 valid_loss: 0.09531 test_loss: 0.09988 \n",
      "[197/200] train_loss: 0.07858 valid_loss: 0.09220 test_loss: 0.09980 \n",
      "[198/200] train_loss: 0.07526 valid_loss: 0.09046 test_loss: 0.09924 \n",
      "[199/200] train_loss: 0.07885 valid_loss: 0.10183 test_loss: 0.09975 \n",
      "[200/200] train_loss: 0.07742 valid_loss: 0.09114 test_loss: 0.09857 \n",
      "Model 8 trained for seen data.\n",
      "TRAINING MODEL for seen house 9\n",
      "[  1/200] train_loss: 0.60477 valid_loss: 0.50016 test_loss: 0.49581 \n",
      "Validation loss decreased (inf --> 0.500162).  Saving model ...\n",
      "[  2/200] train_loss: 0.40942 valid_loss: 0.35713 test_loss: 0.36305 \n",
      "Validation loss decreased (0.500162 --> 0.357135).  Saving model ...\n",
      "[  3/200] train_loss: 0.30811 valid_loss: 0.28790 test_loss: 0.30006 \n",
      "Validation loss decreased (0.357135 --> 0.287896).  Saving model ...\n",
      "[  4/200] train_loss: 0.25450 valid_loss: 0.24867 test_loss: 0.26349 \n",
      "Validation loss decreased (0.287896 --> 0.248669).  Saving model ...\n",
      "[  5/200] train_loss: 0.21898 valid_loss: 0.22187 test_loss: 0.23211 \n",
      "Validation loss decreased (0.248669 --> 0.221869).  Saving model ...\n",
      "[  6/200] train_loss: 0.19935 valid_loss: 0.20289 test_loss: 0.21676 \n",
      "Validation loss decreased (0.221869 --> 0.202885).  Saving model ...\n",
      "[  7/200] train_loss: 0.18113 valid_loss: 0.18603 test_loss: 0.19796 \n",
      "Validation loss decreased (0.202885 --> 0.186029).  Saving model ...\n",
      "[  8/200] train_loss: 0.17181 valid_loss: 0.17643 test_loss: 0.18826 \n",
      "Validation loss decreased (0.186029 --> 0.176432).  Saving model ...\n",
      "[  9/200] train_loss: 0.16541 valid_loss: 0.17069 test_loss: 0.18067 \n",
      "Validation loss decreased (0.176432 --> 0.170688).  Saving model ...\n",
      "[ 10/200] train_loss: 0.15563 valid_loss: 0.16443 test_loss: 0.17502 \n",
      "Validation loss decreased (0.170688 --> 0.164425).  Saving model ...\n",
      "[ 11/200] train_loss: 0.15422 valid_loss: 0.16132 test_loss: 0.17229 \n",
      "Validation loss decreased (0.164425 --> 0.161315).  Saving model ...\n",
      "[ 12/200] train_loss: 0.14920 valid_loss: 0.15938 test_loss: 0.16922 \n",
      "Validation loss decreased (0.161315 --> 0.159379).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14808 valid_loss: 0.15500 test_loss: 0.16498 \n",
      "Validation loss decreased (0.159379 --> 0.155000).  Saving model ...\n",
      "[ 14/200] train_loss: 0.14104 valid_loss: 0.15296 test_loss: 0.16130 \n",
      "Validation loss decreased (0.155000 --> 0.152958).  Saving model ...\n",
      "[ 15/200] train_loss: 0.14537 valid_loss: 0.15208 test_loss: 0.16161 \n",
      "Validation loss decreased (0.152958 --> 0.152083).  Saving model ...\n",
      "[ 16/200] train_loss: 0.13458 valid_loss: 0.14764 test_loss: 0.15767 \n",
      "Validation loss decreased (0.152083 --> 0.147641).  Saving model ...\n",
      "[ 17/200] train_loss: 0.13973 valid_loss: 0.14554 test_loss: 0.15734 \n",
      "Validation loss decreased (0.147641 --> 0.145539).  Saving model ...\n",
      "[ 18/200] train_loss: 0.13233 valid_loss: 0.14302 test_loss: 0.15508 \n",
      "Validation loss decreased (0.145539 --> 0.143021).  Saving model ...\n",
      "[ 19/200] train_loss: 0.13080 valid_loss: 0.14575 test_loss: 0.15802 \n",
      "[ 20/200] train_loss: 0.12902 valid_loss: 0.13998 test_loss: 0.15267 \n",
      "Validation loss decreased (0.143021 --> 0.139981).  Saving model ...\n",
      "[ 21/200] train_loss: 0.12647 valid_loss: 0.13748 test_loss: 0.14927 \n",
      "Validation loss decreased (0.139981 --> 0.137485).  Saving model ...\n",
      "[ 22/200] train_loss: 0.12623 valid_loss: 0.14131 test_loss: 0.15306 \n",
      "[ 23/200] train_loss: 0.12667 valid_loss: 0.13923 test_loss: 0.15011 \n",
      "[ 24/200] train_loss: 0.12288 valid_loss: 0.13358 test_loss: 0.14775 \n",
      "Validation loss decreased (0.137485 --> 0.133576).  Saving model ...\n",
      "[ 25/200] train_loss: 0.12072 valid_loss: 0.13587 test_loss: 0.14939 \n",
      "[ 26/200] train_loss: 0.12403 valid_loss: 0.13293 test_loss: 0.14598 \n",
      "Validation loss decreased (0.133576 --> 0.132931).  Saving model ...\n",
      "[ 27/200] train_loss: 0.11812 valid_loss: 0.13286 test_loss: 0.14693 \n",
      "Validation loss decreased (0.132931 --> 0.132862).  Saving model ...\n",
      "[ 28/200] train_loss: 0.12247 valid_loss: 0.13051 test_loss: 0.14423 \n",
      "Validation loss decreased (0.132862 --> 0.130510).  Saving model ...\n",
      "[ 29/200] train_loss: 0.12005 valid_loss: 0.13010 test_loss: 0.14313 \n",
      "Validation loss decreased (0.130510 --> 0.130097).  Saving model ...\n",
      "[ 30/200] train_loss: 0.11732 valid_loss: 0.12806 test_loss: 0.14146 \n",
      "Validation loss decreased (0.130097 --> 0.128063).  Saving model ...\n",
      "[ 31/200] train_loss: 0.11850 valid_loss: 0.12638 test_loss: 0.14236 \n",
      "Validation loss decreased (0.128063 --> 0.126379).  Saving model ...\n",
      "[ 32/200] train_loss: 0.11566 valid_loss: 0.13019 test_loss: 0.14547 \n",
      "[ 33/200] train_loss: 0.11566 valid_loss: 0.12547 test_loss: 0.14160 \n",
      "Validation loss decreased (0.126379 --> 0.125468).  Saving model ...\n",
      "[ 34/200] train_loss: 0.11438 valid_loss: 0.12477 test_loss: 0.14033 \n",
      "Validation loss decreased (0.125468 --> 0.124770).  Saving model ...\n",
      "[ 35/200] train_loss: 0.11300 valid_loss: 0.12388 test_loss: 0.14040 \n",
      "Validation loss decreased (0.124770 --> 0.123882).  Saving model ...\n",
      "[ 36/200] train_loss: 0.11551 valid_loss: 0.12603 test_loss: 0.13832 \n",
      "[ 37/200] train_loss: 0.11137 valid_loss: 0.12187 test_loss: 0.13940 \n",
      "Validation loss decreased (0.123882 --> 0.121875).  Saving model ...\n",
      "[ 38/200] train_loss: 0.11173 valid_loss: 0.12440 test_loss: 0.13848 \n",
      "[ 39/200] train_loss: 0.11240 valid_loss: 0.12089 test_loss: 0.13354 \n",
      "Validation loss decreased (0.121875 --> 0.120894).  Saving model ...\n",
      "[ 40/200] train_loss: 0.11054 valid_loss: 0.12370 test_loss: 0.13703 \n",
      "[ 41/200] train_loss: 0.11338 valid_loss: 0.12313 test_loss: 0.13444 \n",
      "[ 42/200] train_loss: 0.11019 valid_loss: 0.11982 test_loss: 0.13387 \n",
      "Validation loss decreased (0.120894 --> 0.119818).  Saving model ...\n",
      "[ 43/200] train_loss: 0.10954 valid_loss: 0.11913 test_loss: 0.13450 \n",
      "Validation loss decreased (0.119818 --> 0.119133).  Saving model ...\n",
      "[ 44/200] train_loss: 0.10919 valid_loss: 0.11944 test_loss: 0.13424 \n",
      "[ 45/200] train_loss: 0.10440 valid_loss: 0.11644 test_loss: 0.13265 \n",
      "Validation loss decreased (0.119133 --> 0.116442).  Saving model ...\n",
      "[ 46/200] train_loss: 0.10733 valid_loss: 0.11697 test_loss: 0.13442 \n",
      "[ 47/200] train_loss: 0.10765 valid_loss: 0.12062 test_loss: 0.13277 \n",
      "[ 48/200] train_loss: 0.10732 valid_loss: 0.11685 test_loss: 0.13018 \n",
      "[ 49/200] train_loss: 0.10592 valid_loss: 0.11746 test_loss: 0.13080 \n",
      "[ 50/200] train_loss: 0.10525 valid_loss: 0.11347 test_loss: 0.12894 \n",
      "Validation loss decreased (0.116442 --> 0.113469).  Saving model ...\n",
      "[ 51/200] train_loss: 0.10773 valid_loss: 0.11147 test_loss: 0.12866 \n",
      "Validation loss decreased (0.113469 --> 0.111465).  Saving model ...\n",
      "[ 52/200] train_loss: 0.10486 valid_loss: 0.11390 test_loss: 0.13075 \n",
      "[ 53/200] train_loss: 0.10600 valid_loss: 0.11162 test_loss: 0.12867 \n",
      "[ 54/200] train_loss: 0.10118 valid_loss: 0.11265 test_loss: 0.12791 \n",
      "[ 55/200] train_loss: 0.10769 valid_loss: 0.11035 test_loss: 0.12637 \n",
      "Validation loss decreased (0.111465 --> 0.110347).  Saving model ...\n",
      "[ 56/200] train_loss: 0.10002 valid_loss: 0.11007 test_loss: 0.12717 \n",
      "Validation loss decreased (0.110347 --> 0.110065).  Saving model ...\n",
      "[ 57/200] train_loss: 0.10387 valid_loss: 0.11298 test_loss: 0.12665 \n",
      "[ 58/200] train_loss: 0.10194 valid_loss: 0.11185 test_loss: 0.12611 \n",
      "[ 59/200] train_loss: 0.09938 valid_loss: 0.11254 test_loss: 0.12837 \n",
      "[ 60/200] train_loss: 0.10299 valid_loss: 0.10965 test_loss: 0.12521 \n",
      "Validation loss decreased (0.110065 --> 0.109652).  Saving model ...\n",
      "[ 61/200] train_loss: 0.10103 valid_loss: 0.11400 test_loss: 0.12792 \n",
      "[ 62/200] train_loss: 0.10089 valid_loss: 0.10984 test_loss: 0.12378 \n",
      "[ 63/200] train_loss: 0.09975 valid_loss: 0.10891 test_loss: 0.12528 \n",
      "Validation loss decreased (0.109652 --> 0.108911).  Saving model ...\n",
      "[ 64/200] train_loss: 0.10044 valid_loss: 0.10761 test_loss: 0.12289 \n",
      "Validation loss decreased (0.108911 --> 0.107606).  Saving model ...\n",
      "[ 65/200] train_loss: 0.09864 valid_loss: 0.10937 test_loss: 0.12423 \n",
      "[ 66/200] train_loss: 0.09776 valid_loss: 0.11255 test_loss: 0.12317 \n",
      "[ 67/200] train_loss: 0.10108 valid_loss: 0.11126 test_loss: 0.12274 \n",
      "[ 68/200] train_loss: 0.09627 valid_loss: 0.11091 test_loss: 0.12320 \n",
      "[ 69/200] train_loss: 0.09887 valid_loss: 0.11175 test_loss: 0.12491 \n",
      "[ 70/200] train_loss: 0.09800 valid_loss: 0.10522 test_loss: 0.12142 \n",
      "Validation loss decreased (0.107606 --> 0.105216).  Saving model ...\n",
      "[ 71/200] train_loss: 0.09658 valid_loss: 0.10805 test_loss: 0.12212 \n",
      "[ 72/200] train_loss: 0.09654 valid_loss: 0.10604 test_loss: 0.12081 \n",
      "[ 73/200] train_loss: 0.09999 valid_loss: 0.10712 test_loss: 0.12009 \n",
      "[ 74/200] train_loss: 0.09924 valid_loss: 0.10748 test_loss: 0.12079 \n",
      "[ 75/200] train_loss: 0.09657 valid_loss: 0.10586 test_loss: 0.11987 \n",
      "[ 76/200] train_loss: 0.09430 valid_loss: 0.10678 test_loss: 0.12102 \n",
      "[ 77/200] train_loss: 0.09349 valid_loss: 0.10769 test_loss: 0.12096 \n",
      "[ 78/200] train_loss: 0.09593 valid_loss: 0.10650 test_loss: 0.12047 \n",
      "[ 79/200] train_loss: 0.09357 valid_loss: 0.10297 test_loss: 0.11857 \n",
      "Validation loss decreased (0.105216 --> 0.102971).  Saving model ...\n",
      "[ 80/200] train_loss: 0.09607 valid_loss: 0.10705 test_loss: 0.11786 \n",
      "[ 81/200] train_loss: 0.09773 valid_loss: 0.10669 test_loss: 0.11787 \n",
      "[ 82/200] train_loss: 0.09322 valid_loss: 0.10684 test_loss: 0.12078 \n",
      "[ 83/200] train_loss: 0.09635 valid_loss: 0.10236 test_loss: 0.11598 \n",
      "Validation loss decreased (0.102971 --> 0.102361).  Saving model ...\n",
      "[ 84/200] train_loss: 0.09463 valid_loss: 0.10445 test_loss: 0.11784 \n",
      "[ 85/200] train_loss: 0.09557 valid_loss: 0.10336 test_loss: 0.11806 \n",
      "[ 86/200] train_loss: 0.09496 valid_loss: 0.10328 test_loss: 0.11557 \n",
      "[ 87/200] train_loss: 0.09340 valid_loss: 0.10212 test_loss: 0.11662 \n",
      "Validation loss decreased (0.102361 --> 0.102116).  Saving model ...\n",
      "[ 88/200] train_loss: 0.09382 valid_loss: 0.10586 test_loss: 0.11760 \n",
      "[ 89/200] train_loss: 0.09195 valid_loss: 0.09948 test_loss: 0.11405 \n",
      "Validation loss decreased (0.102116 --> 0.099482).  Saving model ...\n",
      "[ 90/200] train_loss: 0.09155 valid_loss: 0.10361 test_loss: 0.11406 \n",
      "[ 91/200] train_loss: 0.09301 valid_loss: 0.10244 test_loss: 0.11478 \n",
      "[ 92/200] train_loss: 0.09166 valid_loss: 0.10289 test_loss: 0.11623 \n",
      "[ 93/200] train_loss: 0.09060 valid_loss: 0.10280 test_loss: 0.11458 \n",
      "[ 94/200] train_loss: 0.09088 valid_loss: 0.10700 test_loss: 0.11506 \n",
      "[ 95/200] train_loss: 0.09493 valid_loss: 0.10028 test_loss: 0.11230 \n",
      "[ 96/200] train_loss: 0.09075 valid_loss: 0.10056 test_loss: 0.11434 \n",
      "[ 97/200] train_loss: 0.09287 valid_loss: 0.10150 test_loss: 0.11390 \n",
      "[ 98/200] train_loss: 0.08987 valid_loss: 0.10220 test_loss: 0.11389 \n",
      "[ 99/200] train_loss: 0.08978 valid_loss: 0.09967 test_loss: 0.11263 \n",
      "[100/200] train_loss: 0.09268 valid_loss: 0.09980 test_loss: 0.11270 \n",
      "[101/200] train_loss: 0.09043 valid_loss: 0.09775 test_loss: 0.11264 \n",
      "Validation loss decreased (0.099482 --> 0.097750).  Saving model ...\n",
      "[102/200] train_loss: 0.09127 valid_loss: 0.09847 test_loss: 0.11334 \n",
      "[103/200] train_loss: 0.09061 valid_loss: 0.10251 test_loss: 0.11282 \n",
      "[104/200] train_loss: 0.09101 valid_loss: 0.10134 test_loss: 0.11229 \n",
      "[105/200] train_loss: 0.08815 valid_loss: 0.09939 test_loss: 0.11223 \n",
      "[106/200] train_loss: 0.08829 valid_loss: 0.10099 test_loss: 0.11303 \n",
      "[107/200] train_loss: 0.08945 valid_loss: 0.09807 test_loss: 0.11091 \n",
      "[108/200] train_loss: 0.08725 valid_loss: 0.09736 test_loss: 0.11245 \n",
      "Validation loss decreased (0.097750 --> 0.097359).  Saving model ...\n",
      "[109/200] train_loss: 0.08904 valid_loss: 0.09857 test_loss: 0.11155 \n",
      "[110/200] train_loss: 0.08656 valid_loss: 0.09892 test_loss: 0.11117 \n",
      "[111/200] train_loss: 0.08879 valid_loss: 0.09624 test_loss: 0.11055 \n",
      "Validation loss decreased (0.097359 --> 0.096241).  Saving model ...\n",
      "[112/200] train_loss: 0.08756 valid_loss: 0.09471 test_loss: 0.10901 \n",
      "Validation loss decreased (0.096241 --> 0.094708).  Saving model ...\n",
      "[113/200] train_loss: 0.08724 valid_loss: 0.09907 test_loss: 0.11144 \n",
      "[114/200] train_loss: 0.09033 valid_loss: 0.09482 test_loss: 0.11110 \n",
      "[115/200] train_loss: 0.08606 valid_loss: 0.09782 test_loss: 0.10979 \n",
      "[116/200] train_loss: 0.08801 valid_loss: 0.09485 test_loss: 0.10800 \n",
      "[117/200] train_loss: 0.08620 valid_loss: 0.09582 test_loss: 0.10895 \n",
      "[118/200] train_loss: 0.08739 valid_loss: 0.09683 test_loss: 0.10940 \n",
      "[119/200] train_loss: 0.08766 valid_loss: 0.10055 test_loss: 0.10868 \n",
      "[120/200] train_loss: 0.08520 valid_loss: 0.09912 test_loss: 0.10839 \n",
      "[121/200] train_loss: 0.08666 valid_loss: 0.09590 test_loss: 0.10849 \n",
      "[122/200] train_loss: 0.08859 valid_loss: 0.09488 test_loss: 0.10905 \n",
      "[123/200] train_loss: 0.08907 valid_loss: 0.10679 test_loss: 0.10725 \n",
      "[124/200] train_loss: 0.08594 valid_loss: 0.09671 test_loss: 0.10909 \n",
      "[125/200] train_loss: 0.08408 valid_loss: 0.10291 test_loss: 0.10962 \n",
      "[126/200] train_loss: 0.08387 valid_loss: 0.09945 test_loss: 0.10858 \n",
      "[127/200] train_loss: 0.08609 valid_loss: 0.09418 test_loss: 0.10654 \n",
      "Validation loss decreased (0.094708 --> 0.094176).  Saving model ...\n",
      "[128/200] train_loss: 0.08500 valid_loss: 0.09481 test_loss: 0.10769 \n",
      "[129/200] train_loss: 0.08590 valid_loss: 0.09334 test_loss: 0.10650 \n",
      "Validation loss decreased (0.094176 --> 0.093343).  Saving model ...\n",
      "[130/200] train_loss: 0.08524 valid_loss: 0.09343 test_loss: 0.10663 \n",
      "[131/200] train_loss: 0.08407 valid_loss: 0.09367 test_loss: 0.10704 \n",
      "[132/200] train_loss: 0.08585 valid_loss: 0.09477 test_loss: 0.10767 \n",
      "[133/200] train_loss: 0.08540 valid_loss: 0.09113 test_loss: 0.10519 \n",
      "Validation loss decreased (0.093343 --> 0.091134).  Saving model ...\n",
      "[134/200] train_loss: 0.08329 valid_loss: 0.09483 test_loss: 0.10661 \n",
      "[135/200] train_loss: 0.08562 valid_loss: 0.09260 test_loss: 0.10642 \n",
      "[136/200] train_loss: 0.08231 valid_loss: 0.09576 test_loss: 0.10936 \n",
      "[137/200] train_loss: 0.08650 valid_loss: 0.09420 test_loss: 0.10636 \n",
      "[138/200] train_loss: 0.08360 valid_loss: 0.09329 test_loss: 0.10504 \n",
      "[139/200] train_loss: 0.08407 valid_loss: 0.09432 test_loss: 0.10563 \n",
      "[140/200] train_loss: 0.08484 valid_loss: 0.09481 test_loss: 0.10606 \n",
      "[141/200] train_loss: 0.08476 valid_loss: 0.09684 test_loss: 0.10452 \n",
      "[142/200] train_loss: 0.08129 valid_loss: 0.09329 test_loss: 0.10525 \n",
      "[143/200] train_loss: 0.08370 valid_loss: 0.09790 test_loss: 0.10511 \n",
      "[144/200] train_loss: 0.08340 valid_loss: 0.09082 test_loss: 0.10454 \n",
      "Validation loss decreased (0.091134 --> 0.090824).  Saving model ...\n",
      "[145/200] train_loss: 0.08150 valid_loss: 0.09303 test_loss: 0.10481 \n",
      "[146/200] train_loss: 0.08095 valid_loss: 0.09487 test_loss: 0.10337 \n",
      "[147/200] train_loss: 0.08264 valid_loss: 0.09151 test_loss: 0.10399 \n",
      "[148/200] train_loss: 0.08334 valid_loss: 0.09866 test_loss: 0.10512 \n",
      "[149/200] train_loss: 0.08182 valid_loss: 0.09244 test_loss: 0.10223 \n",
      "[150/200] train_loss: 0.08074 valid_loss: 0.09412 test_loss: 0.10326 \n",
      "[151/200] train_loss: 0.08178 valid_loss: 0.09394 test_loss: 0.10457 \n",
      "[152/200] train_loss: 0.08231 valid_loss: 0.09638 test_loss: 0.10600 \n",
      "[153/200] train_loss: 0.08107 valid_loss: 0.09893 test_loss: 0.10354 \n",
      "[154/200] train_loss: 0.08393 valid_loss: 0.09205 test_loss: 0.10242 \n",
      "[155/200] train_loss: 0.08108 valid_loss: 0.09271 test_loss: 0.10423 \n",
      "[156/200] train_loss: 0.08342 valid_loss: 0.09019 test_loss: 0.10453 \n",
      "Validation loss decreased (0.090824 --> 0.090185).  Saving model ...\n",
      "[157/200] train_loss: 0.07794 valid_loss: 0.09245 test_loss: 0.10347 \n",
      "[158/200] train_loss: 0.08069 valid_loss: 0.09216 test_loss: 0.10248 \n",
      "[159/200] train_loss: 0.07873 valid_loss: 0.09231 test_loss: 0.10347 \n",
      "[160/200] train_loss: 0.08410 valid_loss: 0.09082 test_loss: 0.10241 \n",
      "[161/200] train_loss: 0.08183 valid_loss: 0.09287 test_loss: 0.10386 \n",
      "[162/200] train_loss: 0.08189 valid_loss: 0.08897 test_loss: 0.10223 \n",
      "Validation loss decreased (0.090185 --> 0.088966).  Saving model ...\n",
      "[163/200] train_loss: 0.07856 valid_loss: 0.09018 test_loss: 0.10213 \n",
      "[164/200] train_loss: 0.07875 valid_loss: 0.08973 test_loss: 0.10167 \n",
      "[165/200] train_loss: 0.08162 valid_loss: 0.09459 test_loss: 0.10289 \n",
      "[166/200] train_loss: 0.08047 valid_loss: 0.09146 test_loss: 0.10144 \n",
      "[167/200] train_loss: 0.08298 valid_loss: 0.09089 test_loss: 0.10490 \n",
      "[168/200] train_loss: 0.08135 valid_loss: 0.08983 test_loss: 0.10076 \n",
      "[169/200] train_loss: 0.07964 valid_loss: 0.09040 test_loss: 0.10088 \n",
      "[170/200] train_loss: 0.08004 valid_loss: 0.09431 test_loss: 0.10134 \n",
      "[171/200] train_loss: 0.07786 valid_loss: 0.08945 test_loss: 0.10234 \n",
      "[172/200] train_loss: 0.08264 valid_loss: 0.08942 test_loss: 0.10174 \n",
      "[173/200] train_loss: 0.08022 valid_loss: 0.08928 test_loss: 0.10098 \n",
      "[174/200] train_loss: 0.08106 valid_loss: 0.08825 test_loss: 0.10235 \n",
      "Validation loss decreased (0.088966 --> 0.088253).  Saving model ...\n",
      "[175/200] train_loss: 0.07885 valid_loss: 0.08852 test_loss: 0.10192 \n",
      "[176/200] train_loss: 0.08039 valid_loss: 0.08912 test_loss: 0.09939 \n",
      "[177/200] train_loss: 0.07985 valid_loss: 0.08987 test_loss: 0.10093 \n",
      "[178/200] train_loss: 0.07965 valid_loss: 0.09118 test_loss: 0.10325 \n",
      "[179/200] train_loss: 0.07927 valid_loss: 0.09038 test_loss: 0.10215 \n",
      "[180/200] train_loss: 0.07829 valid_loss: 0.09168 test_loss: 0.10122 \n",
      "[181/200] train_loss: 0.07858 valid_loss: 0.08834 test_loss: 0.10157 \n",
      "[182/200] train_loss: 0.07819 valid_loss: 0.09012 test_loss: 0.10059 \n",
      "[183/200] train_loss: 0.07895 valid_loss: 0.09177 test_loss: 0.10012 \n",
      "[184/200] train_loss: 0.07910 valid_loss: 0.09286 test_loss: 0.10111 \n",
      "[185/200] train_loss: 0.07633 valid_loss: 0.09036 test_loss: 0.09944 \n",
      "[186/200] train_loss: 0.08026 valid_loss: 0.08649 test_loss: 0.09958 \n",
      "Validation loss decreased (0.088253 --> 0.086491).  Saving model ...\n",
      "[187/200] train_loss: 0.07716 valid_loss: 0.08916 test_loss: 0.10003 \n",
      "[188/200] train_loss: 0.07632 valid_loss: 0.08869 test_loss: 0.10067 \n",
      "[189/200] train_loss: 0.07822 valid_loss: 0.08804 test_loss: 0.10025 \n",
      "[190/200] train_loss: 0.07634 valid_loss: 0.08709 test_loss: 0.10095 \n",
      "[191/200] train_loss: 0.07947 valid_loss: 0.09205 test_loss: 0.10014 \n",
      "[192/200] train_loss: 0.07739 valid_loss: 0.09028 test_loss: 0.09990 \n",
      "[193/200] train_loss: 0.07924 valid_loss: 0.08859 test_loss: 0.09906 \n",
      "[194/200] train_loss: 0.07895 valid_loss: 0.09214 test_loss: 0.09957 \n",
      "[195/200] train_loss: 0.07731 valid_loss: 0.09204 test_loss: 0.10129 \n",
      "[196/200] train_loss: 0.07787 valid_loss: 0.09126 test_loss: 0.10169 \n",
      "[197/200] train_loss: 0.07593 valid_loss: 0.08823 test_loss: 0.10114 \n",
      "[198/200] train_loss: 0.07694 valid_loss: 0.08988 test_loss: 0.09959 \n",
      "[199/200] train_loss: 0.07567 valid_loss: 0.08884 test_loss: 0.09878 \n",
      "[200/200] train_loss: 0.07727 valid_loss: 0.09062 test_loss: 0.09979 \n",
      "Model 9 trained for seen data.\n",
      "TRAINING MODEL for seen house 10\n",
      "[  1/200] train_loss: 0.54331 valid_loss: 0.44187 test_loss: 0.44493 \n",
      "Validation loss decreased (inf --> 0.441870).  Saving model ...\n",
      "[  2/200] train_loss: 0.35347 valid_loss: 0.32668 test_loss: 0.33308 \n",
      "Validation loss decreased (0.441870 --> 0.326678).  Saving model ...\n",
      "[  3/200] train_loss: 0.28850 valid_loss: 0.28669 test_loss: 0.29872 \n",
      "Validation loss decreased (0.326678 --> 0.286688).  Saving model ...\n",
      "[  4/200] train_loss: 0.25303 valid_loss: 0.24988 test_loss: 0.26560 \n",
      "Validation loss decreased (0.286688 --> 0.249882).  Saving model ...\n",
      "[  5/200] train_loss: 0.22067 valid_loss: 0.22400 test_loss: 0.24016 \n",
      "Validation loss decreased (0.249882 --> 0.223999).  Saving model ...\n",
      "[  6/200] train_loss: 0.20350 valid_loss: 0.20754 test_loss: 0.22312 \n",
      "Validation loss decreased (0.223999 --> 0.207542).  Saving model ...\n",
      "[  7/200] train_loss: 0.18724 valid_loss: 0.18946 test_loss: 0.20437 \n",
      "Validation loss decreased (0.207542 --> 0.189465).  Saving model ...\n",
      "[  8/200] train_loss: 0.17661 valid_loss: 0.17825 test_loss: 0.19410 \n",
      "Validation loss decreased (0.189465 --> 0.178252).  Saving model ...\n",
      "[  9/200] train_loss: 0.16750 valid_loss: 0.16998 test_loss: 0.18528 \n",
      "Validation loss decreased (0.178252 --> 0.169980).  Saving model ...\n",
      "[ 10/200] train_loss: 0.16008 valid_loss: 0.16825 test_loss: 0.17944 \n",
      "Validation loss decreased (0.169980 --> 0.168254).  Saving model ...\n",
      "[ 11/200] train_loss: 0.15674 valid_loss: 0.16052 test_loss: 0.17571 \n",
      "Validation loss decreased (0.168254 --> 0.160524).  Saving model ...\n",
      "[ 12/200] train_loss: 0.15044 valid_loss: 0.15680 test_loss: 0.16976 \n",
      "Validation loss decreased (0.160524 --> 0.156796).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14792 valid_loss: 0.15579 test_loss: 0.17038 \n",
      "Validation loss decreased (0.156796 --> 0.155790).  Saving model ...\n",
      "[ 14/200] train_loss: 0.14343 valid_loss: 0.15046 test_loss: 0.16480 \n",
      "Validation loss decreased (0.155790 --> 0.150461).  Saving model ...\n",
      "[ 15/200] train_loss: 0.14355 valid_loss: 0.15140 test_loss: 0.16637 \n",
      "[ 16/200] train_loss: 0.13954 valid_loss: 0.14653 test_loss: 0.15897 \n",
      "Validation loss decreased (0.150461 --> 0.146533).  Saving model ...\n",
      "[ 17/200] train_loss: 0.14072 valid_loss: 0.14740 test_loss: 0.15880 \n",
      "[ 18/200] train_loss: 0.13416 valid_loss: 0.14324 test_loss: 0.15843 \n",
      "Validation loss decreased (0.146533 --> 0.143241).  Saving model ...\n",
      "[ 19/200] train_loss: 0.13564 valid_loss: 0.13848 test_loss: 0.15299 \n",
      "Validation loss decreased (0.143241 --> 0.138483).  Saving model ...\n",
      "[ 20/200] train_loss: 0.13037 valid_loss: 0.13832 test_loss: 0.15211 \n",
      "Validation loss decreased (0.138483 --> 0.138322).  Saving model ...\n",
      "[ 21/200] train_loss: 0.13018 valid_loss: 0.13659 test_loss: 0.15271 \n",
      "Validation loss decreased (0.138322 --> 0.136594).  Saving model ...\n",
      "[ 22/200] train_loss: 0.12806 valid_loss: 0.13704 test_loss: 0.15019 \n",
      "[ 23/200] train_loss: 0.12934 valid_loss: 0.13868 test_loss: 0.15270 \n",
      "[ 24/200] train_loss: 0.12724 valid_loss: 0.13057 test_loss: 0.14767 \n",
      "Validation loss decreased (0.136594 --> 0.130568).  Saving model ...\n",
      "[ 25/200] train_loss: 0.12467 valid_loss: 0.13534 test_loss: 0.15067 \n",
      "[ 26/200] train_loss: 0.12457 valid_loss: 0.13325 test_loss: 0.14725 \n",
      "[ 27/200] train_loss: 0.12271 valid_loss: 0.13242 test_loss: 0.14849 \n",
      "[ 28/200] train_loss: 0.12247 valid_loss: 0.13176 test_loss: 0.14646 \n",
      "[ 29/200] train_loss: 0.12299 valid_loss: 0.12727 test_loss: 0.14403 \n",
      "Validation loss decreased (0.130568 --> 0.127269).  Saving model ...\n",
      "[ 30/200] train_loss: 0.12163 valid_loss: 0.12716 test_loss: 0.14220 \n",
      "Validation loss decreased (0.127269 --> 0.127162).  Saving model ...\n",
      "[ 31/200] train_loss: 0.11837 valid_loss: 0.12772 test_loss: 0.14276 \n",
      "[ 32/200] train_loss: 0.11655 valid_loss: 0.12835 test_loss: 0.14232 \n",
      "[ 33/200] train_loss: 0.11993 valid_loss: 0.12381 test_loss: 0.14142 \n",
      "Validation loss decreased (0.127162 --> 0.123810).  Saving model ...\n",
      "[ 34/200] train_loss: 0.12231 valid_loss: 0.12327 test_loss: 0.14034 \n",
      "Validation loss decreased (0.123810 --> 0.123270).  Saving model ...\n",
      "[ 35/200] train_loss: 0.11623 valid_loss: 0.11953 test_loss: 0.13896 \n",
      "Validation loss decreased (0.123270 --> 0.119526).  Saving model ...\n",
      "[ 36/200] train_loss: 0.11447 valid_loss: 0.12042 test_loss: 0.13735 \n",
      "[ 37/200] train_loss: 0.11540 valid_loss: 0.12219 test_loss: 0.13987 \n",
      "[ 38/200] train_loss: 0.11235 valid_loss: 0.11950 test_loss: 0.13570 \n",
      "Validation loss decreased (0.119526 --> 0.119500).  Saving model ...\n",
      "[ 39/200] train_loss: 0.11440 valid_loss: 0.12087 test_loss: 0.13628 \n",
      "[ 40/200] train_loss: 0.11442 valid_loss: 0.12497 test_loss: 0.13826 \n",
      "[ 41/200] train_loss: 0.11071 valid_loss: 0.12269 test_loss: 0.13621 \n",
      "[ 42/200] train_loss: 0.11029 valid_loss: 0.11788 test_loss: 0.13305 \n",
      "Validation loss decreased (0.119500 --> 0.117878).  Saving model ...\n",
      "[ 43/200] train_loss: 0.11373 valid_loss: 0.11718 test_loss: 0.13295 \n",
      "Validation loss decreased (0.117878 --> 0.117184).  Saving model ...\n",
      "[ 44/200] train_loss: 0.10957 valid_loss: 0.11779 test_loss: 0.13411 \n",
      "[ 45/200] train_loss: 0.10949 valid_loss: 0.11960 test_loss: 0.13537 \n",
      "[ 46/200] train_loss: 0.11287 valid_loss: 0.11856 test_loss: 0.13539 \n",
      "[ 47/200] train_loss: 0.10884 valid_loss: 0.11610 test_loss: 0.13392 \n",
      "Validation loss decreased (0.117184 --> 0.116097).  Saving model ...\n",
      "[ 48/200] train_loss: 0.10768 valid_loss: 0.11600 test_loss: 0.13443 \n",
      "Validation loss decreased (0.116097 --> 0.116005).  Saving model ...\n",
      "[ 49/200] train_loss: 0.10771 valid_loss: 0.11517 test_loss: 0.13144 \n",
      "Validation loss decreased (0.116005 --> 0.115173).  Saving model ...\n",
      "[ 50/200] train_loss: 0.10457 valid_loss: 0.11517 test_loss: 0.13171 \n",
      "Validation loss decreased (0.115173 --> 0.115166).  Saving model ...\n",
      "[ 51/200] train_loss: 0.10453 valid_loss: 0.11570 test_loss: 0.13192 \n",
      "[ 52/200] train_loss: 0.10650 valid_loss: 0.11574 test_loss: 0.13031 \n",
      "[ 53/200] train_loss: 0.10754 valid_loss: 0.11508 test_loss: 0.12892 \n",
      "Validation loss decreased (0.115166 --> 0.115076).  Saving model ...\n",
      "[ 54/200] train_loss: 0.10691 valid_loss: 0.11509 test_loss: 0.12950 \n",
      "[ 55/200] train_loss: 0.10164 valid_loss: 0.11412 test_loss: 0.13244 \n",
      "Validation loss decreased (0.115076 --> 0.114118).  Saving model ...\n",
      "[ 56/200] train_loss: 0.10583 valid_loss: 0.11113 test_loss: 0.12785 \n",
      "Validation loss decreased (0.114118 --> 0.111127).  Saving model ...\n",
      "[ 57/200] train_loss: 0.10735 valid_loss: 0.11210 test_loss: 0.12798 \n",
      "[ 58/200] train_loss: 0.10463 valid_loss: 0.10965 test_loss: 0.12674 \n",
      "Validation loss decreased (0.111127 --> 0.109650).  Saving model ...\n",
      "[ 59/200] train_loss: 0.10281 valid_loss: 0.11574 test_loss: 0.12846 \n",
      "[ 60/200] train_loss: 0.10393 valid_loss: 0.11022 test_loss: 0.12736 \n",
      "[ 61/200] train_loss: 0.10392 valid_loss: 0.11141 test_loss: 0.12831 \n",
      "[ 62/200] train_loss: 0.10101 valid_loss: 0.10997 test_loss: 0.12503 \n",
      "[ 63/200] train_loss: 0.09848 valid_loss: 0.11166 test_loss: 0.12618 \n",
      "[ 64/200] train_loss: 0.10364 valid_loss: 0.10595 test_loss: 0.12396 \n",
      "Validation loss decreased (0.109650 --> 0.105953).  Saving model ...\n",
      "[ 65/200] train_loss: 0.10144 valid_loss: 0.10703 test_loss: 0.12427 \n",
      "[ 66/200] train_loss: 0.10215 valid_loss: 0.10756 test_loss: 0.12275 \n",
      "[ 67/200] train_loss: 0.10191 valid_loss: 0.10818 test_loss: 0.12288 \n",
      "[ 68/200] train_loss: 0.09864 valid_loss: 0.10679 test_loss: 0.12174 \n",
      "[ 69/200] train_loss: 0.10002 valid_loss: 0.11007 test_loss: 0.12359 \n",
      "[ 70/200] train_loss: 0.09606 valid_loss: 0.10623 test_loss: 0.12023 \n",
      "[ 71/200] train_loss: 0.09850 valid_loss: 0.10810 test_loss: 0.12417 \n",
      "[ 72/200] train_loss: 0.09699 valid_loss: 0.10690 test_loss: 0.12329 \n",
      "[ 73/200] train_loss: 0.09951 valid_loss: 0.10542 test_loss: 0.12108 \n",
      "Validation loss decreased (0.105953 --> 0.105415).  Saving model ...\n",
      "[ 74/200] train_loss: 0.09744 valid_loss: 0.10579 test_loss: 0.12023 \n",
      "[ 75/200] train_loss: 0.09793 valid_loss: 0.10596 test_loss: 0.12055 \n",
      "[ 76/200] train_loss: 0.09873 valid_loss: 0.10237 test_loss: 0.11938 \n",
      "Validation loss decreased (0.105415 --> 0.102371).  Saving model ...\n",
      "[ 77/200] train_loss: 0.09781 valid_loss: 0.10438 test_loss: 0.11973 \n",
      "[ 78/200] train_loss: 0.09838 valid_loss: 0.10775 test_loss: 0.12315 \n",
      "[ 79/200] train_loss: 0.09540 valid_loss: 0.10338 test_loss: 0.11957 \n",
      "[ 80/200] train_loss: 0.09423 valid_loss: 0.10653 test_loss: 0.12005 \n",
      "[ 81/200] train_loss: 0.09571 valid_loss: 0.10405 test_loss: 0.11847 \n",
      "[ 82/200] train_loss: 0.09590 valid_loss: 0.10721 test_loss: 0.11929 \n",
      "[ 83/200] train_loss: 0.09629 valid_loss: 0.10485 test_loss: 0.11798 \n",
      "[ 84/200] train_loss: 0.09622 valid_loss: 0.10375 test_loss: 0.11763 \n",
      "[ 85/200] train_loss: 0.09590 valid_loss: 0.10278 test_loss: 0.11802 \n",
      "[ 86/200] train_loss: 0.09704 valid_loss: 0.10200 test_loss: 0.11747 \n",
      "Validation loss decreased (0.102371 --> 0.101999).  Saving model ...\n",
      "[ 87/200] train_loss: 0.09171 valid_loss: 0.10233 test_loss: 0.11690 \n",
      "[ 88/200] train_loss: 0.09244 valid_loss: 0.10124 test_loss: 0.11636 \n",
      "Validation loss decreased (0.101999 --> 0.101235).  Saving model ...\n",
      "[ 89/200] train_loss: 0.09395 valid_loss: 0.10644 test_loss: 0.12145 \n",
      "[ 90/200] train_loss: 0.09523 valid_loss: 0.10095 test_loss: 0.11757 \n",
      "Validation loss decreased (0.101235 --> 0.100950).  Saving model ...\n",
      "[ 91/200] train_loss: 0.09592 valid_loss: 0.09938 test_loss: 0.11670 \n",
      "Validation loss decreased (0.100950 --> 0.099376).  Saving model ...\n",
      "[ 92/200] train_loss: 0.09255 valid_loss: 0.10130 test_loss: 0.11559 \n",
      "[ 93/200] train_loss: 0.09160 valid_loss: 0.09866 test_loss: 0.11447 \n",
      "Validation loss decreased (0.099376 --> 0.098662).  Saving model ...\n",
      "[ 94/200] train_loss: 0.09171 valid_loss: 0.10018 test_loss: 0.11524 \n",
      "[ 95/200] train_loss: 0.09132 valid_loss: 0.10194 test_loss: 0.11810 \n",
      "[ 96/200] train_loss: 0.09260 valid_loss: 0.09667 test_loss: 0.11417 \n",
      "Validation loss decreased (0.098662 --> 0.096667).  Saving model ...\n",
      "[ 97/200] train_loss: 0.09452 valid_loss: 0.10020 test_loss: 0.11511 \n",
      "[ 98/200] train_loss: 0.09107 valid_loss: 0.10078 test_loss: 0.11516 \n",
      "[ 99/200] train_loss: 0.09128 valid_loss: 0.09847 test_loss: 0.11333 \n",
      "[100/200] train_loss: 0.09360 valid_loss: 0.10126 test_loss: 0.11723 \n",
      "[101/200] train_loss: 0.08696 valid_loss: 0.09974 test_loss: 0.11390 \n",
      "[102/200] train_loss: 0.09173 valid_loss: 0.09999 test_loss: 0.11323 \n",
      "[103/200] train_loss: 0.08995 valid_loss: 0.09861 test_loss: 0.11278 \n",
      "[104/200] train_loss: 0.08732 valid_loss: 0.09772 test_loss: 0.11423 \n",
      "[105/200] train_loss: 0.08729 valid_loss: 0.09785 test_loss: 0.11283 \n",
      "[106/200] train_loss: 0.09143 valid_loss: 0.09690 test_loss: 0.11293 \n",
      "[107/200] train_loss: 0.08979 valid_loss: 0.09731 test_loss: 0.11170 \n",
      "[108/200] train_loss: 0.08805 valid_loss: 0.09662 test_loss: 0.11135 \n",
      "Validation loss decreased (0.096667 --> 0.096620).  Saving model ...\n",
      "[109/200] train_loss: 0.09118 valid_loss: 0.09773 test_loss: 0.11131 \n",
      "[110/200] train_loss: 0.08840 valid_loss: 0.09762 test_loss: 0.11153 \n",
      "[111/200] train_loss: 0.08848 valid_loss: 0.09613 test_loss: 0.11029 \n",
      "Validation loss decreased (0.096620 --> 0.096131).  Saving model ...\n",
      "[112/200] train_loss: 0.09115 valid_loss: 0.09639 test_loss: 0.11203 \n",
      "[113/200] train_loss: 0.08951 valid_loss: 0.09670 test_loss: 0.11166 \n",
      "[114/200] train_loss: 0.08780 valid_loss: 0.09497 test_loss: 0.11016 \n",
      "Validation loss decreased (0.096131 --> 0.094970).  Saving model ...\n",
      "[115/200] train_loss: 0.08710 valid_loss: 0.10005 test_loss: 0.11548 \n",
      "[116/200] train_loss: 0.08641 valid_loss: 0.09707 test_loss: 0.10931 \n",
      "[117/200] train_loss: 0.08794 valid_loss: 0.09553 test_loss: 0.10997 \n",
      "[118/200] train_loss: 0.09044 valid_loss: 0.09685 test_loss: 0.10882 \n",
      "[119/200] train_loss: 0.08528 valid_loss: 0.09652 test_loss: 0.11074 \n",
      "[120/200] train_loss: 0.08833 valid_loss: 0.09284 test_loss: 0.10843 \n",
      "Validation loss decreased (0.094970 --> 0.092837).  Saving model ...\n",
      "[121/200] train_loss: 0.08696 valid_loss: 0.09309 test_loss: 0.10876 \n",
      "[122/200] train_loss: 0.08774 valid_loss: 0.09647 test_loss: 0.10980 \n",
      "[123/200] train_loss: 0.08817 valid_loss: 0.09300 test_loss: 0.10744 \n",
      "[124/200] train_loss: 0.08502 valid_loss: 0.09507 test_loss: 0.10971 \n",
      "[125/200] train_loss: 0.08364 valid_loss: 0.09376 test_loss: 0.10930 \n",
      "[126/200] train_loss: 0.08616 valid_loss: 0.09376 test_loss: 0.10790 \n",
      "[127/200] train_loss: 0.08783 valid_loss: 0.09282 test_loss: 0.10894 \n",
      "Validation loss decreased (0.092837 --> 0.092819).  Saving model ...\n",
      "[128/200] train_loss: 0.08537 valid_loss: 0.09417 test_loss: 0.10907 \n",
      "[129/200] train_loss: 0.08674 valid_loss: 0.09252 test_loss: 0.10753 \n",
      "Validation loss decreased (0.092819 --> 0.092520).  Saving model ...\n",
      "[130/200] train_loss: 0.08641 valid_loss: 0.09191 test_loss: 0.10725 \n",
      "Validation loss decreased (0.092520 --> 0.091913).  Saving model ...\n",
      "[131/200] train_loss: 0.08487 valid_loss: 0.09316 test_loss: 0.10854 \n",
      "[132/200] train_loss: 0.08695 valid_loss: 0.09342 test_loss: 0.10643 \n",
      "[133/200] train_loss: 0.08486 valid_loss: 0.09293 test_loss: 0.10837 \n",
      "[134/200] train_loss: 0.08554 valid_loss: 0.09636 test_loss: 0.10750 \n",
      "[135/200] train_loss: 0.08648 valid_loss: 0.09395 test_loss: 0.10827 \n",
      "[136/200] train_loss: 0.08324 valid_loss: 0.09199 test_loss: 0.10623 \n",
      "[137/200] train_loss: 0.08534 valid_loss: 0.09308 test_loss: 0.10781 \n",
      "[138/200] train_loss: 0.08404 valid_loss: 0.09126 test_loss: 0.10560 \n",
      "Validation loss decreased (0.091913 --> 0.091262).  Saving model ...\n",
      "[139/200] train_loss: 0.08438 valid_loss: 0.09284 test_loss: 0.10736 \n",
      "[140/200] train_loss: 0.08356 valid_loss: 0.09199 test_loss: 0.10704 \n",
      "[141/200] train_loss: 0.08305 valid_loss: 0.09067 test_loss: 0.10498 \n",
      "Validation loss decreased (0.091262 --> 0.090667).  Saving model ...\n",
      "[142/200] train_loss: 0.08296 valid_loss: 0.09283 test_loss: 0.10672 \n",
      "[143/200] train_loss: 0.08292 valid_loss: 0.09194 test_loss: 0.10708 \n",
      "[144/200] train_loss: 0.08104 valid_loss: 0.09168 test_loss: 0.10820 \n",
      "[145/200] train_loss: 0.08523 valid_loss: 0.09444 test_loss: 0.10690 \n",
      "[146/200] train_loss: 0.08199 valid_loss: 0.09519 test_loss: 0.10600 \n",
      "[147/200] train_loss: 0.08305 valid_loss: 0.09287 test_loss: 0.10556 \n",
      "[148/200] train_loss: 0.08505 valid_loss: 0.09321 test_loss: 0.10581 \n",
      "[149/200] train_loss: 0.08567 valid_loss: 0.09199 test_loss: 0.10576 \n",
      "[150/200] train_loss: 0.08256 valid_loss: 0.09222 test_loss: 0.10595 \n",
      "[151/200] train_loss: 0.08473 valid_loss: 0.09211 test_loss: 0.10642 \n",
      "[152/200] train_loss: 0.08188 valid_loss: 0.08972 test_loss: 0.10496 \n",
      "Validation loss decreased (0.090667 --> 0.089723).  Saving model ...\n",
      "[153/200] train_loss: 0.08110 valid_loss: 0.09112 test_loss: 0.10493 \n",
      "[154/200] train_loss: 0.08280 valid_loss: 0.09164 test_loss: 0.10476 \n",
      "[155/200] train_loss: 0.08111 valid_loss: 0.08893 test_loss: 0.10277 \n",
      "Validation loss decreased (0.089723 --> 0.088932).  Saving model ...\n",
      "[156/200] train_loss: 0.08268 valid_loss: 0.09014 test_loss: 0.10284 \n",
      "[157/200] train_loss: 0.08017 valid_loss: 0.09046 test_loss: 0.10454 \n",
      "[158/200] train_loss: 0.08150 valid_loss: 0.09027 test_loss: 0.10349 \n",
      "[159/200] train_loss: 0.08293 valid_loss: 0.09389 test_loss: 0.10519 \n",
      "[160/200] train_loss: 0.08167 valid_loss: 0.09234 test_loss: 0.10523 \n",
      "[161/200] train_loss: 0.08037 valid_loss: 0.09030 test_loss: 0.10274 \n",
      "[162/200] train_loss: 0.08298 valid_loss: 0.09329 test_loss: 0.10433 \n",
      "[163/200] train_loss: 0.08162 valid_loss: 0.08873 test_loss: 0.10273 \n",
      "Validation loss decreased (0.088932 --> 0.088728).  Saving model ...\n",
      "[164/200] train_loss: 0.07903 valid_loss: 0.08994 test_loss: 0.10293 \n",
      "[165/200] train_loss: 0.07948 valid_loss: 0.09145 test_loss: 0.10464 \n",
      "[166/200] train_loss: 0.08170 valid_loss: 0.09038 test_loss: 0.10439 \n",
      "[167/200] train_loss: 0.07926 valid_loss: 0.08996 test_loss: 0.10298 \n",
      "[168/200] train_loss: 0.08057 valid_loss: 0.08888 test_loss: 0.10272 \n",
      "[169/200] train_loss: 0.08055 valid_loss: 0.09059 test_loss: 0.10387 \n",
      "[170/200] train_loss: 0.07903 valid_loss: 0.09352 test_loss: 0.10464 \n",
      "[171/200] train_loss: 0.08080 valid_loss: 0.08944 test_loss: 0.10331 \n",
      "[172/200] train_loss: 0.08163 valid_loss: 0.08949 test_loss: 0.10351 \n",
      "[173/200] train_loss: 0.08059 valid_loss: 0.08785 test_loss: 0.10223 \n",
      "Validation loss decreased (0.088728 --> 0.087851).  Saving model ...\n",
      "[174/200] train_loss: 0.08161 valid_loss: 0.08784 test_loss: 0.10174 \n",
      "Validation loss decreased (0.087851 --> 0.087842).  Saving model ...\n",
      "[175/200] train_loss: 0.07796 valid_loss: 0.08885 test_loss: 0.10228 \n",
      "[176/200] train_loss: 0.08108 valid_loss: 0.08662 test_loss: 0.10202 \n",
      "Validation loss decreased (0.087842 --> 0.086615).  Saving model ...\n",
      "[177/200] train_loss: 0.08110 valid_loss: 0.08885 test_loss: 0.10343 \n",
      "[178/200] train_loss: 0.07901 valid_loss: 0.08664 test_loss: 0.10274 \n",
      "[179/200] train_loss: 0.08164 valid_loss: 0.08836 test_loss: 0.10314 \n",
      "[180/200] train_loss: 0.08096 valid_loss: 0.08887 test_loss: 0.10253 \n",
      "[181/200] train_loss: 0.07797 valid_loss: 0.08785 test_loss: 0.10272 \n",
      "[182/200] train_loss: 0.07924 valid_loss: 0.08915 test_loss: 0.10272 \n",
      "[183/200] train_loss: 0.07958 valid_loss: 0.08782 test_loss: 0.10150 \n",
      "[184/200] train_loss: 0.07673 valid_loss: 0.08670 test_loss: 0.10080 \n",
      "[185/200] train_loss: 0.07762 valid_loss: 0.08617 test_loss: 0.10145 \n",
      "Validation loss decreased (0.086615 --> 0.086167).  Saving model ...\n",
      "[186/200] train_loss: 0.07978 valid_loss: 0.08858 test_loss: 0.10279 \n",
      "[187/200] train_loss: 0.07797 valid_loss: 0.08723 test_loss: 0.10133 \n",
      "[188/200] train_loss: 0.07862 valid_loss: 0.08830 test_loss: 0.10116 \n",
      "[189/200] train_loss: 0.07752 valid_loss: 0.08886 test_loss: 0.10220 \n",
      "[190/200] train_loss: 0.07826 valid_loss: 0.08697 test_loss: 0.10097 \n",
      "[191/200] train_loss: 0.07957 valid_loss: 0.08928 test_loss: 0.10079 \n",
      "[192/200] train_loss: 0.07772 valid_loss: 0.08852 test_loss: 0.10055 \n",
      "[193/200] train_loss: 0.07709 valid_loss: 0.08816 test_loss: 0.10079 \n",
      "[194/200] train_loss: 0.07778 valid_loss: 0.08917 test_loss: 0.10195 \n",
      "[195/200] train_loss: 0.07811 valid_loss: 0.08590 test_loss: 0.10040 \n",
      "Validation loss decreased (0.086167 --> 0.085901).  Saving model ...\n",
      "[196/200] train_loss: 0.07678 valid_loss: 0.08833 test_loss: 0.10186 \n",
      "[197/200] train_loss: 0.07781 valid_loss: 0.08728 test_loss: 0.10084 \n",
      "[198/200] train_loss: 0.07830 valid_loss: 0.08753 test_loss: 0.10204 \n",
      "[199/200] train_loss: 0.07416 valid_loss: 0.08846 test_loss: 0.10172 \n",
      "[200/200] train_loss: 0.07608 valid_loss: 0.08661 test_loss: 0.10055 \n",
      "Model 10 trained for seen data.\n",
      "TRAINING MODEL for seen house 11\n",
      "[  1/200] train_loss: 0.54697 valid_loss: 0.43544 test_loss: 0.42984 \n",
      "Validation loss decreased (inf --> 0.435444).  Saving model ...\n",
      "[  2/200] train_loss: 0.35712 valid_loss: 0.32863 test_loss: 0.33427 \n",
      "Validation loss decreased (0.435444 --> 0.328634).  Saving model ...\n",
      "[  3/200] train_loss: 0.28255 valid_loss: 0.27918 test_loss: 0.28890 \n",
      "Validation loss decreased (0.328634 --> 0.279181).  Saving model ...\n",
      "[  4/200] train_loss: 0.24215 valid_loss: 0.23743 test_loss: 0.24900 \n",
      "Validation loss decreased (0.279181 --> 0.237426).  Saving model ...\n",
      "[  5/200] train_loss: 0.21586 valid_loss: 0.21032 test_loss: 0.22291 \n",
      "Validation loss decreased (0.237426 --> 0.210320).  Saving model ...\n",
      "[  6/200] train_loss: 0.19266 valid_loss: 0.19598 test_loss: 0.20786 \n",
      "Validation loss decreased (0.210320 --> 0.195979).  Saving model ...\n",
      "[  7/200] train_loss: 0.18121 valid_loss: 0.18099 test_loss: 0.19333 \n",
      "Validation loss decreased (0.195979 --> 0.180991).  Saving model ...\n",
      "[  8/200] train_loss: 0.16849 valid_loss: 0.17521 test_loss: 0.18631 \n",
      "Validation loss decreased (0.180991 --> 0.175211).  Saving model ...\n",
      "[  9/200] train_loss: 0.16497 valid_loss: 0.16663 test_loss: 0.17870 \n",
      "Validation loss decreased (0.175211 --> 0.166629).  Saving model ...\n",
      "[ 10/200] train_loss: 0.15497 valid_loss: 0.16256 test_loss: 0.17304 \n",
      "Validation loss decreased (0.166629 --> 0.162564).  Saving model ...\n",
      "[ 11/200] train_loss: 0.15100 valid_loss: 0.15588 test_loss: 0.16952 \n",
      "Validation loss decreased (0.162564 --> 0.155878).  Saving model ...\n",
      "[ 12/200] train_loss: 0.14875 valid_loss: 0.15144 test_loss: 0.16375 \n",
      "Validation loss decreased (0.155878 --> 0.151435).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14412 valid_loss: 0.15082 test_loss: 0.16206 \n",
      "Validation loss decreased (0.151435 --> 0.150817).  Saving model ...\n",
      "[ 14/200] train_loss: 0.14173 valid_loss: 0.14446 test_loss: 0.15739 \n",
      "Validation loss decreased (0.150817 --> 0.144464).  Saving model ...\n",
      "[ 15/200] train_loss: 0.13925 valid_loss: 0.14470 test_loss: 0.15649 \n",
      "[ 16/200] train_loss: 0.13513 valid_loss: 0.14228 test_loss: 0.15401 \n",
      "Validation loss decreased (0.144464 --> 0.142275).  Saving model ...\n",
      "[ 17/200] train_loss: 0.13616 valid_loss: 0.14160 test_loss: 0.15332 \n",
      "Validation loss decreased (0.142275 --> 0.141602).  Saving model ...\n",
      "[ 18/200] train_loss: 0.13238 valid_loss: 0.14052 test_loss: 0.15054 \n",
      "Validation loss decreased (0.141602 --> 0.140520).  Saving model ...\n",
      "[ 19/200] train_loss: 0.13426 valid_loss: 0.13484 test_loss: 0.14741 \n",
      "Validation loss decreased (0.140520 --> 0.134843).  Saving model ...\n",
      "[ 20/200] train_loss: 0.12521 valid_loss: 0.13554 test_loss: 0.14765 \n",
      "[ 21/200] train_loss: 0.12489 valid_loss: 0.13318 test_loss: 0.14644 \n",
      "Validation loss decreased (0.134843 --> 0.133177).  Saving model ...\n",
      "[ 22/200] train_loss: 0.12486 valid_loss: 0.13507 test_loss: 0.14717 \n",
      "[ 23/200] train_loss: 0.12014 valid_loss: 0.13047 test_loss: 0.14462 \n",
      "Validation loss decreased (0.133177 --> 0.130467).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12389 valid_loss: 0.13224 test_loss: 0.14329 \n",
      "[ 25/200] train_loss: 0.12115 valid_loss: 0.13078 test_loss: 0.14060 \n",
      "[ 26/200] train_loss: 0.12016 valid_loss: 0.12604 test_loss: 0.13902 \n",
      "Validation loss decreased (0.130467 --> 0.126038).  Saving model ...\n",
      "[ 27/200] train_loss: 0.11677 valid_loss: 0.12695 test_loss: 0.14112 \n",
      "[ 28/200] train_loss: 0.11590 valid_loss: 0.12471 test_loss: 0.13791 \n",
      "Validation loss decreased (0.126038 --> 0.124710).  Saving model ...\n",
      "[ 29/200] train_loss: 0.11487 valid_loss: 0.12754 test_loss: 0.14096 \n",
      "[ 30/200] train_loss: 0.11555 valid_loss: 0.12808 test_loss: 0.13927 \n",
      "[ 31/200] train_loss: 0.11511 valid_loss: 0.12646 test_loss: 0.13784 \n",
      "[ 32/200] train_loss: 0.11664 valid_loss: 0.12592 test_loss: 0.13705 \n",
      "[ 33/200] train_loss: 0.11285 valid_loss: 0.12150 test_loss: 0.13494 \n",
      "Validation loss decreased (0.124710 --> 0.121501).  Saving model ...\n",
      "[ 34/200] train_loss: 0.11611 valid_loss: 0.12448 test_loss: 0.13577 \n",
      "[ 35/200] train_loss: 0.11391 valid_loss: 0.12112 test_loss: 0.13426 \n",
      "Validation loss decreased (0.121501 --> 0.121125).  Saving model ...\n",
      "[ 36/200] train_loss: 0.11119 valid_loss: 0.11899 test_loss: 0.13220 \n",
      "Validation loss decreased (0.121125 --> 0.118995).  Saving model ...\n",
      "[ 37/200] train_loss: 0.11464 valid_loss: 0.11877 test_loss: 0.13264 \n",
      "Validation loss decreased (0.118995 --> 0.118768).  Saving model ...\n",
      "[ 38/200] train_loss: 0.10681 valid_loss: 0.11799 test_loss: 0.13182 \n",
      "Validation loss decreased (0.118768 --> 0.117987).  Saving model ...\n",
      "[ 39/200] train_loss: 0.10923 valid_loss: 0.11767 test_loss: 0.13206 \n",
      "Validation loss decreased (0.117987 --> 0.117668).  Saving model ...\n",
      "[ 40/200] train_loss: 0.10754 valid_loss: 0.11741 test_loss: 0.13168 \n",
      "Validation loss decreased (0.117668 --> 0.117410).  Saving model ...\n",
      "[ 41/200] train_loss: 0.10805 valid_loss: 0.11281 test_loss: 0.12893 \n",
      "Validation loss decreased (0.117410 --> 0.112811).  Saving model ...\n",
      "[ 42/200] train_loss: 0.11092 valid_loss: 0.11276 test_loss: 0.12801 \n",
      "Validation loss decreased (0.112811 --> 0.112758).  Saving model ...\n",
      "[ 43/200] train_loss: 0.10930 valid_loss: 0.11712 test_loss: 0.13089 \n",
      "[ 44/200] train_loss: 0.10556 valid_loss: 0.11307 test_loss: 0.12756 \n",
      "[ 45/200] train_loss: 0.10948 valid_loss: 0.11399 test_loss: 0.12840 \n",
      "[ 46/200] train_loss: 0.10916 valid_loss: 0.11406 test_loss: 0.12929 \n",
      "[ 47/200] train_loss: 0.10557 valid_loss: 0.11177 test_loss: 0.12680 \n",
      "Validation loss decreased (0.112758 --> 0.111770).  Saving model ...\n",
      "[ 48/200] train_loss: 0.10643 valid_loss: 0.11076 test_loss: 0.12570 \n",
      "Validation loss decreased (0.111770 --> 0.110762).  Saving model ...\n",
      "[ 49/200] train_loss: 0.10538 valid_loss: 0.11240 test_loss: 0.12995 \n",
      "[ 50/200] train_loss: 0.10350 valid_loss: 0.11352 test_loss: 0.12814 \n",
      "[ 51/200] train_loss: 0.10240 valid_loss: 0.11227 test_loss: 0.12576 \n",
      "[ 52/200] train_loss: 0.10499 valid_loss: 0.10892 test_loss: 0.12493 \n",
      "Validation loss decreased (0.110762 --> 0.108916).  Saving model ...\n",
      "[ 53/200] train_loss: 0.10314 valid_loss: 0.10914 test_loss: 0.12421 \n",
      "[ 54/200] train_loss: 0.10231 valid_loss: 0.11171 test_loss: 0.12401 \n",
      "[ 55/200] train_loss: 0.10364 valid_loss: 0.10944 test_loss: 0.12537 \n",
      "[ 56/200] train_loss: 0.10115 valid_loss: 0.10994 test_loss: 0.12355 \n",
      "[ 57/200] train_loss: 0.09970 valid_loss: 0.10753 test_loss: 0.12238 \n",
      "Validation loss decreased (0.108916 --> 0.107526).  Saving model ...\n",
      "[ 58/200] train_loss: 0.10143 valid_loss: 0.11187 test_loss: 0.12261 \n",
      "[ 59/200] train_loss: 0.09909 valid_loss: 0.10828 test_loss: 0.12314 \n",
      "[ 60/200] train_loss: 0.10063 valid_loss: 0.10545 test_loss: 0.12350 \n",
      "Validation loss decreased (0.107526 --> 0.105454).  Saving model ...\n",
      "[ 61/200] train_loss: 0.09934 valid_loss: 0.10575 test_loss: 0.12207 \n",
      "[ 62/200] train_loss: 0.09655 valid_loss: 0.10740 test_loss: 0.12156 \n",
      "[ 63/200] train_loss: 0.10063 valid_loss: 0.10769 test_loss: 0.12123 \n",
      "[ 64/200] train_loss: 0.09864 valid_loss: 0.10589 test_loss: 0.11968 \n",
      "[ 65/200] train_loss: 0.09815 valid_loss: 0.10697 test_loss: 0.12026 \n",
      "[ 66/200] train_loss: 0.10006 valid_loss: 0.10643 test_loss: 0.12007 \n",
      "[ 67/200] train_loss: 0.09424 valid_loss: 0.10461 test_loss: 0.11878 \n",
      "Validation loss decreased (0.105454 --> 0.104612).  Saving model ...\n",
      "[ 68/200] train_loss: 0.09733 valid_loss: 0.10578 test_loss: 0.11949 \n",
      "[ 69/200] train_loss: 0.09443 valid_loss: 0.10717 test_loss: 0.11836 \n",
      "[ 70/200] train_loss: 0.09416 valid_loss: 0.10409 test_loss: 0.11696 \n",
      "Validation loss decreased (0.104612 --> 0.104093).  Saving model ...\n",
      "[ 71/200] train_loss: 0.09548 valid_loss: 0.10328 test_loss: 0.11830 \n",
      "Validation loss decreased (0.104093 --> 0.103277).  Saving model ...\n",
      "[ 72/200] train_loss: 0.09893 valid_loss: 0.10325 test_loss: 0.11810 \n",
      "Validation loss decreased (0.103277 --> 0.103254).  Saving model ...\n",
      "[ 73/200] train_loss: 0.09619 valid_loss: 0.10173 test_loss: 0.11841 \n",
      "Validation loss decreased (0.103254 --> 0.101734).  Saving model ...\n",
      "[ 74/200] train_loss: 0.09561 valid_loss: 0.10096 test_loss: 0.11706 \n",
      "Validation loss decreased (0.101734 --> 0.100962).  Saving model ...\n",
      "[ 75/200] train_loss: 0.09548 valid_loss: 0.09987 test_loss: 0.11576 \n",
      "Validation loss decreased (0.100962 --> 0.099865).  Saving model ...\n",
      "[ 76/200] train_loss: 0.09388 valid_loss: 0.10097 test_loss: 0.11736 \n",
      "[ 77/200] train_loss: 0.09601 valid_loss: 0.10424 test_loss: 0.12068 \n",
      "[ 78/200] train_loss: 0.09535 valid_loss: 0.10061 test_loss: 0.11482 \n",
      "[ 79/200] train_loss: 0.09300 valid_loss: 0.10132 test_loss: 0.11765 \n",
      "[ 80/200] train_loss: 0.09374 valid_loss: 0.10046 test_loss: 0.11497 \n",
      "[ 81/200] train_loss: 0.09232 valid_loss: 0.09985 test_loss: 0.11533 \n",
      "Validation loss decreased (0.099865 --> 0.099851).  Saving model ...\n",
      "[ 82/200] train_loss: 0.09287 valid_loss: 0.09850 test_loss: 0.11469 \n",
      "Validation loss decreased (0.099851 --> 0.098505).  Saving model ...\n",
      "[ 83/200] train_loss: 0.09257 valid_loss: 0.09972 test_loss: 0.11469 \n",
      "[ 84/200] train_loss: 0.09149 valid_loss: 0.10254 test_loss: 0.11897 \n",
      "[ 85/200] train_loss: 0.09298 valid_loss: 0.09981 test_loss: 0.11528 \n",
      "[ 86/200] train_loss: 0.09053 valid_loss: 0.09743 test_loss: 0.11451 \n",
      "Validation loss decreased (0.098505 --> 0.097428).  Saving model ...\n",
      "[ 87/200] train_loss: 0.09299 valid_loss: 0.09955 test_loss: 0.11458 \n",
      "[ 88/200] train_loss: 0.09165 valid_loss: 0.09807 test_loss: 0.11222 \n",
      "[ 89/200] train_loss: 0.09095 valid_loss: 0.09801 test_loss: 0.11373 \n",
      "[ 90/200] train_loss: 0.09318 valid_loss: 0.09870 test_loss: 0.11403 \n",
      "[ 91/200] train_loss: 0.08976 valid_loss: 0.10420 test_loss: 0.11474 \n",
      "[ 92/200] train_loss: 0.08979 valid_loss: 0.09979 test_loss: 0.11390 \n",
      "[ 93/200] train_loss: 0.08898 valid_loss: 0.09888 test_loss: 0.11189 \n",
      "[ 94/200] train_loss: 0.09159 valid_loss: 0.09877 test_loss: 0.11296 \n",
      "[ 95/200] train_loss: 0.08927 valid_loss: 0.09907 test_loss: 0.11336 \n",
      "[ 96/200] train_loss: 0.09077 valid_loss: 0.09765 test_loss: 0.11283 \n",
      "[ 97/200] train_loss: 0.08913 valid_loss: 0.09861 test_loss: 0.11246 \n",
      "[ 98/200] train_loss: 0.08907 valid_loss: 0.09781 test_loss: 0.11240 \n",
      "[ 99/200] train_loss: 0.08703 valid_loss: 0.09621 test_loss: 0.11093 \n",
      "Validation loss decreased (0.097428 --> 0.096213).  Saving model ...\n",
      "[100/200] train_loss: 0.09080 valid_loss: 0.09494 test_loss: 0.10977 \n",
      "Validation loss decreased (0.096213 --> 0.094942).  Saving model ...\n",
      "[101/200] train_loss: 0.08703 valid_loss: 0.09546 test_loss: 0.11147 \n",
      "[102/200] train_loss: 0.09053 valid_loss: 0.09562 test_loss: 0.11175 \n",
      "[103/200] train_loss: 0.08672 valid_loss: 0.09643 test_loss: 0.11157 \n",
      "[104/200] train_loss: 0.08849 valid_loss: 0.09529 test_loss: 0.11119 \n",
      "[105/200] train_loss: 0.08881 valid_loss: 0.09669 test_loss: 0.11051 \n",
      "[106/200] train_loss: 0.08847 valid_loss: 0.09616 test_loss: 0.11147 \n",
      "[107/200] train_loss: 0.08896 valid_loss: 0.09506 test_loss: 0.11063 \n",
      "[108/200] train_loss: 0.08786 valid_loss: 0.09605 test_loss: 0.11234 \n",
      "[109/200] train_loss: 0.08832 valid_loss: 0.09411 test_loss: 0.10914 \n",
      "Validation loss decreased (0.094942 --> 0.094109).  Saving model ...\n",
      "[110/200] train_loss: 0.08736 valid_loss: 0.09382 test_loss: 0.10941 \n",
      "Validation loss decreased (0.094109 --> 0.093820).  Saving model ...\n",
      "[111/200] train_loss: 0.08620 valid_loss: 0.09376 test_loss: 0.10902 \n",
      "Validation loss decreased (0.093820 --> 0.093759).  Saving model ...\n",
      "[112/200] train_loss: 0.08524 valid_loss: 0.09398 test_loss: 0.10862 \n",
      "[113/200] train_loss: 0.08772 valid_loss: 0.09407 test_loss: 0.11058 \n",
      "[114/200] train_loss: 0.08661 valid_loss: 0.09291 test_loss: 0.10770 \n",
      "Validation loss decreased (0.093759 --> 0.092911).  Saving model ...\n",
      "[115/200] train_loss: 0.08599 valid_loss: 0.09389 test_loss: 0.10973 \n",
      "[116/200] train_loss: 0.08465 valid_loss: 0.09343 test_loss: 0.10919 \n",
      "[117/200] train_loss: 0.08361 valid_loss: 0.09391 test_loss: 0.10886 \n",
      "[118/200] train_loss: 0.08701 valid_loss: 0.09522 test_loss: 0.11020 \n",
      "[119/200] train_loss: 0.08681 valid_loss: 0.09399 test_loss: 0.10911 \n",
      "[120/200] train_loss: 0.08523 valid_loss: 0.09277 test_loss: 0.10736 \n",
      "Validation loss decreased (0.092911 --> 0.092771).  Saving model ...\n",
      "[121/200] train_loss: 0.08418 valid_loss: 0.09465 test_loss: 0.10782 \n",
      "[122/200] train_loss: 0.08635 valid_loss: 0.09376 test_loss: 0.10843 \n",
      "[123/200] train_loss: 0.08258 valid_loss: 0.09441 test_loss: 0.10702 \n",
      "[124/200] train_loss: 0.08415 valid_loss: 0.09272 test_loss: 0.10671 \n",
      "Validation loss decreased (0.092771 --> 0.092717).  Saving model ...\n",
      "[125/200] train_loss: 0.08304 valid_loss: 0.09196 test_loss: 0.10614 \n",
      "Validation loss decreased (0.092717 --> 0.091955).  Saving model ...\n",
      "[126/200] train_loss: 0.08382 valid_loss: 0.09222 test_loss: 0.10622 \n",
      "[127/200] train_loss: 0.08581 valid_loss: 0.09298 test_loss: 0.10856 \n",
      "[128/200] train_loss: 0.08075 valid_loss: 0.09328 test_loss: 0.10758 \n",
      "[129/200] train_loss: 0.08187 valid_loss: 0.09117 test_loss: 0.10715 \n",
      "Validation loss decreased (0.091955 --> 0.091167).  Saving model ...\n",
      "[130/200] train_loss: 0.08388 valid_loss: 0.09562 test_loss: 0.10892 \n",
      "[131/200] train_loss: 0.08654 valid_loss: 0.09069 test_loss: 0.10567 \n",
      "Validation loss decreased (0.091167 --> 0.090691).  Saving model ...\n",
      "[132/200] train_loss: 0.08305 valid_loss: 0.08986 test_loss: 0.10431 \n",
      "Validation loss decreased (0.090691 --> 0.089861).  Saving model ...\n",
      "[133/200] train_loss: 0.08578 valid_loss: 0.09029 test_loss: 0.10515 \n",
      "[134/200] train_loss: 0.08209 valid_loss: 0.09183 test_loss: 0.10574 \n",
      "[135/200] train_loss: 0.08255 valid_loss: 0.09046 test_loss: 0.10431 \n",
      "[136/200] train_loss: 0.08239 valid_loss: 0.09112 test_loss: 0.10510 \n",
      "[137/200] train_loss: 0.08282 valid_loss: 0.09105 test_loss: 0.10412 \n",
      "[138/200] train_loss: 0.08069 valid_loss: 0.09135 test_loss: 0.10455 \n",
      "[139/200] train_loss: 0.08410 valid_loss: 0.09078 test_loss: 0.10478 \n",
      "[140/200] train_loss: 0.08456 valid_loss: 0.09114 test_loss: 0.10513 \n",
      "[141/200] train_loss: 0.08259 valid_loss: 0.09227 test_loss: 0.10512 \n",
      "[142/200] train_loss: 0.08158 valid_loss: 0.09055 test_loss: 0.10362 \n",
      "[143/200] train_loss: 0.08021 valid_loss: 0.08995 test_loss: 0.10450 \n",
      "[144/200] train_loss: 0.08327 valid_loss: 0.08988 test_loss: 0.10382 \n",
      "[145/200] train_loss: 0.08189 valid_loss: 0.09231 test_loss: 0.10630 \n",
      "[146/200] train_loss: 0.08316 valid_loss: 0.08969 test_loss: 0.10432 \n",
      "Validation loss decreased (0.089861 --> 0.089693).  Saving model ...\n",
      "[147/200] train_loss: 0.08335 valid_loss: 0.09114 test_loss: 0.10356 \n",
      "[148/200] train_loss: 0.08134 valid_loss: 0.09132 test_loss: 0.10372 \n",
      "[149/200] train_loss: 0.08056 valid_loss: 0.09072 test_loss: 0.10469 \n",
      "[150/200] train_loss: 0.08129 valid_loss: 0.08996 test_loss: 0.10319 \n",
      "[151/200] train_loss: 0.08048 valid_loss: 0.08878 test_loss: 0.10293 \n",
      "Validation loss decreased (0.089693 --> 0.088778).  Saving model ...\n",
      "[152/200] train_loss: 0.08225 valid_loss: 0.09026 test_loss: 0.10429 \n",
      "[153/200] train_loss: 0.08174 valid_loss: 0.09041 test_loss: 0.10302 \n",
      "[154/200] train_loss: 0.08022 valid_loss: 0.09061 test_loss: 0.10397 \n",
      "[155/200] train_loss: 0.08330 valid_loss: 0.09090 test_loss: 0.10444 \n",
      "[156/200] train_loss: 0.07851 valid_loss: 0.09154 test_loss: 0.10307 \n",
      "[157/200] train_loss: 0.08110 valid_loss: 0.08921 test_loss: 0.10244 \n",
      "[158/200] train_loss: 0.08083 valid_loss: 0.08893 test_loss: 0.10251 \n",
      "[159/200] train_loss: 0.07936 valid_loss: 0.08896 test_loss: 0.10198 \n",
      "[160/200] train_loss: 0.07866 valid_loss: 0.08815 test_loss: 0.10144 \n",
      "Validation loss decreased (0.088778 --> 0.088149).  Saving model ...\n",
      "[161/200] train_loss: 0.07953 valid_loss: 0.08979 test_loss: 0.10193 \n",
      "[162/200] train_loss: 0.08069 valid_loss: 0.08983 test_loss: 0.10116 \n",
      "[163/200] train_loss: 0.07995 valid_loss: 0.08810 test_loss: 0.10203 \n",
      "Validation loss decreased (0.088149 --> 0.088097).  Saving model ...\n",
      "[164/200] train_loss: 0.07890 valid_loss: 0.08888 test_loss: 0.10331 \n",
      "[165/200] train_loss: 0.08053 valid_loss: 0.08917 test_loss: 0.10138 \n",
      "[166/200] train_loss: 0.07669 valid_loss: 0.08909 test_loss: 0.10370 \n",
      "[167/200] train_loss: 0.08272 valid_loss: 0.08933 test_loss: 0.10128 \n",
      "[168/200] train_loss: 0.07893 valid_loss: 0.08925 test_loss: 0.10281 \n",
      "[169/200] train_loss: 0.08073 valid_loss: 0.09077 test_loss: 0.10136 \n",
      "[170/200] train_loss: 0.07834 valid_loss: 0.08791 test_loss: 0.10097 \n",
      "Validation loss decreased (0.088097 --> 0.087912).  Saving model ...\n",
      "[171/200] train_loss: 0.07889 valid_loss: 0.09101 test_loss: 0.10230 \n",
      "[172/200] train_loss: 0.07867 valid_loss: 0.08667 test_loss: 0.09993 \n",
      "Validation loss decreased (0.087912 --> 0.086665).  Saving model ...\n",
      "[173/200] train_loss: 0.07790 valid_loss: 0.08730 test_loss: 0.10055 \n",
      "[174/200] train_loss: 0.07912 valid_loss: 0.08652 test_loss: 0.09976 \n",
      "Validation loss decreased (0.086665 --> 0.086520).  Saving model ...\n",
      "[175/200] train_loss: 0.07576 valid_loss: 0.08883 test_loss: 0.10175 \n",
      "[176/200] train_loss: 0.07517 valid_loss: 0.08834 test_loss: 0.10116 \n",
      "[177/200] train_loss: 0.07568 valid_loss: 0.08793 test_loss: 0.10128 \n",
      "[178/200] train_loss: 0.07872 valid_loss: 0.08829 test_loss: 0.10165 \n",
      "[179/200] train_loss: 0.07967 valid_loss: 0.08643 test_loss: 0.09907 \n",
      "Validation loss decreased (0.086520 --> 0.086432).  Saving model ...\n",
      "[180/200] train_loss: 0.07762 valid_loss: 0.08819 test_loss: 0.10116 \n",
      "[181/200] train_loss: 0.07997 valid_loss: 0.08798 test_loss: 0.10060 \n",
      "[182/200] train_loss: 0.07761 valid_loss: 0.08968 test_loss: 0.10300 \n",
      "[183/200] train_loss: 0.07940 valid_loss: 0.09179 test_loss: 0.10210 \n",
      "[184/200] train_loss: 0.07649 valid_loss: 0.08742 test_loss: 0.10087 \n",
      "[185/200] train_loss: 0.07965 valid_loss: 0.08692 test_loss: 0.10035 \n",
      "[186/200] train_loss: 0.07701 valid_loss: 0.08663 test_loss: 0.09861 \n",
      "[187/200] train_loss: 0.07695 valid_loss: 0.08735 test_loss: 0.09995 \n",
      "[188/200] train_loss: 0.07670 valid_loss: 0.08917 test_loss: 0.10011 \n",
      "[189/200] train_loss: 0.07600 valid_loss: 0.08636 test_loss: 0.09995 \n",
      "Validation loss decreased (0.086432 --> 0.086357).  Saving model ...\n",
      "[190/200] train_loss: 0.07518 valid_loss: 0.08933 test_loss: 0.10046 \n",
      "[191/200] train_loss: 0.07702 valid_loss: 0.08676 test_loss: 0.10030 \n",
      "[192/200] train_loss: 0.07602 valid_loss: 0.08710 test_loss: 0.09954 \n",
      "[193/200] train_loss: 0.07574 valid_loss: 0.08620 test_loss: 0.09836 \n",
      "Validation loss decreased (0.086357 --> 0.086201).  Saving model ...\n",
      "[194/200] train_loss: 0.07513 valid_loss: 0.08595 test_loss: 0.09896 \n",
      "Validation loss decreased (0.086201 --> 0.085946).  Saving model ...\n",
      "[195/200] train_loss: 0.07645 valid_loss: 0.08844 test_loss: 0.09971 \n",
      "[196/200] train_loss: 0.07671 valid_loss: 0.08891 test_loss: 0.09970 \n",
      "[197/200] train_loss: 0.07630 valid_loss: 0.08622 test_loss: 0.09931 \n",
      "[198/200] train_loss: 0.07675 valid_loss: 0.08765 test_loss: 0.09982 \n",
      "[199/200] train_loss: 0.07610 valid_loss: 0.08818 test_loss: 0.09957 \n",
      "[200/200] train_loss: 0.07552 valid_loss: 0.08610 test_loss: 0.09790 \n",
      "Model 11 trained for seen data.\n",
      "TRAINING MODEL for seen house 12\n",
      "[  1/200] train_loss: 0.50036 valid_loss: 0.39636 test_loss: 0.40687 \n",
      "Validation loss decreased (inf --> 0.396363).  Saving model ...\n",
      "[  2/200] train_loss: 0.32105 valid_loss: 0.29976 test_loss: 0.31597 \n",
      "Validation loss decreased (0.396363 --> 0.299763).  Saving model ...\n",
      "[  3/200] train_loss: 0.25777 valid_loss: 0.24950 test_loss: 0.26703 \n",
      "Validation loss decreased (0.299763 --> 0.249502).  Saving model ...\n",
      "[  4/200] train_loss: 0.21890 valid_loss: 0.21877 test_loss: 0.23694 \n",
      "Validation loss decreased (0.249502 --> 0.218771).  Saving model ...\n",
      "[  5/200] train_loss: 0.19439 valid_loss: 0.19675 test_loss: 0.21345 \n",
      "Validation loss decreased (0.218771 --> 0.196754).  Saving model ...\n",
      "[  6/200] train_loss: 0.17957 valid_loss: 0.18276 test_loss: 0.19976 \n",
      "Validation loss decreased (0.196754 --> 0.182764).  Saving model ...\n",
      "[  7/200] train_loss: 0.17296 valid_loss: 0.17516 test_loss: 0.19160 \n",
      "Validation loss decreased (0.182764 --> 0.175163).  Saving model ...\n",
      "[  8/200] train_loss: 0.16548 valid_loss: 0.16933 test_loss: 0.18277 \n",
      "Validation loss decreased (0.175163 --> 0.169333).  Saving model ...\n",
      "[  9/200] train_loss: 0.15689 valid_loss: 0.16544 test_loss: 0.17802 \n",
      "Validation loss decreased (0.169333 --> 0.165439).  Saving model ...\n",
      "[ 10/200] train_loss: 0.15431 valid_loss: 0.16030 test_loss: 0.17195 \n",
      "Validation loss decreased (0.165439 --> 0.160300).  Saving model ...\n",
      "[ 11/200] train_loss: 0.14859 valid_loss: 0.15230 test_loss: 0.16666 \n",
      "Validation loss decreased (0.160300 --> 0.152302).  Saving model ...\n",
      "[ 12/200] train_loss: 0.14460 valid_loss: 0.15125 test_loss: 0.16368 \n",
      "Validation loss decreased (0.152302 --> 0.151248).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14337 valid_loss: 0.14872 test_loss: 0.16142 \n",
      "Validation loss decreased (0.151248 --> 0.148721).  Saving model ...\n",
      "[ 14/200] train_loss: 0.14261 valid_loss: 0.14862 test_loss: 0.16279 \n",
      "Validation loss decreased (0.148721 --> 0.148615).  Saving model ...\n",
      "[ 15/200] train_loss: 0.13661 valid_loss: 0.14297 test_loss: 0.15632 \n",
      "Validation loss decreased (0.148615 --> 0.142966).  Saving model ...\n",
      "[ 16/200] train_loss: 0.13472 valid_loss: 0.14438 test_loss: 0.15621 \n",
      "[ 17/200] train_loss: 0.13158 valid_loss: 0.14081 test_loss: 0.15377 \n",
      "Validation loss decreased (0.142966 --> 0.140815).  Saving model ...\n",
      "[ 18/200] train_loss: 0.13488 valid_loss: 0.14069 test_loss: 0.15332 \n",
      "Validation loss decreased (0.140815 --> 0.140687).  Saving model ...\n",
      "[ 19/200] train_loss: 0.13048 valid_loss: 0.14105 test_loss: 0.15093 \n",
      "[ 20/200] train_loss: 0.12754 valid_loss: 0.14091 test_loss: 0.15085 \n",
      "[ 21/200] train_loss: 0.12529 valid_loss: 0.13304 test_loss: 0.14509 \n",
      "Validation loss decreased (0.140687 --> 0.133042).  Saving model ...\n",
      "[ 22/200] train_loss: 0.12343 valid_loss: 0.13715 test_loss: 0.14763 \n",
      "[ 23/200] train_loss: 0.12219 valid_loss: 0.13276 test_loss: 0.14645 \n",
      "Validation loss decreased (0.133042 --> 0.132761).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12157 valid_loss: 0.13218 test_loss: 0.14487 \n",
      "Validation loss decreased (0.132761 --> 0.132184).  Saving model ...\n",
      "[ 25/200] train_loss: 0.12458 valid_loss: 0.13510 test_loss: 0.14550 \n",
      "[ 26/200] train_loss: 0.12022 valid_loss: 0.13067 test_loss: 0.14634 \n",
      "Validation loss decreased (0.132184 --> 0.130669).  Saving model ...\n",
      "[ 27/200] train_loss: 0.12298 valid_loss: 0.13184 test_loss: 0.14331 \n",
      "[ 28/200] train_loss: 0.11972 valid_loss: 0.12712 test_loss: 0.14023 \n",
      "Validation loss decreased (0.130669 --> 0.127118).  Saving model ...\n",
      "[ 29/200] train_loss: 0.11887 valid_loss: 0.12779 test_loss: 0.14350 \n",
      "[ 30/200] train_loss: 0.11623 valid_loss: 0.12881 test_loss: 0.14282 \n",
      "[ 31/200] train_loss: 0.11997 valid_loss: 0.12585 test_loss: 0.13793 \n",
      "Validation loss decreased (0.127118 --> 0.125849).  Saving model ...\n",
      "[ 32/200] train_loss: 0.11833 valid_loss: 0.12609 test_loss: 0.14106 \n",
      "[ 33/200] train_loss: 0.11592 valid_loss: 0.12236 test_loss: 0.13785 \n",
      "Validation loss decreased (0.125849 --> 0.122361).  Saving model ...\n",
      "[ 34/200] train_loss: 0.11263 valid_loss: 0.12344 test_loss: 0.13700 \n",
      "[ 35/200] train_loss: 0.11442 valid_loss: 0.11997 test_loss: 0.13596 \n",
      "Validation loss decreased (0.122361 --> 0.119971).  Saving model ...\n",
      "[ 36/200] train_loss: 0.11275 valid_loss: 0.12377 test_loss: 0.13547 \n",
      "[ 37/200] train_loss: 0.11558 valid_loss: 0.12073 test_loss: 0.13547 \n",
      "[ 38/200] train_loss: 0.11286 valid_loss: 0.12304 test_loss: 0.13425 \n",
      "[ 39/200] train_loss: 0.10860 valid_loss: 0.11707 test_loss: 0.13282 \n",
      "Validation loss decreased (0.119971 --> 0.117066).  Saving model ...\n",
      "[ 40/200] train_loss: 0.11191 valid_loss: 0.11829 test_loss: 0.13333 \n",
      "[ 41/200] train_loss: 0.11340 valid_loss: 0.11645 test_loss: 0.13199 \n",
      "Validation loss decreased (0.117066 --> 0.116445).  Saving model ...\n",
      "[ 42/200] train_loss: 0.10774 valid_loss: 0.11776 test_loss: 0.13095 \n",
      "[ 43/200] train_loss: 0.10715 valid_loss: 0.11708 test_loss: 0.13161 \n",
      "[ 44/200] train_loss: 0.10705 valid_loss: 0.11739 test_loss: 0.12947 \n",
      "[ 45/200] train_loss: 0.10745 valid_loss: 0.11514 test_loss: 0.12909 \n",
      "Validation loss decreased (0.116445 --> 0.115141).  Saving model ...\n",
      "[ 46/200] train_loss: 0.10497 valid_loss: 0.12220 test_loss: 0.13174 \n",
      "[ 47/200] train_loss: 0.10340 valid_loss: 0.11699 test_loss: 0.13078 \n",
      "[ 48/200] train_loss: 0.10575 valid_loss: 0.11281 test_loss: 0.12855 \n",
      "Validation loss decreased (0.115141 --> 0.112805).  Saving model ...\n",
      "[ 49/200] train_loss: 0.10460 valid_loss: 0.11661 test_loss: 0.12873 \n",
      "[ 50/200] train_loss: 0.10660 valid_loss: 0.11034 test_loss: 0.12725 \n",
      "Validation loss decreased (0.112805 --> 0.110342).  Saving model ...\n",
      "[ 51/200] train_loss: 0.10549 valid_loss: 0.11315 test_loss: 0.12991 \n",
      "[ 52/200] train_loss: 0.10087 valid_loss: 0.11049 test_loss: 0.12705 \n",
      "[ 53/200] train_loss: 0.10299 valid_loss: 0.11188 test_loss: 0.12774 \n",
      "[ 54/200] train_loss: 0.10410 valid_loss: 0.11127 test_loss: 0.12880 \n",
      "[ 55/200] train_loss: 0.10282 valid_loss: 0.11206 test_loss: 0.12539 \n",
      "[ 56/200] train_loss: 0.10392 valid_loss: 0.11272 test_loss: 0.12569 \n",
      "[ 57/200] train_loss: 0.10171 valid_loss: 0.10613 test_loss: 0.12329 \n",
      "Validation loss decreased (0.110342 --> 0.106133).  Saving model ...\n",
      "[ 58/200] train_loss: 0.10233 valid_loss: 0.10882 test_loss: 0.12390 \n",
      "[ 59/200] train_loss: 0.10146 valid_loss: 0.10749 test_loss: 0.12281 \n",
      "[ 60/200] train_loss: 0.10188 valid_loss: 0.11202 test_loss: 0.12453 \n",
      "[ 61/200] train_loss: 0.10044 valid_loss: 0.10787 test_loss: 0.12383 \n",
      "[ 62/200] train_loss: 0.10089 valid_loss: 0.10541 test_loss: 0.12166 \n",
      "Validation loss decreased (0.106133 --> 0.105406).  Saving model ...\n",
      "[ 63/200] train_loss: 0.10177 valid_loss: 0.11426 test_loss: 0.12223 \n",
      "[ 64/200] train_loss: 0.09969 valid_loss: 0.10869 test_loss: 0.12220 \n",
      "[ 65/200] train_loss: 0.10125 valid_loss: 0.10901 test_loss: 0.12258 \n",
      "[ 66/200] train_loss: 0.09945 valid_loss: 0.10445 test_loss: 0.12012 \n",
      "Validation loss decreased (0.105406 --> 0.104447).  Saving model ...\n",
      "[ 67/200] train_loss: 0.09874 valid_loss: 0.10624 test_loss: 0.12078 \n",
      "[ 68/200] train_loss: 0.09769 valid_loss: 0.10541 test_loss: 0.12023 \n",
      "[ 69/200] train_loss: 0.09697 valid_loss: 0.10540 test_loss: 0.12081 \n",
      "[ 70/200] train_loss: 0.09893 valid_loss: 0.10283 test_loss: 0.11992 \n",
      "Validation loss decreased (0.104447 --> 0.102835).  Saving model ...\n",
      "[ 71/200] train_loss: 0.09519 valid_loss: 0.10629 test_loss: 0.11869 \n",
      "[ 72/200] train_loss: 0.09848 valid_loss: 0.10655 test_loss: 0.12028 \n",
      "[ 73/200] train_loss: 0.09757 valid_loss: 0.10760 test_loss: 0.12056 \n",
      "[ 74/200] train_loss: 0.09427 valid_loss: 0.10125 test_loss: 0.11694 \n",
      "Validation loss decreased (0.102835 --> 0.101247).  Saving model ...\n",
      "[ 75/200] train_loss: 0.09833 valid_loss: 0.10805 test_loss: 0.11833 \n",
      "[ 76/200] train_loss: 0.09476 valid_loss: 0.10479 test_loss: 0.11794 \n",
      "[ 77/200] train_loss: 0.09840 valid_loss: 0.10511 test_loss: 0.11769 \n",
      "[ 78/200] train_loss: 0.09561 valid_loss: 0.10571 test_loss: 0.11778 \n",
      "[ 79/200] train_loss: 0.09451 valid_loss: 0.10541 test_loss: 0.11835 \n",
      "[ 80/200] train_loss: 0.09536 valid_loss: 0.10414 test_loss: 0.11700 \n",
      "[ 81/200] train_loss: 0.09312 valid_loss: 0.10231 test_loss: 0.11737 \n",
      "[ 82/200] train_loss: 0.09501 valid_loss: 0.10309 test_loss: 0.11587 \n",
      "[ 83/200] train_loss: 0.09421 valid_loss: 0.10200 test_loss: 0.11369 \n",
      "[ 84/200] train_loss: 0.09615 valid_loss: 0.09953 test_loss: 0.11712 \n",
      "Validation loss decreased (0.101247 --> 0.099535).  Saving model ...\n",
      "[ 85/200] train_loss: 0.09296 valid_loss: 0.09943 test_loss: 0.11489 \n",
      "Validation loss decreased (0.099535 --> 0.099433).  Saving model ...\n",
      "[ 86/200] train_loss: 0.09188 valid_loss: 0.10389 test_loss: 0.11529 \n",
      "[ 87/200] train_loss: 0.09301 valid_loss: 0.10050 test_loss: 0.11332 \n",
      "[ 88/200] train_loss: 0.09488 valid_loss: 0.10288 test_loss: 0.11413 \n",
      "[ 89/200] train_loss: 0.09154 valid_loss: 0.10175 test_loss: 0.11363 \n",
      "[ 90/200] train_loss: 0.09152 valid_loss: 0.09891 test_loss: 0.11240 \n",
      "Validation loss decreased (0.099433 --> 0.098907).  Saving model ...\n",
      "[ 91/200] train_loss: 0.09390 valid_loss: 0.10092 test_loss: 0.11262 \n",
      "[ 92/200] train_loss: 0.08996 valid_loss: 0.10132 test_loss: 0.11183 \n",
      "[ 93/200] train_loss: 0.09087 valid_loss: 0.10369 test_loss: 0.11202 \n",
      "[ 94/200] train_loss: 0.09214 valid_loss: 0.10060 test_loss: 0.11285 \n",
      "[ 95/200] train_loss: 0.08977 valid_loss: 0.09733 test_loss: 0.11195 \n",
      "Validation loss decreased (0.098907 --> 0.097331).  Saving model ...\n",
      "[ 96/200] train_loss: 0.09141 valid_loss: 0.09749 test_loss: 0.11237 \n",
      "[ 97/200] train_loss: 0.08937 valid_loss: 0.09621 test_loss: 0.11095 \n",
      "Validation loss decreased (0.097331 --> 0.096210).  Saving model ...\n",
      "[ 98/200] train_loss: 0.08918 valid_loss: 0.09677 test_loss: 0.11083 \n",
      "[ 99/200] train_loss: 0.09024 valid_loss: 0.09849 test_loss: 0.11084 \n",
      "[100/200] train_loss: 0.08974 valid_loss: 0.09566 test_loss: 0.11038 \n",
      "Validation loss decreased (0.096210 --> 0.095657).  Saving model ...\n",
      "[101/200] train_loss: 0.08993 valid_loss: 0.10380 test_loss: 0.10983 \n",
      "[102/200] train_loss: 0.08833 valid_loss: 0.09835 test_loss: 0.10926 \n",
      "[103/200] train_loss: 0.08764 valid_loss: 0.10072 test_loss: 0.11174 \n",
      "[104/200] train_loss: 0.08988 valid_loss: 0.09982 test_loss: 0.10863 \n",
      "[105/200] train_loss: 0.08942 valid_loss: 0.09661 test_loss: 0.11035 \n",
      "[106/200] train_loss: 0.08737 valid_loss: 0.09538 test_loss: 0.10851 \n",
      "Validation loss decreased (0.095657 --> 0.095379).  Saving model ...\n",
      "[107/200] train_loss: 0.08791 valid_loss: 0.09522 test_loss: 0.10920 \n",
      "Validation loss decreased (0.095379 --> 0.095222).  Saving model ...\n",
      "[108/200] train_loss: 0.08797 valid_loss: 0.09740 test_loss: 0.10954 \n",
      "[109/200] train_loss: 0.08678 valid_loss: 0.09650 test_loss: 0.10943 \n",
      "[110/200] train_loss: 0.08578 valid_loss: 0.09608 test_loss: 0.10856 \n",
      "[111/200] train_loss: 0.08626 valid_loss: 0.09436 test_loss: 0.10836 \n",
      "Validation loss decreased (0.095222 --> 0.094358).  Saving model ...\n",
      "[112/200] train_loss: 0.08736 valid_loss: 0.09522 test_loss: 0.10743 \n",
      "[113/200] train_loss: 0.08604 valid_loss: 0.09842 test_loss: 0.10936 \n",
      "[114/200] train_loss: 0.08619 valid_loss: 0.09909 test_loss: 0.10880 \n",
      "[115/200] train_loss: 0.08758 valid_loss: 0.09457 test_loss: 0.10869 \n",
      "[116/200] train_loss: 0.08690 valid_loss: 0.09642 test_loss: 0.10760 \n",
      "[117/200] train_loss: 0.08571 valid_loss: 0.09544 test_loss: 0.10823 \n",
      "[118/200] train_loss: 0.08732 valid_loss: 0.09442 test_loss: 0.10706 \n",
      "[119/200] train_loss: 0.08520 valid_loss: 0.09357 test_loss: 0.10657 \n",
      "Validation loss decreased (0.094358 --> 0.093569).  Saving model ...\n",
      "[120/200] train_loss: 0.08739 valid_loss: 0.09372 test_loss: 0.10693 \n",
      "[121/200] train_loss: 0.08597 valid_loss: 0.09747 test_loss: 0.10551 \n",
      "[122/200] train_loss: 0.08380 valid_loss: 0.09366 test_loss: 0.10622 \n",
      "[123/200] train_loss: 0.08357 valid_loss: 0.09385 test_loss: 0.10655 \n",
      "[124/200] train_loss: 0.08641 valid_loss: 0.09523 test_loss: 0.10840 \n",
      "[125/200] train_loss: 0.08302 valid_loss: 0.09383 test_loss: 0.10653 \n",
      "[126/200] train_loss: 0.08622 valid_loss: 0.09213 test_loss: 0.10609 \n",
      "Validation loss decreased (0.093569 --> 0.092133).  Saving model ...\n",
      "[127/200] train_loss: 0.08337 valid_loss: 0.09219 test_loss: 0.10663 \n",
      "[128/200] train_loss: 0.08245 valid_loss: 0.09051 test_loss: 0.10510 \n",
      "Validation loss decreased (0.092133 --> 0.090506).  Saving model ...\n",
      "[129/200] train_loss: 0.08571 valid_loss: 0.09410 test_loss: 0.10579 \n",
      "[130/200] train_loss: 0.08630 valid_loss: 0.09339 test_loss: 0.10607 \n",
      "[131/200] train_loss: 0.08380 valid_loss: 0.09391 test_loss: 0.10567 \n",
      "[132/200] train_loss: 0.08409 valid_loss: 0.09441 test_loss: 0.10680 \n",
      "[133/200] train_loss: 0.08261 valid_loss: 0.09489 test_loss: 0.10564 \n",
      "[134/200] train_loss: 0.08451 valid_loss: 0.09079 test_loss: 0.10437 \n",
      "[135/200] train_loss: 0.08450 valid_loss: 0.09757 test_loss: 0.10503 \n",
      "[136/200] train_loss: 0.08179 valid_loss: 0.09239 test_loss: 0.10564 \n",
      "[137/200] train_loss: 0.08268 valid_loss: 0.09017 test_loss: 0.10354 \n",
      "Validation loss decreased (0.090506 --> 0.090168).  Saving model ...\n",
      "[138/200] train_loss: 0.08420 valid_loss: 0.09551 test_loss: 0.10451 \n",
      "[139/200] train_loss: 0.08244 valid_loss: 0.09180 test_loss: 0.10431 \n",
      "[140/200] train_loss: 0.08087 valid_loss: 0.09134 test_loss: 0.10438 \n",
      "[141/200] train_loss: 0.08327 valid_loss: 0.09253 test_loss: 0.10449 \n",
      "[142/200] train_loss: 0.08452 valid_loss: 0.09158 test_loss: 0.10383 \n",
      "[143/200] train_loss: 0.08348 valid_loss: 0.09170 test_loss: 0.10353 \n",
      "[144/200] train_loss: 0.08068 valid_loss: 0.09001 test_loss: 0.10352 \n",
      "Validation loss decreased (0.090168 --> 0.090006).  Saving model ...\n",
      "[145/200] train_loss: 0.08174 valid_loss: 0.09229 test_loss: 0.10328 \n",
      "[146/200] train_loss: 0.08027 valid_loss: 0.09384 test_loss: 0.10343 \n",
      "[147/200] train_loss: 0.08055 valid_loss: 0.09107 test_loss: 0.10266 \n",
      "[148/200] train_loss: 0.08027 valid_loss: 0.09076 test_loss: 0.10359 \n",
      "[149/200] train_loss: 0.08181 valid_loss: 0.09711 test_loss: 0.10259 \n",
      "[150/200] train_loss: 0.07907 valid_loss: 0.09121 test_loss: 0.10312 \n",
      "[151/200] train_loss: 0.08416 valid_loss: 0.09061 test_loss: 0.10246 \n",
      "[152/200] train_loss: 0.08178 valid_loss: 0.08912 test_loss: 0.10151 \n",
      "Validation loss decreased (0.090006 --> 0.089119).  Saving model ...\n",
      "[153/200] train_loss: 0.07925 valid_loss: 0.09056 test_loss: 0.10220 \n",
      "[154/200] train_loss: 0.08100 valid_loss: 0.09120 test_loss: 0.10345 \n",
      "[155/200] train_loss: 0.08326 valid_loss: 0.09006 test_loss: 0.10278 \n",
      "[156/200] train_loss: 0.08131 valid_loss: 0.09042 test_loss: 0.10236 \n",
      "[157/200] train_loss: 0.07930 valid_loss: 0.09075 test_loss: 0.10254 \n",
      "[158/200] train_loss: 0.07865 valid_loss: 0.09061 test_loss: 0.10229 \n",
      "[159/200] train_loss: 0.07977 valid_loss: 0.08904 test_loss: 0.10143 \n",
      "Validation loss decreased (0.089119 --> 0.089036).  Saving model ...\n",
      "[160/200] train_loss: 0.08045 valid_loss: 0.08699 test_loss: 0.10078 \n",
      "Validation loss decreased (0.089036 --> 0.086988).  Saving model ...\n",
      "[161/200] train_loss: 0.08024 valid_loss: 0.08812 test_loss: 0.10056 \n",
      "[162/200] train_loss: 0.08041 valid_loss: 0.09130 test_loss: 0.10162 \n",
      "[163/200] train_loss: 0.08057 valid_loss: 0.08788 test_loss: 0.10100 \n",
      "[164/200] train_loss: 0.08348 valid_loss: 0.09060 test_loss: 0.10210 \n",
      "[165/200] train_loss: 0.08221 valid_loss: 0.08886 test_loss: 0.10198 \n",
      "[166/200] train_loss: 0.08160 valid_loss: 0.08780 test_loss: 0.10245 \n",
      "[167/200] train_loss: 0.07989 valid_loss: 0.08913 test_loss: 0.10167 \n",
      "[168/200] train_loss: 0.08068 valid_loss: 0.08799 test_loss: 0.10101 \n",
      "[169/200] train_loss: 0.07882 valid_loss: 0.08950 test_loss: 0.10142 \n",
      "[170/200] train_loss: 0.07839 valid_loss: 0.09023 test_loss: 0.10182 \n",
      "[171/200] train_loss: 0.07724 valid_loss: 0.08975 test_loss: 0.10121 \n",
      "[172/200] train_loss: 0.07781 valid_loss: 0.08932 test_loss: 0.09957 \n",
      "[173/200] train_loss: 0.07885 valid_loss: 0.08954 test_loss: 0.10095 \n",
      "[174/200] train_loss: 0.07722 valid_loss: 0.08644 test_loss: 0.10024 \n",
      "Validation loss decreased (0.086988 --> 0.086438).  Saving model ...\n",
      "[175/200] train_loss: 0.07779 valid_loss: 0.08700 test_loss: 0.09909 \n",
      "[176/200] train_loss: 0.07750 valid_loss: 0.08863 test_loss: 0.10046 \n",
      "[177/200] train_loss: 0.07948 valid_loss: 0.08878 test_loss: 0.09987 \n",
      "[178/200] train_loss: 0.07398 valid_loss: 0.08933 test_loss: 0.10233 \n",
      "[179/200] train_loss: 0.07719 valid_loss: 0.08824 test_loss: 0.09868 \n",
      "[180/200] train_loss: 0.08108 valid_loss: 0.09262 test_loss: 0.10621 \n",
      "[181/200] train_loss: 0.07647 valid_loss: 0.09731 test_loss: 0.09918 \n",
      "[182/200] train_loss: 0.07813 valid_loss: 0.08609 test_loss: 0.09885 \n",
      "Validation loss decreased (0.086438 --> 0.086085).  Saving model ...\n",
      "[183/200] train_loss: 0.07641 valid_loss: 0.08705 test_loss: 0.10048 \n",
      "[184/200] train_loss: 0.07850 valid_loss: 0.08826 test_loss: 0.09886 \n",
      "[185/200] train_loss: 0.08116 valid_loss: 0.08594 test_loss: 0.09847 \n",
      "Validation loss decreased (0.086085 --> 0.085944).  Saving model ...\n",
      "[186/200] train_loss: 0.07872 valid_loss: 0.09424 test_loss: 0.09935 \n",
      "[187/200] train_loss: 0.07604 valid_loss: 0.08719 test_loss: 0.09876 \n",
      "[188/200] train_loss: 0.07744 valid_loss: 0.08951 test_loss: 0.09976 \n",
      "[189/200] train_loss: 0.07710 valid_loss: 0.08621 test_loss: 0.09780 \n",
      "[190/200] train_loss: 0.07747 valid_loss: 0.08791 test_loss: 0.09781 \n",
      "[191/200] train_loss: 0.07664 valid_loss: 0.08529 test_loss: 0.09773 \n",
      "Validation loss decreased (0.085944 --> 0.085291).  Saving model ...\n",
      "[192/200] train_loss: 0.07668 valid_loss: 0.08720 test_loss: 0.09888 \n",
      "[193/200] train_loss: 0.07677 valid_loss: 0.08550 test_loss: 0.09695 \n",
      "[194/200] train_loss: 0.07492 valid_loss: 0.08529 test_loss: 0.09828 \n",
      "Validation loss decreased (0.085291 --> 0.085291).  Saving model ...\n",
      "[195/200] train_loss: 0.07702 valid_loss: 0.08405 test_loss: 0.09770 \n",
      "Validation loss decreased (0.085291 --> 0.084052).  Saving model ...\n",
      "[196/200] train_loss: 0.07632 valid_loss: 0.08775 test_loss: 0.09858 \n",
      "[197/200] train_loss: 0.07634 valid_loss: 0.08766 test_loss: 0.09895 \n",
      "[198/200] train_loss: 0.07773 valid_loss: 0.09090 test_loss: 0.09876 \n",
      "[199/200] train_loss: 0.07634 valid_loss: 0.08710 test_loss: 0.09872 \n",
      "[200/200] train_loss: 0.07877 valid_loss: 0.08567 test_loss: 0.09832 \n",
      "Model 12 trained for seen data.\n",
      "TRAINING MODEL for seen house 13\n",
      "[  1/200] train_loss: 0.61456 valid_loss: 0.48951 test_loss: 0.47681 \n",
      "Validation loss decreased (inf --> 0.489510).  Saving model ...\n",
      "[  2/200] train_loss: 0.40318 valid_loss: 0.35661 test_loss: 0.35724 \n",
      "Validation loss decreased (0.489510 --> 0.356613).  Saving model ...\n",
      "[  3/200] train_loss: 0.31134 valid_loss: 0.29739 test_loss: 0.30406 \n",
      "Validation loss decreased (0.356613 --> 0.297387).  Saving model ...\n",
      "[  4/200] train_loss: 0.26306 valid_loss: 0.25595 test_loss: 0.26660 \n",
      "Validation loss decreased (0.297387 --> 0.255952).  Saving model ...\n",
      "[  5/200] train_loss: 0.22751 valid_loss: 0.22546 test_loss: 0.23662 \n",
      "Validation loss decreased (0.255952 --> 0.225455).  Saving model ...\n",
      "[  6/200] train_loss: 0.20387 valid_loss: 0.20658 test_loss: 0.21706 \n",
      "Validation loss decreased (0.225455 --> 0.206583).  Saving model ...\n",
      "[  7/200] train_loss: 0.18806 valid_loss: 0.18855 test_loss: 0.20104 \n",
      "Validation loss decreased (0.206583 --> 0.188552).  Saving model ...\n",
      "[  8/200] train_loss: 0.17503 valid_loss: 0.18052 test_loss: 0.19298 \n",
      "Validation loss decreased (0.188552 --> 0.180517).  Saving model ...\n",
      "[  9/200] train_loss: 0.16508 valid_loss: 0.17517 test_loss: 0.18538 \n",
      "Validation loss decreased (0.180517 --> 0.175174).  Saving model ...\n",
      "[ 10/200] train_loss: 0.15965 valid_loss: 0.16725 test_loss: 0.17670 \n",
      "Validation loss decreased (0.175174 --> 0.167247).  Saving model ...\n",
      "[ 11/200] train_loss: 0.15874 valid_loss: 0.16381 test_loss: 0.17236 \n",
      "Validation loss decreased (0.167247 --> 0.163810).  Saving model ...\n",
      "[ 12/200] train_loss: 0.15294 valid_loss: 0.16209 test_loss: 0.16897 \n",
      "Validation loss decreased (0.163810 --> 0.162087).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14862 valid_loss: 0.15561 test_loss: 0.16540 \n",
      "Validation loss decreased (0.162087 --> 0.155605).  Saving model ...\n",
      "[ 14/200] train_loss: 0.14818 valid_loss: 0.15169 test_loss: 0.16036 \n",
      "Validation loss decreased (0.155605 --> 0.151693).  Saving model ...\n",
      "[ 15/200] train_loss: 0.14117 valid_loss: 0.15084 test_loss: 0.15920 \n",
      "Validation loss decreased (0.151693 --> 0.150841).  Saving model ...\n",
      "[ 16/200] train_loss: 0.13867 valid_loss: 0.15214 test_loss: 0.16129 \n",
      "[ 17/200] train_loss: 0.13622 valid_loss: 0.14537 test_loss: 0.15399 \n",
      "Validation loss decreased (0.150841 --> 0.145374).  Saving model ...\n",
      "[ 18/200] train_loss: 0.13166 valid_loss: 0.14808 test_loss: 0.15312 \n",
      "[ 19/200] train_loss: 0.12973 valid_loss: 0.14250 test_loss: 0.15175 \n",
      "Validation loss decreased (0.145374 --> 0.142496).  Saving model ...\n",
      "[ 20/200] train_loss: 0.13406 valid_loss: 0.14045 test_loss: 0.15056 \n",
      "Validation loss decreased (0.142496 --> 0.140446).  Saving model ...\n",
      "[ 21/200] train_loss: 0.12783 valid_loss: 0.14336 test_loss: 0.14912 \n",
      "[ 22/200] train_loss: 0.12662 valid_loss: 0.14172 test_loss: 0.14800 \n",
      "[ 23/200] train_loss: 0.12763 valid_loss: 0.13882 test_loss: 0.14848 \n",
      "Validation loss decreased (0.140446 --> 0.138820).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12544 valid_loss: 0.14294 test_loss: 0.14813 \n",
      "[ 25/200] train_loss: 0.12391 valid_loss: 0.13636 test_loss: 0.14466 \n",
      "Validation loss decreased (0.138820 --> 0.136358).  Saving model ...\n",
      "[ 26/200] train_loss: 0.11931 valid_loss: 0.13383 test_loss: 0.14281 \n",
      "Validation loss decreased (0.136358 --> 0.133832).  Saving model ...\n",
      "[ 27/200] train_loss: 0.12515 valid_loss: 0.13604 test_loss: 0.14316 \n",
      "[ 28/200] train_loss: 0.11816 valid_loss: 0.12929 test_loss: 0.14003 \n",
      "Validation loss decreased (0.133832 --> 0.129290).  Saving model ...\n",
      "[ 29/200] train_loss: 0.11683 valid_loss: 0.12872 test_loss: 0.13957 \n",
      "Validation loss decreased (0.129290 --> 0.128717).  Saving model ...\n",
      "[ 30/200] train_loss: 0.11803 valid_loss: 0.12867 test_loss: 0.13938 \n",
      "Validation loss decreased (0.128717 --> 0.128673).  Saving model ...\n",
      "[ 31/200] train_loss: 0.11548 valid_loss: 0.12835 test_loss: 0.13965 \n",
      "Validation loss decreased (0.128673 --> 0.128347).  Saving model ...\n",
      "[ 32/200] train_loss: 0.11626 valid_loss: 0.12529 test_loss: 0.13740 \n",
      "Validation loss decreased (0.128347 --> 0.125291).  Saving model ...\n",
      "[ 33/200] train_loss: 0.11424 valid_loss: 0.12467 test_loss: 0.13962 \n",
      "Validation loss decreased (0.125291 --> 0.124671).  Saving model ...\n",
      "[ 34/200] train_loss: 0.11614 valid_loss: 0.12621 test_loss: 0.14056 \n",
      "[ 35/200] train_loss: 0.11849 valid_loss: 0.12477 test_loss: 0.13624 \n",
      "[ 36/200] train_loss: 0.11419 valid_loss: 0.12650 test_loss: 0.13606 \n",
      "[ 37/200] train_loss: 0.11054 valid_loss: 0.12405 test_loss: 0.13608 \n",
      "Validation loss decreased (0.124671 --> 0.124048).  Saving model ...\n",
      "[ 38/200] train_loss: 0.11132 valid_loss: 0.12198 test_loss: 0.13453 \n",
      "Validation loss decreased (0.124048 --> 0.121978).  Saving model ...\n",
      "[ 39/200] train_loss: 0.11059 valid_loss: 0.12036 test_loss: 0.13339 \n",
      "Validation loss decreased (0.121978 --> 0.120359).  Saving model ...\n",
      "[ 40/200] train_loss: 0.10966 valid_loss: 0.12001 test_loss: 0.13470 \n",
      "Validation loss decreased (0.120359 --> 0.120007).  Saving model ...\n",
      "[ 41/200] train_loss: 0.10998 valid_loss: 0.11940 test_loss: 0.13156 \n",
      "Validation loss decreased (0.120007 --> 0.119400).  Saving model ...\n",
      "[ 42/200] train_loss: 0.11016 valid_loss: 0.11840 test_loss: 0.13123 \n",
      "Validation loss decreased (0.119400 --> 0.118403).  Saving model ...\n",
      "[ 43/200] train_loss: 0.10965 valid_loss: 0.11649 test_loss: 0.13197 \n",
      "Validation loss decreased (0.118403 --> 0.116490).  Saving model ...\n",
      "[ 44/200] train_loss: 0.11158 valid_loss: 0.11716 test_loss: 0.13047 \n",
      "[ 45/200] train_loss: 0.10786 valid_loss: 0.12211 test_loss: 0.13477 \n",
      "[ 46/200] train_loss: 0.10927 valid_loss: 0.12099 test_loss: 0.12969 \n",
      "[ 47/200] train_loss: 0.10634 valid_loss: 0.11849 test_loss: 0.13144 \n",
      "[ 48/200] train_loss: 0.10530 valid_loss: 0.11703 test_loss: 0.13022 \n",
      "[ 49/200] train_loss: 0.10941 valid_loss: 0.11669 test_loss: 0.12720 \n",
      "[ 50/200] train_loss: 0.10332 valid_loss: 0.11671 test_loss: 0.12809 \n",
      "[ 51/200] train_loss: 0.10414 valid_loss: 0.11338 test_loss: 0.12676 \n",
      "Validation loss decreased (0.116490 --> 0.113383).  Saving model ...\n",
      "[ 52/200] train_loss: 0.10524 valid_loss: 0.11080 test_loss: 0.12753 \n",
      "Validation loss decreased (0.113383 --> 0.110805).  Saving model ...\n",
      "[ 53/200] train_loss: 0.10599 valid_loss: 0.11364 test_loss: 0.12667 \n",
      "[ 54/200] train_loss: 0.09947 valid_loss: 0.11566 test_loss: 0.12610 \n",
      "[ 55/200] train_loss: 0.10381 valid_loss: 0.11157 test_loss: 0.12659 \n",
      "[ 56/200] train_loss: 0.10201 valid_loss: 0.11623 test_loss: 0.12634 \n",
      "[ 57/200] train_loss: 0.10343 valid_loss: 0.11133 test_loss: 0.12481 \n",
      "[ 58/200] train_loss: 0.10184 valid_loss: 0.11067 test_loss: 0.12591 \n",
      "Validation loss decreased (0.110805 --> 0.110672).  Saving model ...\n",
      "[ 59/200] train_loss: 0.10493 valid_loss: 0.11199 test_loss: 0.12550 \n",
      "[ 60/200] train_loss: 0.10019 valid_loss: 0.10943 test_loss: 0.12372 \n",
      "Validation loss decreased (0.110672 --> 0.109430).  Saving model ...\n",
      "[ 61/200] train_loss: 0.09916 valid_loss: 0.11262 test_loss: 0.12442 \n",
      "[ 62/200] train_loss: 0.10113 valid_loss: 0.11400 test_loss: 0.12257 \n",
      "[ 63/200] train_loss: 0.09929 valid_loss: 0.11139 test_loss: 0.12452 \n",
      "[ 64/200] train_loss: 0.10038 valid_loss: 0.10877 test_loss: 0.12371 \n",
      "Validation loss decreased (0.109430 --> 0.108769).  Saving model ...\n",
      "[ 65/200] train_loss: 0.09801 valid_loss: 0.11497 test_loss: 0.12150 \n",
      "[ 66/200] train_loss: 0.10140 valid_loss: 0.10617 test_loss: 0.12044 \n",
      "Validation loss decreased (0.108769 --> 0.106168).  Saving model ...\n",
      "[ 67/200] train_loss: 0.09856 valid_loss: 0.10546 test_loss: 0.11907 \n",
      "Validation loss decreased (0.106168 --> 0.105463).  Saving model ...\n",
      "[ 68/200] train_loss: 0.09792 valid_loss: 0.10678 test_loss: 0.11996 \n",
      "[ 69/200] train_loss: 0.09737 valid_loss: 0.10659 test_loss: 0.12152 \n",
      "[ 70/200] train_loss: 0.09769 valid_loss: 0.10640 test_loss: 0.12023 \n",
      "[ 71/200] train_loss: 0.09657 valid_loss: 0.10726 test_loss: 0.11989 \n",
      "[ 72/200] train_loss: 0.09356 valid_loss: 0.10897 test_loss: 0.12284 \n",
      "[ 73/200] train_loss: 0.09610 valid_loss: 0.10803 test_loss: 0.12196 \n",
      "[ 74/200] train_loss: 0.09640 valid_loss: 0.10507 test_loss: 0.11930 \n",
      "Validation loss decreased (0.105463 --> 0.105070).  Saving model ...\n",
      "[ 75/200] train_loss: 0.09257 valid_loss: 0.10997 test_loss: 0.12016 \n",
      "[ 76/200] train_loss: 0.09425 valid_loss: 0.10525 test_loss: 0.11917 \n",
      "[ 77/200] train_loss: 0.09510 valid_loss: 0.10142 test_loss: 0.11649 \n",
      "Validation loss decreased (0.105070 --> 0.101422).  Saving model ...\n",
      "[ 78/200] train_loss: 0.09467 valid_loss: 0.10859 test_loss: 0.11654 \n",
      "[ 79/200] train_loss: 0.09543 valid_loss: 0.10455 test_loss: 0.11559 \n",
      "[ 80/200] train_loss: 0.09599 valid_loss: 0.10450 test_loss: 0.11651 \n",
      "[ 81/200] train_loss: 0.09399 valid_loss: 0.10214 test_loss: 0.11618 \n",
      "[ 82/200] train_loss: 0.09419 valid_loss: 0.10228 test_loss: 0.11723 \n",
      "[ 83/200] train_loss: 0.09413 valid_loss: 0.10038 test_loss: 0.11432 \n",
      "Validation loss decreased (0.101422 --> 0.100377).  Saving model ...\n",
      "[ 84/200] train_loss: 0.09187 valid_loss: 0.10039 test_loss: 0.11527 \n",
      "[ 85/200] train_loss: 0.09489 valid_loss: 0.10446 test_loss: 0.11911 \n",
      "[ 86/200] train_loss: 0.09243 valid_loss: 0.10096 test_loss: 0.11538 \n",
      "[ 87/200] train_loss: 0.09265 valid_loss: 0.09962 test_loss: 0.11402 \n",
      "Validation loss decreased (0.100377 --> 0.099617).  Saving model ...\n",
      "[ 88/200] train_loss: 0.09239 valid_loss: 0.09979 test_loss: 0.11554 \n",
      "[ 89/200] train_loss: 0.09253 valid_loss: 0.10119 test_loss: 0.11533 \n",
      "[ 90/200] train_loss: 0.09201 valid_loss: 0.10151 test_loss: 0.11632 \n",
      "[ 91/200] train_loss: 0.09152 valid_loss: 0.09906 test_loss: 0.11316 \n",
      "Validation loss decreased (0.099617 --> 0.099058).  Saving model ...\n",
      "[ 92/200] train_loss: 0.09195 valid_loss: 0.10011 test_loss: 0.11485 \n",
      "[ 93/200] train_loss: 0.09133 valid_loss: 0.10119 test_loss: 0.11396 \n",
      "[ 94/200] train_loss: 0.09438 valid_loss: 0.09933 test_loss: 0.11362 \n",
      "[ 95/200] train_loss: 0.08996 valid_loss: 0.10144 test_loss: 0.11401 \n",
      "[ 96/200] train_loss: 0.08903 valid_loss: 0.10042 test_loss: 0.11380 \n",
      "[ 97/200] train_loss: 0.09020 valid_loss: 0.10113 test_loss: 0.11338 \n",
      "[ 98/200] train_loss: 0.09049 valid_loss: 0.09634 test_loss: 0.11192 \n",
      "Validation loss decreased (0.099058 --> 0.096335).  Saving model ...\n",
      "[ 99/200] train_loss: 0.09067 valid_loss: 0.10094 test_loss: 0.11319 \n",
      "[100/200] train_loss: 0.08996 valid_loss: 0.09880 test_loss: 0.11040 \n",
      "[101/200] train_loss: 0.09223 valid_loss: 0.09801 test_loss: 0.11222 \n",
      "[102/200] train_loss: 0.08875 valid_loss: 0.09779 test_loss: 0.11172 \n",
      "[103/200] train_loss: 0.09082 valid_loss: 0.09695 test_loss: 0.11133 \n",
      "[104/200] train_loss: 0.08809 valid_loss: 0.09599 test_loss: 0.10989 \n",
      "Validation loss decreased (0.096335 --> 0.095985).  Saving model ...\n",
      "[105/200] train_loss: 0.09187 valid_loss: 0.09876 test_loss: 0.10972 \n",
      "[106/200] train_loss: 0.08738 valid_loss: 0.09764 test_loss: 0.11061 \n",
      "[107/200] train_loss: 0.08878 valid_loss: 0.09585 test_loss: 0.10947 \n",
      "Validation loss decreased (0.095985 --> 0.095850).  Saving model ...\n",
      "[108/200] train_loss: 0.08608 valid_loss: 0.09671 test_loss: 0.11018 \n",
      "[109/200] train_loss: 0.08688 valid_loss: 0.09623 test_loss: 0.11033 \n",
      "[110/200] train_loss: 0.08646 valid_loss: 0.10469 test_loss: 0.11176 \n",
      "[111/200] train_loss: 0.08804 valid_loss: 0.09714 test_loss: 0.10890 \n",
      "[112/200] train_loss: 0.08902 valid_loss: 0.09879 test_loss: 0.11012 \n",
      "[113/200] train_loss: 0.08638 valid_loss: 0.09976 test_loss: 0.10999 \n",
      "[114/200] train_loss: 0.08924 valid_loss: 0.10215 test_loss: 0.10986 \n",
      "[115/200] train_loss: 0.08465 valid_loss: 0.09717 test_loss: 0.10897 \n",
      "[116/200] train_loss: 0.08787 valid_loss: 0.09521 test_loss: 0.10862 \n",
      "Validation loss decreased (0.095850 --> 0.095209).  Saving model ...\n",
      "[117/200] train_loss: 0.08652 valid_loss: 0.09519 test_loss: 0.10716 \n",
      "Validation loss decreased (0.095209 --> 0.095187).  Saving model ...\n",
      "[118/200] train_loss: 0.08598 valid_loss: 0.09766 test_loss: 0.10976 \n",
      "[119/200] train_loss: 0.08535 valid_loss: 0.09596 test_loss: 0.10851 \n",
      "[120/200] train_loss: 0.08638 valid_loss: 0.09527 test_loss: 0.10849 \n",
      "[121/200] train_loss: 0.08358 valid_loss: 0.09601 test_loss: 0.10799 \n",
      "[122/200] train_loss: 0.08361 valid_loss: 0.09510 test_loss: 0.10678 \n",
      "Validation loss decreased (0.095187 --> 0.095099).  Saving model ...\n",
      "[123/200] train_loss: 0.08721 valid_loss: 0.09574 test_loss: 0.10647 \n",
      "[124/200] train_loss: 0.08575 valid_loss: 0.09348 test_loss: 0.10642 \n",
      "Validation loss decreased (0.095099 --> 0.093482).  Saving model ...\n",
      "[125/200] train_loss: 0.08678 valid_loss: 0.09715 test_loss: 0.10776 \n",
      "[126/200] train_loss: 0.08394 valid_loss: 0.09516 test_loss: 0.10727 \n",
      "[127/200] train_loss: 0.08612 valid_loss: 0.09442 test_loss: 0.10592 \n",
      "[128/200] train_loss: 0.08370 valid_loss: 0.09439 test_loss: 0.10678 \n",
      "[129/200] train_loss: 0.08447 valid_loss: 0.09363 test_loss: 0.10629 \n",
      "[130/200] train_loss: 0.08433 valid_loss: 0.09360 test_loss: 0.10687 \n",
      "[131/200] train_loss: 0.08312 valid_loss: 0.09218 test_loss: 0.10553 \n",
      "Validation loss decreased (0.093482 --> 0.092175).  Saving model ...\n",
      "[132/200] train_loss: 0.08506 valid_loss: 0.09491 test_loss: 0.10767 \n",
      "[133/200] train_loss: 0.08064 valid_loss: 0.09287 test_loss: 0.10548 \n",
      "[134/200] train_loss: 0.08473 valid_loss: 0.09242 test_loss: 0.10584 \n",
      "[135/200] train_loss: 0.08353 valid_loss: 0.09197 test_loss: 0.10515 \n",
      "Validation loss decreased (0.092175 --> 0.091971).  Saving model ...\n",
      "[136/200] train_loss: 0.08251 valid_loss: 0.09304 test_loss: 0.10518 \n",
      "[137/200] train_loss: 0.08571 valid_loss: 0.09465 test_loss: 0.10481 \n",
      "[138/200] train_loss: 0.08361 valid_loss: 0.09381 test_loss: 0.10591 \n",
      "[139/200] train_loss: 0.08339 valid_loss: 0.09289 test_loss: 0.10464 \n",
      "[140/200] train_loss: 0.08198 valid_loss: 0.09184 test_loss: 0.10477 \n",
      "Validation loss decreased (0.091971 --> 0.091840).  Saving model ...\n",
      "[141/200] train_loss: 0.08252 valid_loss: 0.09268 test_loss: 0.10592 \n",
      "[142/200] train_loss: 0.08458 valid_loss: 0.09300 test_loss: 0.10412 \n",
      "[143/200] train_loss: 0.08257 valid_loss: 0.09422 test_loss: 0.10425 \n",
      "[144/200] train_loss: 0.08133 valid_loss: 0.09219 test_loss: 0.10486 \n",
      "[145/200] train_loss: 0.08190 valid_loss: 0.09170 test_loss: 0.10415 \n",
      "Validation loss decreased (0.091840 --> 0.091699).  Saving model ...\n",
      "[146/200] train_loss: 0.08413 valid_loss: 0.09125 test_loss: 0.10411 \n",
      "Validation loss decreased (0.091699 --> 0.091254).  Saving model ...\n",
      "[147/200] train_loss: 0.08515 valid_loss: 0.09703 test_loss: 0.10365 \n",
      "[148/200] train_loss: 0.07848 valid_loss: 0.09320 test_loss: 0.10665 \n",
      "[149/200] train_loss: 0.08170 valid_loss: 0.09485 test_loss: 0.10432 \n",
      "[150/200] train_loss: 0.08082 valid_loss: 0.09263 test_loss: 0.10409 \n",
      "[151/200] train_loss: 0.08166 valid_loss: 0.09278 test_loss: 0.10421 \n",
      "[152/200] train_loss: 0.08329 valid_loss: 0.09045 test_loss: 0.10375 \n",
      "Validation loss decreased (0.091254 --> 0.090452).  Saving model ...\n",
      "[153/200] train_loss: 0.07902 valid_loss: 0.09227 test_loss: 0.10357 \n",
      "[154/200] train_loss: 0.08295 valid_loss: 0.09287 test_loss: 0.10359 \n",
      "[155/200] train_loss: 0.08238 valid_loss: 0.09109 test_loss: 0.10376 \n",
      "[156/200] train_loss: 0.08060 valid_loss: 0.09212 test_loss: 0.10384 \n",
      "[157/200] train_loss: 0.08271 valid_loss: 0.09297 test_loss: 0.10268 \n",
      "[158/200] train_loss: 0.08276 valid_loss: 0.09154 test_loss: 0.10402 \n",
      "[159/200] train_loss: 0.08024 valid_loss: 0.09194 test_loss: 0.10429 \n",
      "[160/200] train_loss: 0.07970 valid_loss: 0.09416 test_loss: 0.10324 \n",
      "[161/200] train_loss: 0.07884 valid_loss: 0.09036 test_loss: 0.10203 \n",
      "Validation loss decreased (0.090452 --> 0.090364).  Saving model ...\n",
      "[162/200] train_loss: 0.08074 valid_loss: 0.09012 test_loss: 0.10232 \n",
      "Validation loss decreased (0.090364 --> 0.090124).  Saving model ...\n",
      "[163/200] train_loss: 0.07801 valid_loss: 0.09159 test_loss: 0.10331 \n",
      "[164/200] train_loss: 0.08006 valid_loss: 0.09094 test_loss: 0.10363 \n",
      "[165/200] train_loss: 0.08003 valid_loss: 0.08957 test_loss: 0.10292 \n",
      "Validation loss decreased (0.090124 --> 0.089567).  Saving model ...\n",
      "[166/200] train_loss: 0.07929 valid_loss: 0.09065 test_loss: 0.10232 \n",
      "[167/200] train_loss: 0.07980 valid_loss: 0.09064 test_loss: 0.10247 \n",
      "[168/200] train_loss: 0.07693 valid_loss: 0.09132 test_loss: 0.10306 \n",
      "[169/200] train_loss: 0.08035 valid_loss: 0.08960 test_loss: 0.10165 \n",
      "[170/200] train_loss: 0.07986 valid_loss: 0.09047 test_loss: 0.10217 \n",
      "[171/200] train_loss: 0.07945 valid_loss: 0.09053 test_loss: 0.10271 \n",
      "[172/200] train_loss: 0.07991 valid_loss: 0.08903 test_loss: 0.10180 \n",
      "Validation loss decreased (0.089567 --> 0.089033).  Saving model ...\n",
      "[173/200] train_loss: 0.07982 valid_loss: 0.08861 test_loss: 0.10070 \n",
      "Validation loss decreased (0.089033 --> 0.088612).  Saving model ...\n",
      "[174/200] train_loss: 0.07899 valid_loss: 0.09134 test_loss: 0.10068 \n",
      "[175/200] train_loss: 0.07964 valid_loss: 0.08726 test_loss: 0.09961 \n",
      "Validation loss decreased (0.088612 --> 0.087264).  Saving model ...\n",
      "[176/200] train_loss: 0.07933 valid_loss: 0.08838 test_loss: 0.09951 \n",
      "[177/200] train_loss: 0.07865 valid_loss: 0.08907 test_loss: 0.10071 \n",
      "[178/200] train_loss: 0.08090 valid_loss: 0.08922 test_loss: 0.10072 \n",
      "[179/200] train_loss: 0.07972 valid_loss: 0.08801 test_loss: 0.10005 \n",
      "[180/200] train_loss: 0.07622 valid_loss: 0.08989 test_loss: 0.10072 \n",
      "[181/200] train_loss: 0.07657 valid_loss: 0.08788 test_loss: 0.09960 \n",
      "[182/200] train_loss: 0.07885 valid_loss: 0.09130 test_loss: 0.09931 \n",
      "[183/200] train_loss: 0.07798 valid_loss: 0.09082 test_loss: 0.10052 \n",
      "[184/200] train_loss: 0.07619 valid_loss: 0.08886 test_loss: 0.10057 \n",
      "[185/200] train_loss: 0.07961 valid_loss: 0.08744 test_loss: 0.09909 \n",
      "[186/200] train_loss: 0.07624 valid_loss: 0.08821 test_loss: 0.09985 \n",
      "[187/200] train_loss: 0.07686 valid_loss: 0.08769 test_loss: 0.09940 \n",
      "[188/200] train_loss: 0.07644 valid_loss: 0.09216 test_loss: 0.10046 \n",
      "[189/200] train_loss: 0.07482 valid_loss: 0.08703 test_loss: 0.09854 \n",
      "Validation loss decreased (0.087264 --> 0.087030).  Saving model ...\n",
      "[190/200] train_loss: 0.07771 valid_loss: 0.09035 test_loss: 0.09947 \n",
      "[191/200] train_loss: 0.07676 valid_loss: 0.08729 test_loss: 0.09820 \n",
      "[192/200] train_loss: 0.07634 valid_loss: 0.08708 test_loss: 0.09810 \n",
      "[193/200] train_loss: 0.07625 valid_loss: 0.08913 test_loss: 0.10014 \n",
      "[194/200] train_loss: 0.07741 valid_loss: 0.08704 test_loss: 0.09828 \n",
      "[195/200] train_loss: 0.07390 valid_loss: 0.08727 test_loss: 0.09843 \n",
      "[196/200] train_loss: 0.07814 valid_loss: 0.08732 test_loss: 0.09974 \n",
      "[197/200] train_loss: 0.07543 valid_loss: 0.08802 test_loss: 0.09927 \n",
      "[198/200] train_loss: 0.07414 valid_loss: 0.08910 test_loss: 0.09994 \n",
      "[199/200] train_loss: 0.07605 valid_loss: 0.08707 test_loss: 0.10028 \n",
      "[200/200] train_loss: 0.07405 valid_loss: 0.08700 test_loss: 0.09880 \n",
      "Validation loss decreased (0.087030 --> 0.087000).  Saving model ...\n",
      "Model 13 trained for seen data.\n",
      "TRAINING MODEL for seen house 14\n",
      "[  1/200] train_loss: 0.61997 valid_loss: 0.51202 test_loss: 0.52066 \n",
      "Validation loss decreased (inf --> 0.512016).  Saving model ...\n",
      "[  2/200] train_loss: 0.42271 valid_loss: 0.37922 test_loss: 0.39665 \n",
      "Validation loss decreased (0.512016 --> 0.379219).  Saving model ...\n",
      "[  3/200] train_loss: 0.32393 valid_loss: 0.29977 test_loss: 0.32271 \n",
      "Validation loss decreased (0.379219 --> 0.299774).  Saving model ...\n",
      "[  4/200] train_loss: 0.26629 valid_loss: 0.25772 test_loss: 0.27780 \n",
      "Validation loss decreased (0.299774 --> 0.257716).  Saving model ...\n",
      "[  5/200] train_loss: 0.22806 valid_loss: 0.22842 test_loss: 0.24354 \n",
      "Validation loss decreased (0.257716 --> 0.228419).  Saving model ...\n",
      "[  6/200] train_loss: 0.20715 valid_loss: 0.20268 test_loss: 0.21858 \n",
      "Validation loss decreased (0.228419 --> 0.202677).  Saving model ...\n",
      "[  7/200] train_loss: 0.18910 valid_loss: 0.19139 test_loss: 0.20519 \n",
      "Validation loss decreased (0.202677 --> 0.191393).  Saving model ...\n",
      "[  8/200] train_loss: 0.17560 valid_loss: 0.18094 test_loss: 0.19268 \n",
      "Validation loss decreased (0.191393 --> 0.180944).  Saving model ...\n",
      "[  9/200] train_loss: 0.16436 valid_loss: 0.17285 test_loss: 0.18389 \n",
      "Validation loss decreased (0.180944 --> 0.172846).  Saving model ...\n",
      "[ 10/200] train_loss: 0.16099 valid_loss: 0.16489 test_loss: 0.17656 \n",
      "Validation loss decreased (0.172846 --> 0.164892).  Saving model ...\n",
      "[ 11/200] train_loss: 0.15591 valid_loss: 0.16128 test_loss: 0.17275 \n",
      "Validation loss decreased (0.164892 --> 0.161276).  Saving model ...\n",
      "[ 12/200] train_loss: 0.15100 valid_loss: 0.15910 test_loss: 0.16852 \n",
      "Validation loss decreased (0.161276 --> 0.159105).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14784 valid_loss: 0.15430 test_loss: 0.16480 \n",
      "Validation loss decreased (0.159105 --> 0.154300).  Saving model ...\n",
      "[ 14/200] train_loss: 0.14328 valid_loss: 0.15448 test_loss: 0.16244 \n",
      "[ 15/200] train_loss: 0.14230 valid_loss: 0.14729 test_loss: 0.15880 \n",
      "Validation loss decreased (0.154300 --> 0.147291).  Saving model ...\n",
      "[ 16/200] train_loss: 0.13962 valid_loss: 0.14847 test_loss: 0.15762 \n",
      "[ 17/200] train_loss: 0.13496 valid_loss: 0.14141 test_loss: 0.15515 \n",
      "Validation loss decreased (0.147291 --> 0.141407).  Saving model ...\n",
      "[ 18/200] train_loss: 0.13536 valid_loss: 0.14329 test_loss: 0.15530 \n",
      "[ 19/200] train_loss: 0.13277 valid_loss: 0.14198 test_loss: 0.15429 \n",
      "[ 20/200] train_loss: 0.13049 valid_loss: 0.13630 test_loss: 0.14999 \n",
      "Validation loss decreased (0.141407 --> 0.136296).  Saving model ...\n",
      "[ 21/200] train_loss: 0.12868 valid_loss: 0.13531 test_loss: 0.15005 \n",
      "Validation loss decreased (0.136296 --> 0.135312).  Saving model ...\n",
      "[ 22/200] train_loss: 0.12863 valid_loss: 0.14046 test_loss: 0.15275 \n",
      "[ 23/200] train_loss: 0.12778 valid_loss: 0.13265 test_loss: 0.14662 \n",
      "Validation loss decreased (0.135312 --> 0.132651).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12095 valid_loss: 0.12972 test_loss: 0.14539 \n",
      "Validation loss decreased (0.132651 --> 0.129720).  Saving model ...\n",
      "[ 25/200] train_loss: 0.12420 valid_loss: 0.13139 test_loss: 0.14417 \n",
      "[ 26/200] train_loss: 0.12262 valid_loss: 0.12793 test_loss: 0.14338 \n",
      "Validation loss decreased (0.129720 --> 0.127929).  Saving model ...\n",
      "[ 27/200] train_loss: 0.12410 valid_loss: 0.13029 test_loss: 0.14271 \n",
      "[ 28/200] train_loss: 0.12418 valid_loss: 0.12837 test_loss: 0.14413 \n",
      "[ 29/200] train_loss: 0.11931 valid_loss: 0.12643 test_loss: 0.14054 \n",
      "Validation loss decreased (0.127929 --> 0.126434).  Saving model ...\n",
      "[ 30/200] train_loss: 0.11622 valid_loss: 0.12469 test_loss: 0.14068 \n",
      "Validation loss decreased (0.126434 --> 0.124688).  Saving model ...\n",
      "[ 31/200] train_loss: 0.11773 valid_loss: 0.12415 test_loss: 0.13965 \n",
      "Validation loss decreased (0.124688 --> 0.124151).  Saving model ...\n",
      "[ 32/200] train_loss: 0.11731 valid_loss: 0.12445 test_loss: 0.13717 \n",
      "[ 33/200] train_loss: 0.11565 valid_loss: 0.11981 test_loss: 0.13730 \n",
      "Validation loss decreased (0.124151 --> 0.119810).  Saving model ...\n",
      "[ 34/200] train_loss: 0.11429 valid_loss: 0.12625 test_loss: 0.13838 \n",
      "[ 35/200] train_loss: 0.11618 valid_loss: 0.11938 test_loss: 0.13652 \n",
      "Validation loss decreased (0.119810 --> 0.119385).  Saving model ...\n",
      "[ 36/200] train_loss: 0.11549 valid_loss: 0.12267 test_loss: 0.13798 \n",
      "[ 37/200] train_loss: 0.11494 valid_loss: 0.11801 test_loss: 0.13519 \n",
      "Validation loss decreased (0.119385 --> 0.118014).  Saving model ...\n",
      "[ 38/200] train_loss: 0.11418 valid_loss: 0.12108 test_loss: 0.13617 \n",
      "[ 39/200] train_loss: 0.11250 valid_loss: 0.11925 test_loss: 0.13665 \n",
      "[ 40/200] train_loss: 0.10929 valid_loss: 0.11693 test_loss: 0.13417 \n",
      "Validation loss decreased (0.118014 --> 0.116931).  Saving model ...\n",
      "[ 41/200] train_loss: 0.10997 valid_loss: 0.11855 test_loss: 0.13399 \n",
      "[ 42/200] train_loss: 0.11320 valid_loss: 0.11795 test_loss: 0.13315 \n",
      "[ 43/200] train_loss: 0.11012 valid_loss: 0.11544 test_loss: 0.13431 \n",
      "Validation loss decreased (0.116931 --> 0.115437).  Saving model ...\n",
      "[ 44/200] train_loss: 0.10877 valid_loss: 0.11841 test_loss: 0.13437 \n",
      "[ 45/200] train_loss: 0.10483 valid_loss: 0.11808 test_loss: 0.13149 \n",
      "[ 46/200] train_loss: 0.10393 valid_loss: 0.11692 test_loss: 0.13340 \n",
      "[ 47/200] train_loss: 0.10842 valid_loss: 0.11660 test_loss: 0.13130 \n",
      "[ 48/200] train_loss: 0.10949 valid_loss: 0.11197 test_loss: 0.12894 \n",
      "Validation loss decreased (0.115437 --> 0.111968).  Saving model ...\n",
      "[ 49/200] train_loss: 0.10699 valid_loss: 0.11541 test_loss: 0.13079 \n",
      "[ 50/200] train_loss: 0.10428 valid_loss: 0.11598 test_loss: 0.13328 \n",
      "[ 51/200] train_loss: 0.10550 valid_loss: 0.11409 test_loss: 0.12779 \n",
      "[ 52/200] train_loss: 0.10249 valid_loss: 0.11466 test_loss: 0.12810 \n",
      "[ 53/200] train_loss: 0.10464 valid_loss: 0.11009 test_loss: 0.12576 \n",
      "Validation loss decreased (0.111968 --> 0.110088).  Saving model ...\n",
      "[ 54/200] train_loss: 0.10361 valid_loss: 0.10830 test_loss: 0.12491 \n",
      "Validation loss decreased (0.110088 --> 0.108299).  Saving model ...\n",
      "[ 55/200] train_loss: 0.10412 valid_loss: 0.10979 test_loss: 0.12720 \n",
      "[ 56/200] train_loss: 0.10176 valid_loss: 0.11265 test_loss: 0.12405 \n",
      "[ 57/200] train_loss: 0.10190 valid_loss: 0.11383 test_loss: 0.12423 \n",
      "[ 58/200] train_loss: 0.10154 valid_loss: 0.11226 test_loss: 0.12417 \n",
      "[ 59/200] train_loss: 0.09990 valid_loss: 0.10815 test_loss: 0.12311 \n",
      "Validation loss decreased (0.108299 --> 0.108148).  Saving model ...\n",
      "[ 60/200] train_loss: 0.09859 valid_loss: 0.11221 test_loss: 0.12670 \n",
      "[ 61/200] train_loss: 0.10416 valid_loss: 0.10917 test_loss: 0.12307 \n",
      "[ 62/200] train_loss: 0.09945 valid_loss: 0.10929 test_loss: 0.12424 \n",
      "[ 63/200] train_loss: 0.10090 valid_loss: 0.10952 test_loss: 0.12394 \n",
      "[ 64/200] train_loss: 0.09899 valid_loss: 0.10929 test_loss: 0.12099 \n",
      "[ 65/200] train_loss: 0.09890 valid_loss: 0.10758 test_loss: 0.12110 \n",
      "Validation loss decreased (0.108148 --> 0.107580).  Saving model ...\n",
      "[ 66/200] train_loss: 0.09627 valid_loss: 0.10493 test_loss: 0.12105 \n",
      "Validation loss decreased (0.107580 --> 0.104934).  Saving model ...\n",
      "[ 67/200] train_loss: 0.09895 valid_loss: 0.10525 test_loss: 0.12088 \n",
      "[ 68/200] train_loss: 0.09836 valid_loss: 0.10559 test_loss: 0.11925 \n",
      "[ 69/200] train_loss: 0.10054 valid_loss: 0.10768 test_loss: 0.11911 \n",
      "[ 70/200] train_loss: 0.09542 valid_loss: 0.10574 test_loss: 0.12146 \n",
      "[ 71/200] train_loss: 0.09984 valid_loss: 0.10554 test_loss: 0.12000 \n",
      "[ 72/200] train_loss: 0.09800 valid_loss: 0.10439 test_loss: 0.11876 \n",
      "Validation loss decreased (0.104934 --> 0.104394).  Saving model ...\n",
      "[ 73/200] train_loss: 0.09747 valid_loss: 0.10391 test_loss: 0.11884 \n",
      "Validation loss decreased (0.104394 --> 0.103913).  Saving model ...\n",
      "[ 74/200] train_loss: 0.09720 valid_loss: 0.10744 test_loss: 0.11736 \n",
      "[ 75/200] train_loss: 0.09555 valid_loss: 0.10355 test_loss: 0.11914 \n",
      "Validation loss decreased (0.103913 --> 0.103555).  Saving model ...\n",
      "[ 76/200] train_loss: 0.09624 valid_loss: 0.10328 test_loss: 0.11858 \n",
      "Validation loss decreased (0.103555 --> 0.103282).  Saving model ...\n",
      "[ 77/200] train_loss: 0.09493 valid_loss: 0.10271 test_loss: 0.11669 \n",
      "Validation loss decreased (0.103282 --> 0.102706).  Saving model ...\n",
      "[ 78/200] train_loss: 0.09364 valid_loss: 0.10432 test_loss: 0.11767 \n",
      "[ 79/200] train_loss: 0.09280 valid_loss: 0.10410 test_loss: 0.11690 \n",
      "[ 80/200] train_loss: 0.09855 valid_loss: 0.10370 test_loss: 0.11719 \n",
      "[ 81/200] train_loss: 0.09551 valid_loss: 0.10375 test_loss: 0.11681 \n",
      "[ 82/200] train_loss: 0.09678 valid_loss: 0.10593 test_loss: 0.11710 \n",
      "[ 83/200] train_loss: 0.09446 valid_loss: 0.10542 test_loss: 0.11724 \n",
      "[ 84/200] train_loss: 0.09521 valid_loss: 0.10246 test_loss: 0.11469 \n",
      "Validation loss decreased (0.102706 --> 0.102458).  Saving model ...\n",
      "[ 85/200] train_loss: 0.09125 valid_loss: 0.09971 test_loss: 0.11401 \n",
      "Validation loss decreased (0.102458 --> 0.099709).  Saving model ...\n",
      "[ 86/200] train_loss: 0.09295 valid_loss: 0.10918 test_loss: 0.11700 \n",
      "[ 87/200] train_loss: 0.09550 valid_loss: 0.10257 test_loss: 0.11457 \n",
      "[ 88/200] train_loss: 0.09367 valid_loss: 0.10387 test_loss: 0.11512 \n",
      "[ 89/200] train_loss: 0.09503 valid_loss: 0.10256 test_loss: 0.11323 \n",
      "[ 90/200] train_loss: 0.09529 valid_loss: 0.10138 test_loss: 0.11363 \n",
      "[ 91/200] train_loss: 0.08972 valid_loss: 0.09970 test_loss: 0.11371 \n",
      "Validation loss decreased (0.099709 --> 0.099701).  Saving model ...\n",
      "[ 92/200] train_loss: 0.09255 valid_loss: 0.10088 test_loss: 0.11327 \n",
      "[ 93/200] train_loss: 0.09033 valid_loss: 0.09903 test_loss: 0.11241 \n",
      "Validation loss decreased (0.099701 --> 0.099027).  Saving model ...\n",
      "[ 94/200] train_loss: 0.09026 valid_loss: 0.09925 test_loss: 0.11356 \n",
      "[ 95/200] train_loss: 0.09142 valid_loss: 0.10033 test_loss: 0.11378 \n",
      "[ 96/200] train_loss: 0.09202 valid_loss: 0.09736 test_loss: 0.11267 \n",
      "Validation loss decreased (0.099027 --> 0.097357).  Saving model ...\n",
      "[ 97/200] train_loss: 0.09051 valid_loss: 0.09990 test_loss: 0.11197 \n",
      "[ 98/200] train_loss: 0.09132 valid_loss: 0.10283 test_loss: 0.11317 \n",
      "[ 99/200] train_loss: 0.09003 valid_loss: 0.09990 test_loss: 0.11225 \n",
      "[100/200] train_loss: 0.09019 valid_loss: 0.09743 test_loss: 0.11230 \n",
      "[101/200] train_loss: 0.09008 valid_loss: 0.09817 test_loss: 0.11145 \n",
      "[102/200] train_loss: 0.08848 valid_loss: 0.09595 test_loss: 0.10946 \n",
      "Validation loss decreased (0.097357 --> 0.095952).  Saving model ...\n",
      "[103/200] train_loss: 0.08954 valid_loss: 0.09638 test_loss: 0.10993 \n",
      "[104/200] train_loss: 0.08814 valid_loss: 0.09719 test_loss: 0.11057 \n",
      "[105/200] train_loss: 0.08631 valid_loss: 0.09681 test_loss: 0.11285 \n",
      "[106/200] train_loss: 0.08919 valid_loss: 0.09675 test_loss: 0.11017 \n",
      "[107/200] train_loss: 0.08674 valid_loss: 0.09643 test_loss: 0.10954 \n",
      "[108/200] train_loss: 0.08804 valid_loss: 0.09556 test_loss: 0.10977 \n",
      "Validation loss decreased (0.095952 --> 0.095563).  Saving model ...\n",
      "[109/200] train_loss: 0.09092 valid_loss: 0.09647 test_loss: 0.11204 \n",
      "[110/200] train_loss: 0.08810 valid_loss: 0.09713 test_loss: 0.10892 \n",
      "[111/200] train_loss: 0.08807 valid_loss: 0.09744 test_loss: 0.10909 \n",
      "[112/200] train_loss: 0.08646 valid_loss: 0.09896 test_loss: 0.11298 \n",
      "[113/200] train_loss: 0.08476 valid_loss: 0.09619 test_loss: 0.10901 \n",
      "[114/200] train_loss: 0.08661 valid_loss: 0.09589 test_loss: 0.10864 \n",
      "[115/200] train_loss: 0.08646 valid_loss: 0.09753 test_loss: 0.11191 \n",
      "[116/200] train_loss: 0.08841 valid_loss: 0.09851 test_loss: 0.10942 \n",
      "[117/200] train_loss: 0.08856 valid_loss: 0.09725 test_loss: 0.11117 \n",
      "[118/200] train_loss: 0.08732 valid_loss: 0.09655 test_loss: 0.10903 \n",
      "[119/200] train_loss: 0.08665 valid_loss: 0.09450 test_loss: 0.10887 \n",
      "Validation loss decreased (0.095563 --> 0.094499).  Saving model ...\n",
      "[120/200] train_loss: 0.08586 valid_loss: 0.09532 test_loss: 0.10716 \n",
      "[121/200] train_loss: 0.08650 valid_loss: 0.09394 test_loss: 0.10691 \n",
      "Validation loss decreased (0.094499 --> 0.093941).  Saving model ...\n",
      "[122/200] train_loss: 0.08794 valid_loss: 0.09405 test_loss: 0.10804 \n",
      "[123/200] train_loss: 0.08527 valid_loss: 0.09674 test_loss: 0.10869 \n",
      "[124/200] train_loss: 0.08695 valid_loss: 0.09468 test_loss: 0.10833 \n",
      "[125/200] train_loss: 0.08690 valid_loss: 0.09548 test_loss: 0.10697 \n",
      "[126/200] train_loss: 0.08533 valid_loss: 0.09576 test_loss: 0.10702 \n",
      "[127/200] train_loss: 0.08302 valid_loss: 0.09663 test_loss: 0.10822 \n",
      "[128/200] train_loss: 0.08321 valid_loss: 0.09546 test_loss: 0.10673 \n",
      "[129/200] train_loss: 0.08532 valid_loss: 0.09322 test_loss: 0.10737 \n",
      "Validation loss decreased (0.093941 --> 0.093222).  Saving model ...\n",
      "[130/200] train_loss: 0.08364 valid_loss: 0.09613 test_loss: 0.10716 \n",
      "[131/200] train_loss: 0.08391 valid_loss: 0.09544 test_loss: 0.10865 \n",
      "[132/200] train_loss: 0.08370 valid_loss: 0.09247 test_loss: 0.10648 \n",
      "Validation loss decreased (0.093222 --> 0.092472).  Saving model ...\n",
      "[133/200] train_loss: 0.08393 valid_loss: 0.09267 test_loss: 0.10588 \n",
      "[134/200] train_loss: 0.08269 valid_loss: 0.09507 test_loss: 0.10625 \n",
      "[135/200] train_loss: 0.08365 valid_loss: 0.09496 test_loss: 0.10694 \n",
      "[136/200] train_loss: 0.08418 valid_loss: 0.09234 test_loss: 0.10571 \n",
      "Validation loss decreased (0.092472 --> 0.092343).  Saving model ...\n",
      "[137/200] train_loss: 0.08103 valid_loss: 0.09204 test_loss: 0.10609 \n",
      "Validation loss decreased (0.092343 --> 0.092042).  Saving model ...\n",
      "[138/200] train_loss: 0.08433 valid_loss: 0.09157 test_loss: 0.10590 \n",
      "Validation loss decreased (0.092042 --> 0.091572).  Saving model ...\n",
      "[139/200] train_loss: 0.08472 valid_loss: 0.09342 test_loss: 0.10617 \n",
      "[140/200] train_loss: 0.08253 valid_loss: 0.09405 test_loss: 0.10663 \n",
      "[141/200] train_loss: 0.08278 valid_loss: 0.09049 test_loss: 0.10431 \n",
      "Validation loss decreased (0.091572 --> 0.090493).  Saving model ...\n",
      "[142/200] train_loss: 0.08243 valid_loss: 0.09264 test_loss: 0.10445 \n",
      "[143/200] train_loss: 0.08276 valid_loss: 0.09142 test_loss: 0.10427 \n",
      "[144/200] train_loss: 0.08158 valid_loss: 0.09035 test_loss: 0.10397 \n",
      "Validation loss decreased (0.090493 --> 0.090354).  Saving model ...\n",
      "[145/200] train_loss: 0.08463 valid_loss: 0.09067 test_loss: 0.10413 \n",
      "[146/200] train_loss: 0.08324 valid_loss: 0.09122 test_loss: 0.10531 \n",
      "[147/200] train_loss: 0.08244 valid_loss: 0.09406 test_loss: 0.10598 \n",
      "[148/200] train_loss: 0.08226 valid_loss: 0.08932 test_loss: 0.10470 \n",
      "Validation loss decreased (0.090354 --> 0.089317).  Saving model ...\n",
      "[149/200] train_loss: 0.08265 valid_loss: 0.08927 test_loss: 0.10456 \n",
      "Validation loss decreased (0.089317 --> 0.089272).  Saving model ...\n",
      "[150/200] train_loss: 0.08311 valid_loss: 0.09037 test_loss: 0.10469 \n",
      "[151/200] train_loss: 0.08051 valid_loss: 0.09240 test_loss: 0.10574 \n",
      "[152/200] train_loss: 0.08278 valid_loss: 0.09360 test_loss: 0.10312 \n",
      "[153/200] train_loss: 0.08007 valid_loss: 0.09079 test_loss: 0.10458 \n",
      "[154/200] train_loss: 0.08127 valid_loss: 0.08943 test_loss: 0.10374 \n",
      "[155/200] train_loss: 0.08048 valid_loss: 0.08910 test_loss: 0.10442 \n",
      "Validation loss decreased (0.089272 --> 0.089098).  Saving model ...\n",
      "[156/200] train_loss: 0.08242 valid_loss: 0.08959 test_loss: 0.10464 \n",
      "[157/200] train_loss: 0.08227 valid_loss: 0.08882 test_loss: 0.10170 \n",
      "Validation loss decreased (0.089098 --> 0.088818).  Saving model ...\n",
      "[158/200] train_loss: 0.08241 valid_loss: 0.09122 test_loss: 0.10368 \n",
      "[159/200] train_loss: 0.07976 valid_loss: 0.08992 test_loss: 0.10381 \n",
      "[160/200] train_loss: 0.07923 valid_loss: 0.08793 test_loss: 0.10340 \n",
      "Validation loss decreased (0.088818 --> 0.087930).  Saving model ...\n",
      "[161/200] train_loss: 0.07919 valid_loss: 0.08762 test_loss: 0.10325 \n",
      "Validation loss decreased (0.087930 --> 0.087619).  Saving model ...\n",
      "[162/200] train_loss: 0.08057 valid_loss: 0.09067 test_loss: 0.10361 \n",
      "[163/200] train_loss: 0.07940 valid_loss: 0.09055 test_loss: 0.10334 \n",
      "[164/200] train_loss: 0.07830 valid_loss: 0.08955 test_loss: 0.10450 \n",
      "[165/200] train_loss: 0.07899 valid_loss: 0.09010 test_loss: 0.10444 \n",
      "[166/200] train_loss: 0.08230 valid_loss: 0.08898 test_loss: 0.10348 \n",
      "[167/200] train_loss: 0.07965 valid_loss: 0.08783 test_loss: 0.10218 \n",
      "[168/200] train_loss: 0.07999 valid_loss: 0.08708 test_loss: 0.10186 \n",
      "Validation loss decreased (0.087619 --> 0.087077).  Saving model ...\n",
      "[169/200] train_loss: 0.07782 valid_loss: 0.08879 test_loss: 0.10250 \n",
      "[170/200] train_loss: 0.07858 valid_loss: 0.08995 test_loss: 0.10159 \n",
      "[171/200] train_loss: 0.07963 valid_loss: 0.08830 test_loss: 0.10159 \n",
      "[172/200] train_loss: 0.07953 valid_loss: 0.09014 test_loss: 0.10113 \n",
      "[173/200] train_loss: 0.07884 valid_loss: 0.08639 test_loss: 0.10134 \n",
      "Validation loss decreased (0.087077 --> 0.086389).  Saving model ...\n",
      "[174/200] train_loss: 0.08048 valid_loss: 0.08735 test_loss: 0.10125 \n",
      "[175/200] train_loss: 0.07903 valid_loss: 0.08740 test_loss: 0.10153 \n",
      "[176/200] train_loss: 0.07982 valid_loss: 0.08741 test_loss: 0.10154 \n",
      "[177/200] train_loss: 0.07791 valid_loss: 0.08714 test_loss: 0.10211 \n",
      "[178/200] train_loss: 0.07771 valid_loss: 0.08933 test_loss: 0.10378 \n",
      "[179/200] train_loss: 0.07969 valid_loss: 0.08940 test_loss: 0.10311 \n",
      "[180/200] train_loss: 0.08027 valid_loss: 0.08778 test_loss: 0.10134 \n",
      "[181/200] train_loss: 0.07640 valid_loss: 0.09226 test_loss: 0.10194 \n",
      "[182/200] train_loss: 0.07768 valid_loss: 0.08845 test_loss: 0.10018 \n",
      "[183/200] train_loss: 0.07746 valid_loss: 0.09092 test_loss: 0.10150 \n",
      "[184/200] train_loss: 0.07850 valid_loss: 0.09107 test_loss: 0.10074 \n",
      "[185/200] train_loss: 0.07689 valid_loss: 0.08724 test_loss: 0.10226 \n",
      "[186/200] train_loss: 0.07958 valid_loss: 0.08637 test_loss: 0.10049 \n",
      "Validation loss decreased (0.086389 --> 0.086369).  Saving model ...\n",
      "[187/200] train_loss: 0.07746 valid_loss: 0.08751 test_loss: 0.09964 \n",
      "[188/200] train_loss: 0.07637 valid_loss: 0.08797 test_loss: 0.09989 \n",
      "[189/200] train_loss: 0.07644 valid_loss: 0.08689 test_loss: 0.10040 \n",
      "[190/200] train_loss: 0.07721 valid_loss: 0.08642 test_loss: 0.09996 \n",
      "[191/200] train_loss: 0.07876 valid_loss: 0.08559 test_loss: 0.09910 \n",
      "Validation loss decreased (0.086369 --> 0.085593).  Saving model ...\n",
      "[192/200] train_loss: 0.07507 valid_loss: 0.08641 test_loss: 0.10061 \n",
      "[193/200] train_loss: 0.07939 valid_loss: 0.08756 test_loss: 0.09903 \n",
      "[194/200] train_loss: 0.07827 valid_loss: 0.08915 test_loss: 0.10094 \n",
      "[195/200] train_loss: 0.07639 valid_loss: 0.08774 test_loss: 0.10066 \n",
      "[196/200] train_loss: 0.07488 valid_loss: 0.08726 test_loss: 0.10048 \n",
      "[197/200] train_loss: 0.07791 valid_loss: 0.08665 test_loss: 0.10026 \n",
      "[198/200] train_loss: 0.07594 valid_loss: 0.08735 test_loss: 0.09930 \n",
      "[199/200] train_loss: 0.07556 valid_loss: 0.08643 test_loss: 0.09966 \n",
      "[200/200] train_loss: 0.07498 valid_loss: 0.08733 test_loss: 0.10151 \n",
      "Model 14 trained for seen data.\n",
      "TRAINING MODEL for seen house 15\n",
      "[  1/200] train_loss: 0.56026 valid_loss: 0.46448 test_loss: 0.47507 \n",
      "Validation loss decreased (inf --> 0.464483).  Saving model ...\n",
      "[  2/200] train_loss: 0.37928 valid_loss: 0.33983 test_loss: 0.35432 \n",
      "Validation loss decreased (0.464483 --> 0.339831).  Saving model ...\n",
      "[  3/200] train_loss: 0.29309 valid_loss: 0.28675 test_loss: 0.30529 \n",
      "Validation loss decreased (0.339831 --> 0.286745).  Saving model ...\n",
      "[  4/200] train_loss: 0.24767 valid_loss: 0.24523 test_loss: 0.26559 \n",
      "Validation loss decreased (0.286745 --> 0.245230).  Saving model ...\n",
      "[  5/200] train_loss: 0.22032 valid_loss: 0.21458 test_loss: 0.23096 \n",
      "Validation loss decreased (0.245230 --> 0.214576).  Saving model ...\n",
      "[  6/200] train_loss: 0.19897 valid_loss: 0.20081 test_loss: 0.21554 \n",
      "Validation loss decreased (0.214576 --> 0.200806).  Saving model ...\n",
      "[  7/200] train_loss: 0.18165 valid_loss: 0.18642 test_loss: 0.20206 \n",
      "Validation loss decreased (0.200806 --> 0.186415).  Saving model ...\n",
      "[  8/200] train_loss: 0.17156 valid_loss: 0.17650 test_loss: 0.19204 \n",
      "Validation loss decreased (0.186415 --> 0.176505).  Saving model ...\n",
      "[  9/200] train_loss: 0.16351 valid_loss: 0.17034 test_loss: 0.18433 \n",
      "Validation loss decreased (0.176505 --> 0.170338).  Saving model ...\n",
      "[ 10/200] train_loss: 0.15896 valid_loss: 0.16432 test_loss: 0.17731 \n",
      "Validation loss decreased (0.170338 --> 0.164323).  Saving model ...\n",
      "[ 11/200] train_loss: 0.15548 valid_loss: 0.15904 test_loss: 0.17152 \n",
      "Validation loss decreased (0.164323 --> 0.159042).  Saving model ...\n",
      "[ 12/200] train_loss: 0.15027 valid_loss: 0.15927 test_loss: 0.17060 \n",
      "[ 13/200] train_loss: 0.14647 valid_loss: 0.15222 test_loss: 0.16501 \n",
      "Validation loss decreased (0.159042 --> 0.152225).  Saving model ...\n",
      "[ 14/200] train_loss: 0.14288 valid_loss: 0.15195 test_loss: 0.16473 \n",
      "Validation loss decreased (0.152225 --> 0.151953).  Saving model ...\n",
      "[ 15/200] train_loss: 0.13903 valid_loss: 0.14711 test_loss: 0.15990 \n",
      "Validation loss decreased (0.151953 --> 0.147108).  Saving model ...\n",
      "[ 16/200] train_loss: 0.13563 valid_loss: 0.14649 test_loss: 0.15707 \n",
      "Validation loss decreased (0.147108 --> 0.146492).  Saving model ...\n",
      "[ 17/200] train_loss: 0.13412 valid_loss: 0.15124 test_loss: 0.15911 \n",
      "[ 18/200] train_loss: 0.13308 valid_loss: 0.14242 test_loss: 0.15539 \n",
      "Validation loss decreased (0.146492 --> 0.142419).  Saving model ...\n",
      "[ 19/200] train_loss: 0.13296 valid_loss: 0.14143 test_loss: 0.15140 \n",
      "Validation loss decreased (0.142419 --> 0.141434).  Saving model ...\n",
      "[ 20/200] train_loss: 0.12719 valid_loss: 0.14223 test_loss: 0.15114 \n",
      "[ 21/200] train_loss: 0.12701 valid_loss: 0.13683 test_loss: 0.14738 \n",
      "Validation loss decreased (0.141434 --> 0.136825).  Saving model ...\n",
      "[ 22/200] train_loss: 0.12709 valid_loss: 0.13575 test_loss: 0.15014 \n",
      "Validation loss decreased (0.136825 --> 0.135747).  Saving model ...\n",
      "[ 23/200] train_loss: 0.12493 valid_loss: 0.13508 test_loss: 0.14842 \n",
      "Validation loss decreased (0.135747 --> 0.135082).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12040 valid_loss: 0.13153 test_loss: 0.14585 \n",
      "Validation loss decreased (0.135082 --> 0.131528).  Saving model ...\n",
      "[ 25/200] train_loss: 0.12160 valid_loss: 0.13377 test_loss: 0.14804 \n",
      "[ 26/200] train_loss: 0.12173 valid_loss: 0.12870 test_loss: 0.14481 \n",
      "Validation loss decreased (0.131528 --> 0.128700).  Saving model ...\n",
      "[ 27/200] train_loss: 0.12481 valid_loss: 0.12747 test_loss: 0.14121 \n",
      "Validation loss decreased (0.128700 --> 0.127467).  Saving model ...\n",
      "[ 28/200] train_loss: 0.11705 valid_loss: 0.12563 test_loss: 0.13990 \n",
      "Validation loss decreased (0.127467 --> 0.125634).  Saving model ...\n",
      "[ 29/200] train_loss: 0.11717 valid_loss: 0.12416 test_loss: 0.13998 \n",
      "Validation loss decreased (0.125634 --> 0.124158).  Saving model ...\n",
      "[ 30/200] train_loss: 0.11775 valid_loss: 0.12617 test_loss: 0.13842 \n",
      "[ 31/200] train_loss: 0.12012 valid_loss: 0.12756 test_loss: 0.13934 \n",
      "[ 32/200] train_loss: 0.11433 valid_loss: 0.12707 test_loss: 0.13942 \n",
      "[ 33/200] train_loss: 0.11420 valid_loss: 0.12228 test_loss: 0.13701 \n",
      "Validation loss decreased (0.124158 --> 0.122277).  Saving model ...\n",
      "[ 34/200] train_loss: 0.11149 valid_loss: 0.11937 test_loss: 0.13719 \n",
      "Validation loss decreased (0.122277 --> 0.119374).  Saving model ...\n",
      "[ 35/200] train_loss: 0.11228 valid_loss: 0.12258 test_loss: 0.13590 \n",
      "[ 36/200] train_loss: 0.11213 valid_loss: 0.12730 test_loss: 0.13564 \n",
      "[ 37/200] train_loss: 0.11142 valid_loss: 0.12026 test_loss: 0.13558 \n",
      "[ 38/200] train_loss: 0.11197 valid_loss: 0.11950 test_loss: 0.13401 \n",
      "[ 39/200] train_loss: 0.11190 valid_loss: 0.11526 test_loss: 0.13327 \n",
      "Validation loss decreased (0.119374 --> 0.115264).  Saving model ...\n",
      "[ 40/200] train_loss: 0.11009 valid_loss: 0.12067 test_loss: 0.13626 \n",
      "[ 41/200] train_loss: 0.10941 valid_loss: 0.12071 test_loss: 0.13320 \n",
      "[ 42/200] train_loss: 0.10701 valid_loss: 0.11556 test_loss: 0.13085 \n",
      "[ 43/200] train_loss: 0.11033 valid_loss: 0.11660 test_loss: 0.13126 \n",
      "[ 44/200] train_loss: 0.10668 valid_loss: 0.11639 test_loss: 0.13016 \n",
      "[ 45/200] train_loss: 0.10430 valid_loss: 0.11749 test_loss: 0.13125 \n",
      "[ 46/200] train_loss: 0.10895 valid_loss: 0.11327 test_loss: 0.13132 \n",
      "Validation loss decreased (0.115264 --> 0.113275).  Saving model ...\n",
      "[ 47/200] train_loss: 0.10646 valid_loss: 0.11502 test_loss: 0.13023 \n",
      "[ 48/200] train_loss: 0.10422 valid_loss: 0.11273 test_loss: 0.12874 \n",
      "Validation loss decreased (0.113275 --> 0.112732).  Saving model ...\n",
      "[ 49/200] train_loss: 0.10448 valid_loss: 0.11289 test_loss: 0.12882 \n",
      "[ 50/200] train_loss: 0.10788 valid_loss: 0.11240 test_loss: 0.12895 \n",
      "Validation loss decreased (0.112732 --> 0.112402).  Saving model ...\n",
      "[ 51/200] train_loss: 0.10462 valid_loss: 0.11223 test_loss: 0.12758 \n",
      "Validation loss decreased (0.112402 --> 0.112225).  Saving model ...\n",
      "[ 52/200] train_loss: 0.10370 valid_loss: 0.11141 test_loss: 0.12849 \n",
      "Validation loss decreased (0.112225 --> 0.111411).  Saving model ...\n",
      "[ 53/200] train_loss: 0.10194 valid_loss: 0.10918 test_loss: 0.12639 \n",
      "Validation loss decreased (0.111411 --> 0.109177).  Saving model ...\n",
      "[ 54/200] train_loss: 0.10149 valid_loss: 0.11296 test_loss: 0.12992 \n",
      "[ 55/200] train_loss: 0.10296 valid_loss: 0.11025 test_loss: 0.12517 \n",
      "[ 56/200] train_loss: 0.10259 valid_loss: 0.10926 test_loss: 0.12650 \n",
      "[ 57/200] train_loss: 0.10354 valid_loss: 0.10957 test_loss: 0.12557 \n",
      "[ 58/200] train_loss: 0.10282 valid_loss: 0.11277 test_loss: 0.12956 \n",
      "[ 59/200] train_loss: 0.10083 valid_loss: 0.10710 test_loss: 0.12268 \n",
      "Validation loss decreased (0.109177 --> 0.107098).  Saving model ...\n",
      "[ 60/200] train_loss: 0.09993 valid_loss: 0.10942 test_loss: 0.12279 \n",
      "[ 61/200] train_loss: 0.10157 valid_loss: 0.11170 test_loss: 0.12759 \n",
      "[ 62/200] train_loss: 0.09936 valid_loss: 0.10724 test_loss: 0.12343 \n",
      "[ 63/200] train_loss: 0.10098 valid_loss: 0.10352 test_loss: 0.12068 \n",
      "Validation loss decreased (0.107098 --> 0.103521).  Saving model ...\n",
      "[ 64/200] train_loss: 0.09751 valid_loss: 0.10690 test_loss: 0.12229 \n",
      "[ 65/200] train_loss: 0.09531 valid_loss: 0.10572 test_loss: 0.12096 \n",
      "[ 66/200] train_loss: 0.09683 valid_loss: 0.10533 test_loss: 0.12049 \n",
      "[ 67/200] train_loss: 0.09745 valid_loss: 0.10934 test_loss: 0.12215 \n",
      "[ 68/200] train_loss: 0.09721 valid_loss: 0.10599 test_loss: 0.11893 \n",
      "[ 69/200] train_loss: 0.09683 valid_loss: 0.10282 test_loss: 0.11907 \n",
      "Validation loss decreased (0.103521 --> 0.102822).  Saving model ...\n",
      "[ 70/200] train_loss: 0.09553 valid_loss: 0.10387 test_loss: 0.12090 \n",
      "[ 71/200] train_loss: 0.09502 valid_loss: 0.10806 test_loss: 0.12080 \n",
      "[ 72/200] train_loss: 0.09530 valid_loss: 0.10849 test_loss: 0.11853 \n",
      "[ 73/200] train_loss: 0.09851 valid_loss: 0.10201 test_loss: 0.11747 \n",
      "Validation loss decreased (0.102822 --> 0.102012).  Saving model ...\n",
      "[ 74/200] train_loss: 0.10023 valid_loss: 0.10202 test_loss: 0.11838 \n",
      "[ 75/200] train_loss: 0.09780 valid_loss: 0.10459 test_loss: 0.12084 \n",
      "[ 76/200] train_loss: 0.09425 valid_loss: 0.10253 test_loss: 0.11723 \n",
      "[ 77/200] train_loss: 0.09470 valid_loss: 0.10313 test_loss: 0.11766 \n",
      "[ 78/200] train_loss: 0.09529 valid_loss: 0.10294 test_loss: 0.11871 \n",
      "[ 79/200] train_loss: 0.09299 valid_loss: 0.10180 test_loss: 0.11691 \n",
      "Validation loss decreased (0.102012 --> 0.101803).  Saving model ...\n",
      "[ 80/200] train_loss: 0.09559 valid_loss: 0.10133 test_loss: 0.11618 \n",
      "Validation loss decreased (0.101803 --> 0.101327).  Saving model ...\n",
      "[ 81/200] train_loss: 0.09687 valid_loss: 0.10495 test_loss: 0.11920 \n",
      "[ 82/200] train_loss: 0.09387 valid_loss: 0.10523 test_loss: 0.11714 \n",
      "[ 83/200] train_loss: 0.09582 valid_loss: 0.10285 test_loss: 0.11834 \n",
      "[ 84/200] train_loss: 0.09222 valid_loss: 0.10321 test_loss: 0.11653 \n",
      "[ 85/200] train_loss: 0.09320 valid_loss: 0.10524 test_loss: 0.11624 \n",
      "[ 86/200] train_loss: 0.09330 valid_loss: 0.10020 test_loss: 0.11398 \n",
      "Validation loss decreased (0.101327 --> 0.100204).  Saving model ...\n",
      "[ 87/200] train_loss: 0.09329 valid_loss: 0.09844 test_loss: 0.11403 \n",
      "Validation loss decreased (0.100204 --> 0.098442).  Saving model ...\n",
      "[ 88/200] train_loss: 0.09159 valid_loss: 0.10099 test_loss: 0.11545 \n",
      "[ 89/200] train_loss: 0.09040 valid_loss: 0.10122 test_loss: 0.11482 \n",
      "[ 90/200] train_loss: 0.09177 valid_loss: 0.09882 test_loss: 0.11395 \n",
      "[ 91/200] train_loss: 0.09103 valid_loss: 0.10001 test_loss: 0.11268 \n",
      "[ 92/200] train_loss: 0.09204 valid_loss: 0.10035 test_loss: 0.11341 \n",
      "[ 93/200] train_loss: 0.08814 valid_loss: 0.10262 test_loss: 0.11658 \n",
      "[ 94/200] train_loss: 0.09016 valid_loss: 0.10288 test_loss: 0.11393 \n",
      "[ 95/200] train_loss: 0.08990 valid_loss: 0.10217 test_loss: 0.11330 \n",
      "[ 96/200] train_loss: 0.08940 valid_loss: 0.09849 test_loss: 0.11425 \n",
      "[ 97/200] train_loss: 0.09015 valid_loss: 0.09699 test_loss: 0.11252 \n",
      "Validation loss decreased (0.098442 --> 0.096991).  Saving model ...\n",
      "[ 98/200] train_loss: 0.08779 valid_loss: 0.10051 test_loss: 0.11332 \n",
      "[ 99/200] train_loss: 0.08864 valid_loss: 0.09858 test_loss: 0.11263 \n",
      "[100/200] train_loss: 0.08935 valid_loss: 0.09790 test_loss: 0.11149 \n",
      "[101/200] train_loss: 0.08892 valid_loss: 0.10014 test_loss: 0.11378 \n",
      "[102/200] train_loss: 0.08880 valid_loss: 0.09579 test_loss: 0.11096 \n",
      "Validation loss decreased (0.096991 --> 0.095790).  Saving model ...\n",
      "[103/200] train_loss: 0.08785 valid_loss: 0.09969 test_loss: 0.11301 \n",
      "[104/200] train_loss: 0.08903 valid_loss: 0.09727 test_loss: 0.11190 \n",
      "[105/200] train_loss: 0.09003 valid_loss: 0.09474 test_loss: 0.10823 \n",
      "Validation loss decreased (0.095790 --> 0.094739).  Saving model ...\n",
      "[106/200] train_loss: 0.08774 valid_loss: 0.09950 test_loss: 0.11001 \n",
      "[107/200] train_loss: 0.09040 valid_loss: 0.09739 test_loss: 0.10912 \n",
      "[108/200] train_loss: 0.08729 valid_loss: 0.09955 test_loss: 0.10913 \n",
      "[109/200] train_loss: 0.08821 valid_loss: 0.09553 test_loss: 0.10860 \n",
      "[110/200] train_loss: 0.08830 valid_loss: 0.09464 test_loss: 0.10941 \n",
      "Validation loss decreased (0.094739 --> 0.094636).  Saving model ...\n",
      "[111/200] train_loss: 0.08913 valid_loss: 0.09446 test_loss: 0.10830 \n",
      "Validation loss decreased (0.094636 --> 0.094460).  Saving model ...\n",
      "[112/200] train_loss: 0.08689 valid_loss: 0.09464 test_loss: 0.10796 \n",
      "[113/200] train_loss: 0.08750 valid_loss: 0.09694 test_loss: 0.11102 \n",
      "[114/200] train_loss: 0.08508 valid_loss: 0.09570 test_loss: 0.10871 \n",
      "[115/200] train_loss: 0.08599 valid_loss: 0.09471 test_loss: 0.10933 \n",
      "[116/200] train_loss: 0.08567 valid_loss: 0.09465 test_loss: 0.10861 \n",
      "[117/200] train_loss: 0.08589 valid_loss: 0.09358 test_loss: 0.10753 \n",
      "Validation loss decreased (0.094460 --> 0.093578).  Saving model ...\n",
      "[118/200] train_loss: 0.08466 valid_loss: 0.09595 test_loss: 0.10821 \n",
      "[119/200] train_loss: 0.08731 valid_loss: 0.09768 test_loss: 0.10800 \n",
      "[120/200] train_loss: 0.08394 valid_loss: 0.09296 test_loss: 0.10717 \n",
      "Validation loss decreased (0.093578 --> 0.092964).  Saving model ...\n",
      "[121/200] train_loss: 0.08519 valid_loss: 0.09270 test_loss: 0.10747 \n",
      "Validation loss decreased (0.092964 --> 0.092705).  Saving model ...\n",
      "[122/200] train_loss: 0.08750 valid_loss: 0.09479 test_loss: 0.10731 \n",
      "[123/200] train_loss: 0.08243 valid_loss: 0.09769 test_loss: 0.10840 \n",
      "[124/200] train_loss: 0.08388 valid_loss: 0.09474 test_loss: 0.10772 \n",
      "[125/200] train_loss: 0.08330 valid_loss: 0.09575 test_loss: 0.10723 \n",
      "[126/200] train_loss: 0.08667 valid_loss: 0.09429 test_loss: 0.10814 \n",
      "[127/200] train_loss: 0.08444 valid_loss: 0.09471 test_loss: 0.10770 \n",
      "[128/200] train_loss: 0.08517 valid_loss: 0.09476 test_loss: 0.10542 \n",
      "[129/200] train_loss: 0.08310 valid_loss: 0.09394 test_loss: 0.10605 \n",
      "[130/200] train_loss: 0.08050 valid_loss: 0.09101 test_loss: 0.10614 \n",
      "Validation loss decreased (0.092705 --> 0.091012).  Saving model ...\n",
      "[131/200] train_loss: 0.08270 valid_loss: 0.09652 test_loss: 0.10641 \n",
      "[132/200] train_loss: 0.08544 valid_loss: 0.09486 test_loss: 0.10616 \n",
      "[133/200] train_loss: 0.08375 valid_loss: 0.09629 test_loss: 0.10628 \n",
      "[134/200] train_loss: 0.08391 valid_loss: 0.09674 test_loss: 0.10498 \n",
      "[135/200] train_loss: 0.08338 valid_loss: 0.09555 test_loss: 0.10424 \n",
      "[136/200] train_loss: 0.08226 valid_loss: 0.09642 test_loss: 0.10537 \n",
      "[137/200] train_loss: 0.08050 valid_loss: 0.09657 test_loss: 0.10576 \n",
      "[138/200] train_loss: 0.08322 valid_loss: 0.09307 test_loss: 0.10497 \n",
      "[139/200] train_loss: 0.08095 valid_loss: 0.09510 test_loss: 0.10533 \n",
      "[140/200] train_loss: 0.08213 valid_loss: 0.09132 test_loss: 0.10431 \n",
      "[141/200] train_loss: 0.08259 valid_loss: 0.09240 test_loss: 0.10599 \n",
      "[142/200] train_loss: 0.08291 valid_loss: 0.09046 test_loss: 0.10487 \n",
      "Validation loss decreased (0.091012 --> 0.090464).  Saving model ...\n",
      "[143/200] train_loss: 0.08167 valid_loss: 0.09395 test_loss: 0.10503 \n",
      "[144/200] train_loss: 0.08211 valid_loss: 0.09313 test_loss: 0.10547 \n",
      "[145/200] train_loss: 0.08345 valid_loss: 0.09180 test_loss: 0.10342 \n",
      "[146/200] train_loss: 0.08186 valid_loss: 0.08888 test_loss: 0.10257 \n",
      "Validation loss decreased (0.090464 --> 0.088882).  Saving model ...\n",
      "[147/200] train_loss: 0.07851 valid_loss: 0.09127 test_loss: 0.10459 \n",
      "[148/200] train_loss: 0.08196 valid_loss: 0.09387 test_loss: 0.10611 \n",
      "[149/200] train_loss: 0.08007 valid_loss: 0.08997 test_loss: 0.10241 \n",
      "[150/200] train_loss: 0.08042 valid_loss: 0.08921 test_loss: 0.10137 \n",
      "[151/200] train_loss: 0.08304 valid_loss: 0.09352 test_loss: 0.10348 \n",
      "[152/200] train_loss: 0.08078 valid_loss: 0.09116 test_loss: 0.10339 \n",
      "[153/200] train_loss: 0.07939 valid_loss: 0.08821 test_loss: 0.10115 \n",
      "Validation loss decreased (0.088882 --> 0.088206).  Saving model ...\n",
      "[154/200] train_loss: 0.08003 valid_loss: 0.08922 test_loss: 0.10127 \n",
      "[155/200] train_loss: 0.07918 valid_loss: 0.09052 test_loss: 0.10185 \n",
      "[156/200] train_loss: 0.07898 valid_loss: 0.09118 test_loss: 0.10217 \n",
      "[157/200] train_loss: 0.08132 valid_loss: 0.08968 test_loss: 0.10202 \n",
      "[158/200] train_loss: 0.07921 valid_loss: 0.09085 test_loss: 0.10157 \n",
      "[159/200] train_loss: 0.07905 valid_loss: 0.08923 test_loss: 0.10206 \n",
      "[160/200] train_loss: 0.07858 valid_loss: 0.08962 test_loss: 0.10103 \n",
      "[161/200] train_loss: 0.07832 valid_loss: 0.09141 test_loss: 0.10113 \n",
      "[162/200] train_loss: 0.08155 valid_loss: 0.08880 test_loss: 0.10000 \n",
      "[163/200] train_loss: 0.08031 valid_loss: 0.08902 test_loss: 0.10330 \n",
      "[164/200] train_loss: 0.07795 valid_loss: 0.08980 test_loss: 0.10208 \n",
      "[165/200] train_loss: 0.07932 valid_loss: 0.08591 test_loss: 0.10071 \n",
      "Validation loss decreased (0.088206 --> 0.085910).  Saving model ...\n",
      "[166/200] train_loss: 0.07941 valid_loss: 0.09146 test_loss: 0.10040 \n",
      "[167/200] train_loss: 0.07795 valid_loss: 0.08674 test_loss: 0.10027 \n",
      "[168/200] train_loss: 0.08067 valid_loss: 0.08838 test_loss: 0.10009 \n",
      "[169/200] train_loss: 0.07751 valid_loss: 0.08813 test_loss: 0.10078 \n",
      "[170/200] train_loss: 0.07828 valid_loss: 0.08739 test_loss: 0.10000 \n",
      "[171/200] train_loss: 0.07632 valid_loss: 0.08909 test_loss: 0.10196 \n",
      "[172/200] train_loss: 0.07838 valid_loss: 0.09182 test_loss: 0.10309 \n",
      "[173/200] train_loss: 0.07982 valid_loss: 0.08755 test_loss: 0.10058 \n",
      "[174/200] train_loss: 0.08034 valid_loss: 0.08639 test_loss: 0.09979 \n",
      "[175/200] train_loss: 0.07804 valid_loss: 0.08727 test_loss: 0.09955 \n",
      "[176/200] train_loss: 0.07634 valid_loss: 0.08775 test_loss: 0.09940 \n",
      "[177/200] train_loss: 0.07883 valid_loss: 0.08894 test_loss: 0.10152 \n",
      "[178/200] train_loss: 0.07999 valid_loss: 0.08988 test_loss: 0.10021 \n",
      "[179/200] train_loss: 0.07623 valid_loss: 0.08797 test_loss: 0.09998 \n",
      "[180/200] train_loss: 0.07844 valid_loss: 0.09061 test_loss: 0.10084 \n",
      "[181/200] train_loss: 0.08090 valid_loss: 0.08818 test_loss: 0.09929 \n",
      "[182/200] train_loss: 0.07668 valid_loss: 0.08988 test_loss: 0.10069 \n",
      "[183/200] train_loss: 0.08020 valid_loss: 0.08701 test_loss: 0.09919 \n",
      "[184/200] train_loss: 0.07771 valid_loss: 0.08770 test_loss: 0.10077 \n",
      "[185/200] train_loss: 0.07651 valid_loss: 0.08939 test_loss: 0.09930 \n",
      "[186/200] train_loss: 0.07681 valid_loss: 0.08740 test_loss: 0.09862 \n",
      "[187/200] train_loss: 0.07727 valid_loss: 0.08806 test_loss: 0.10034 \n",
      "[188/200] train_loss: 0.07583 valid_loss: 0.08761 test_loss: 0.09836 \n",
      "[189/200] train_loss: 0.07893 valid_loss: 0.08814 test_loss: 0.10029 \n",
      "[190/200] train_loss: 0.07703 valid_loss: 0.08883 test_loss: 0.10020 \n",
      "[191/200] train_loss: 0.07567 valid_loss: 0.08783 test_loss: 0.09978 \n",
      "[192/200] train_loss: 0.07807 valid_loss: 0.08542 test_loss: 0.09726 \n",
      "Validation loss decreased (0.085910 --> 0.085424).  Saving model ...\n",
      "[193/200] train_loss: 0.07636 valid_loss: 0.08588 test_loss: 0.09837 \n",
      "[194/200] train_loss: 0.07649 valid_loss: 0.08575 test_loss: 0.09806 \n",
      "[195/200] train_loss: 0.07363 valid_loss: 0.08696 test_loss: 0.09865 \n",
      "[196/200] train_loss: 0.07615 valid_loss: 0.08647 test_loss: 0.09868 \n",
      "[197/200] train_loss: 0.07510 valid_loss: 0.08690 test_loss: 0.09882 \n",
      "[198/200] train_loss: 0.07534 valid_loss: 0.08672 test_loss: 0.09890 \n",
      "[199/200] train_loss: 0.07583 valid_loss: 0.08626 test_loss: 0.09811 \n",
      "[200/200] train_loss: 0.07334 valid_loss: 0.08683 test_loss: 0.09919 \n",
      "Model 15 trained for seen data.\n",
      "TRAINING MODEL for seen house 16\n",
      "[  1/200] train_loss: 0.61578 valid_loss: 0.49686 test_loss: 0.50280 \n",
      "Validation loss decreased (inf --> 0.496865).  Saving model ...\n",
      "[  2/200] train_loss: 0.43111 valid_loss: 0.37198 test_loss: 0.38439 \n",
      "Validation loss decreased (0.496865 --> 0.371980).  Saving model ...\n",
      "[  3/200] train_loss: 0.32015 valid_loss: 0.29315 test_loss: 0.31341 \n",
      "Validation loss decreased (0.371980 --> 0.293151).  Saving model ...\n",
      "[  4/200] train_loss: 0.25715 valid_loss: 0.25325 test_loss: 0.27391 \n",
      "Validation loss decreased (0.293151 --> 0.253246).  Saving model ...\n",
      "[  5/200] train_loss: 0.22323 valid_loss: 0.21855 test_loss: 0.23534 \n",
      "Validation loss decreased (0.253246 --> 0.218546).  Saving model ...\n",
      "[  6/200] train_loss: 0.19596 valid_loss: 0.19982 test_loss: 0.21497 \n",
      "Validation loss decreased (0.218546 --> 0.199816).  Saving model ...\n",
      "[  7/200] train_loss: 0.18273 valid_loss: 0.18404 test_loss: 0.19845 \n",
      "Validation loss decreased (0.199816 --> 0.184043).  Saving model ...\n",
      "[  8/200] train_loss: 0.16967 valid_loss: 0.17405 test_loss: 0.18864 \n",
      "Validation loss decreased (0.184043 --> 0.174047).  Saving model ...\n",
      "[  9/200] train_loss: 0.15959 valid_loss: 0.16736 test_loss: 0.18007 \n",
      "Validation loss decreased (0.174047 --> 0.167357).  Saving model ...\n",
      "[ 10/200] train_loss: 0.15975 valid_loss: 0.16564 test_loss: 0.17621 \n",
      "Validation loss decreased (0.167357 --> 0.165641).  Saving model ...\n",
      "[ 11/200] train_loss: 0.15161 valid_loss: 0.16032 test_loss: 0.17083 \n",
      "Validation loss decreased (0.165641 --> 0.160324).  Saving model ...\n",
      "[ 12/200] train_loss: 0.15055 valid_loss: 0.15643 test_loss: 0.16774 \n",
      "Validation loss decreased (0.160324 --> 0.156431).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14493 valid_loss: 0.15433 test_loss: 0.16653 \n",
      "Validation loss decreased (0.156431 --> 0.154333).  Saving model ...\n",
      "[ 14/200] train_loss: 0.14464 valid_loss: 0.15074 test_loss: 0.16068 \n",
      "Validation loss decreased (0.154333 --> 0.150744).  Saving model ...\n",
      "[ 15/200] train_loss: 0.13951 valid_loss: 0.14771 test_loss: 0.15936 \n",
      "Validation loss decreased (0.150744 --> 0.147712).  Saving model ...\n",
      "[ 16/200] train_loss: 0.13749 valid_loss: 0.14580 test_loss: 0.15768 \n",
      "Validation loss decreased (0.147712 --> 0.145801).  Saving model ...\n",
      "[ 17/200] train_loss: 0.13461 valid_loss: 0.14005 test_loss: 0.15296 \n",
      "Validation loss decreased (0.145801 --> 0.140055).  Saving model ...\n",
      "[ 18/200] train_loss: 0.13359 valid_loss: 0.14751 test_loss: 0.15511 \n",
      "[ 19/200] train_loss: 0.13183 valid_loss: 0.13934 test_loss: 0.15171 \n",
      "Validation loss decreased (0.140055 --> 0.139342).  Saving model ...\n",
      "[ 20/200] train_loss: 0.12868 valid_loss: 0.13711 test_loss: 0.14962 \n",
      "Validation loss decreased (0.139342 --> 0.137109).  Saving model ...\n",
      "[ 21/200] train_loss: 0.12653 valid_loss: 0.13552 test_loss: 0.14797 \n",
      "Validation loss decreased (0.137109 --> 0.135518).  Saving model ...\n",
      "[ 22/200] train_loss: 0.12614 valid_loss: 0.14083 test_loss: 0.14745 \n",
      "[ 23/200] train_loss: 0.12461 valid_loss: 0.13443 test_loss: 0.14543 \n",
      "Validation loss decreased (0.135518 --> 0.134432).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12246 valid_loss: 0.13045 test_loss: 0.14427 \n",
      "Validation loss decreased (0.134432 --> 0.130447).  Saving model ...\n",
      "[ 25/200] train_loss: 0.12138 valid_loss: 0.12994 test_loss: 0.14560 \n",
      "Validation loss decreased (0.130447 --> 0.129940).  Saving model ...\n",
      "[ 26/200] train_loss: 0.12202 valid_loss: 0.12825 test_loss: 0.14361 \n",
      "Validation loss decreased (0.129940 --> 0.128245).  Saving model ...\n",
      "[ 27/200] train_loss: 0.11776 valid_loss: 0.13010 test_loss: 0.14277 \n",
      "[ 28/200] train_loss: 0.11804 valid_loss: 0.12568 test_loss: 0.14133 \n",
      "Validation loss decreased (0.128245 --> 0.125678).  Saving model ...\n",
      "[ 29/200] train_loss: 0.11917 valid_loss: 0.12623 test_loss: 0.14213 \n",
      "[ 30/200] train_loss: 0.11542 valid_loss: 0.12743 test_loss: 0.14253 \n",
      "[ 31/200] train_loss: 0.11748 valid_loss: 0.12403 test_loss: 0.13667 \n",
      "Validation loss decreased (0.125678 --> 0.124035).  Saving model ...\n",
      "[ 32/200] train_loss: 0.11369 valid_loss: 0.12313 test_loss: 0.13701 \n",
      "Validation loss decreased (0.124035 --> 0.123133).  Saving model ...\n",
      "[ 33/200] train_loss: 0.11360 valid_loss: 0.12076 test_loss: 0.13502 \n",
      "Validation loss decreased (0.123133 --> 0.120761).  Saving model ...\n",
      "[ 34/200] train_loss: 0.11405 valid_loss: 0.11947 test_loss: 0.13431 \n",
      "Validation loss decreased (0.120761 --> 0.119474).  Saving model ...\n",
      "[ 35/200] train_loss: 0.11330 valid_loss: 0.12282 test_loss: 0.13675 \n",
      "[ 36/200] train_loss: 0.11062 valid_loss: 0.11894 test_loss: 0.13352 \n",
      "Validation loss decreased (0.119474 --> 0.118940).  Saving model ...\n",
      "[ 37/200] train_loss: 0.10751 valid_loss: 0.12327 test_loss: 0.13922 \n",
      "[ 38/200] train_loss: 0.11028 valid_loss: 0.12224 test_loss: 0.13331 \n",
      "[ 39/200] train_loss: 0.10684 valid_loss: 0.12102 test_loss: 0.13352 \n",
      "[ 40/200] train_loss: 0.10670 valid_loss: 0.11776 test_loss: 0.13272 \n",
      "Validation loss decreased (0.118940 --> 0.117763).  Saving model ...\n",
      "[ 41/200] train_loss: 0.10733 valid_loss: 0.11649 test_loss: 0.13185 \n",
      "Validation loss decreased (0.117763 --> 0.116490).  Saving model ...\n",
      "[ 42/200] train_loss: 0.10590 valid_loss: 0.11704 test_loss: 0.12878 \n",
      "[ 43/200] train_loss: 0.10241 valid_loss: 0.11495 test_loss: 0.12746 \n",
      "Validation loss decreased (0.116490 --> 0.114950).  Saving model ...\n",
      "[ 44/200] train_loss: 0.10409 valid_loss: 0.11521 test_loss: 0.12823 \n",
      "[ 45/200] train_loss: 0.10691 valid_loss: 0.11589 test_loss: 0.12923 \n",
      "[ 46/200] train_loss: 0.10387 valid_loss: 0.11300 test_loss: 0.12703 \n",
      "Validation loss decreased (0.114950 --> 0.113003).  Saving model ...\n",
      "[ 47/200] train_loss: 0.10308 valid_loss: 0.11392 test_loss: 0.12781 \n",
      "[ 48/200] train_loss: 0.10255 valid_loss: 0.11074 test_loss: 0.12690 \n",
      "Validation loss decreased (0.113003 --> 0.110740).  Saving model ...\n",
      "[ 49/200] train_loss: 0.10214 valid_loss: 0.11056 test_loss: 0.12656 \n",
      "Validation loss decreased (0.110740 --> 0.110556).  Saving model ...\n",
      "[ 50/200] train_loss: 0.10194 valid_loss: 0.11120 test_loss: 0.12487 \n",
      "[ 51/200] train_loss: 0.10512 valid_loss: 0.11203 test_loss: 0.12424 \n",
      "[ 52/200] train_loss: 0.10146 valid_loss: 0.11296 test_loss: 0.12427 \n",
      "[ 53/200] train_loss: 0.10180 valid_loss: 0.10831 test_loss: 0.12425 \n",
      "Validation loss decreased (0.110556 --> 0.108305).  Saving model ...\n",
      "[ 54/200] train_loss: 0.10206 valid_loss: 0.10879 test_loss: 0.12322 \n",
      "[ 55/200] train_loss: 0.10123 valid_loss: 0.10864 test_loss: 0.12330 \n",
      "[ 56/200] train_loss: 0.09772 valid_loss: 0.10917 test_loss: 0.12198 \n",
      "[ 57/200] train_loss: 0.09941 valid_loss: 0.10586 test_loss: 0.12152 \n",
      "Validation loss decreased (0.108305 --> 0.105858).  Saving model ...\n",
      "[ 58/200] train_loss: 0.09885 valid_loss: 0.10993 test_loss: 0.12304 \n",
      "[ 59/200] train_loss: 0.09699 valid_loss: 0.10818 test_loss: 0.12123 \n",
      "[ 60/200] train_loss: 0.09913 valid_loss: 0.11019 test_loss: 0.12018 \n",
      "[ 61/200] train_loss: 0.09707 valid_loss: 0.10940 test_loss: 0.12319 \n",
      "[ 62/200] train_loss: 0.10106 valid_loss: 0.10366 test_loss: 0.11876 \n",
      "Validation loss decreased (0.105858 --> 0.103662).  Saving model ...\n",
      "[ 63/200] train_loss: 0.10051 valid_loss: 0.10501 test_loss: 0.12068 \n",
      "[ 64/200] train_loss: 0.09671 valid_loss: 0.10818 test_loss: 0.12147 \n",
      "[ 65/200] train_loss: 0.09613 valid_loss: 0.10532 test_loss: 0.11859 \n",
      "[ 66/200] train_loss: 0.09512 valid_loss: 0.10625 test_loss: 0.12026 \n",
      "[ 67/200] train_loss: 0.09531 valid_loss: 0.10573 test_loss: 0.11862 \n",
      "[ 68/200] train_loss: 0.09501 valid_loss: 0.10163 test_loss: 0.11743 \n",
      "Validation loss decreased (0.103662 --> 0.101632).  Saving model ...\n",
      "[ 69/200] train_loss: 0.09322 valid_loss: 0.10442 test_loss: 0.11843 \n",
      "[ 70/200] train_loss: 0.09593 valid_loss: 0.10241 test_loss: 0.11811 \n",
      "[ 71/200] train_loss: 0.09422 valid_loss: 0.10492 test_loss: 0.11719 \n",
      "[ 72/200] train_loss: 0.09460 valid_loss: 0.10078 test_loss: 0.11700 \n",
      "Validation loss decreased (0.101632 --> 0.100778).  Saving model ...\n",
      "[ 73/200] train_loss: 0.09174 valid_loss: 0.11055 test_loss: 0.12172 \n",
      "[ 74/200] train_loss: 0.09148 valid_loss: 0.10337 test_loss: 0.11726 \n",
      "[ 75/200] train_loss: 0.09343 valid_loss: 0.10173 test_loss: 0.11816 \n",
      "[ 76/200] train_loss: 0.09208 valid_loss: 0.10524 test_loss: 0.11915 \n",
      "[ 77/200] train_loss: 0.08957 valid_loss: 0.10125 test_loss: 0.11751 \n",
      "[ 78/200] train_loss: 0.09343 valid_loss: 0.10672 test_loss: 0.11632 \n",
      "[ 79/200] train_loss: 0.09288 valid_loss: 0.10567 test_loss: 0.11917 \n",
      "[ 80/200] train_loss: 0.09271 valid_loss: 0.10303 test_loss: 0.11815 \n",
      "[ 81/200] train_loss: 0.09292 valid_loss: 0.10132 test_loss: 0.11504 \n",
      "[ 82/200] train_loss: 0.09136 valid_loss: 0.10026 test_loss: 0.11481 \n",
      "Validation loss decreased (0.100778 --> 0.100263).  Saving model ...\n",
      "[ 83/200] train_loss: 0.09309 valid_loss: 0.10231 test_loss: 0.11579 \n",
      "[ 84/200] train_loss: 0.09071 valid_loss: 0.09972 test_loss: 0.11425 \n",
      "Validation loss decreased (0.100263 --> 0.099720).  Saving model ...\n",
      "[ 85/200] train_loss: 0.09246 valid_loss: 0.09935 test_loss: 0.11442 \n",
      "Validation loss decreased (0.099720 --> 0.099354).  Saving model ...\n",
      "[ 86/200] train_loss: 0.09185 valid_loss: 0.10213 test_loss: 0.11532 \n",
      "[ 87/200] train_loss: 0.09006 valid_loss: 0.09927 test_loss: 0.11370 \n",
      "Validation loss decreased (0.099354 --> 0.099275).  Saving model ...\n",
      "[ 88/200] train_loss: 0.09232 valid_loss: 0.10108 test_loss: 0.11449 \n",
      "[ 89/200] train_loss: 0.08902 valid_loss: 0.10228 test_loss: 0.11343 \n",
      "[ 90/200] train_loss: 0.08834 valid_loss: 0.10001 test_loss: 0.11422 \n",
      "[ 91/200] train_loss: 0.08933 valid_loss: 0.09874 test_loss: 0.11196 \n",
      "Validation loss decreased (0.099275 --> 0.098745).  Saving model ...\n",
      "[ 92/200] train_loss: 0.09255 valid_loss: 0.10100 test_loss: 0.11315 \n",
      "[ 93/200] train_loss: 0.09299 valid_loss: 0.09697 test_loss: 0.11077 \n",
      "Validation loss decreased (0.098745 --> 0.096969).  Saving model ...\n",
      "[ 94/200] train_loss: 0.09011 valid_loss: 0.09923 test_loss: 0.11107 \n",
      "[ 95/200] train_loss: 0.08764 valid_loss: 0.09681 test_loss: 0.11101 \n",
      "Validation loss decreased (0.096969 --> 0.096807).  Saving model ...\n",
      "[ 96/200] train_loss: 0.08877 valid_loss: 0.09580 test_loss: 0.11094 \n",
      "Validation loss decreased (0.096807 --> 0.095800).  Saving model ...\n",
      "[ 97/200] train_loss: 0.08653 valid_loss: 0.09796 test_loss: 0.11104 \n",
      "[ 98/200] train_loss: 0.08976 valid_loss: 0.09749 test_loss: 0.11024 \n",
      "[ 99/200] train_loss: 0.08923 valid_loss: 0.09584 test_loss: 0.11082 \n",
      "[100/200] train_loss: 0.08840 valid_loss: 0.09738 test_loss: 0.11018 \n",
      "[101/200] train_loss: 0.08446 valid_loss: 0.09617 test_loss: 0.11153 \n",
      "[102/200] train_loss: 0.08867 valid_loss: 0.09519 test_loss: 0.10877 \n",
      "Validation loss decreased (0.095800 --> 0.095193).  Saving model ...\n",
      "[103/200] train_loss: 0.08660 valid_loss: 0.09788 test_loss: 0.11001 \n",
      "[104/200] train_loss: 0.08769 valid_loss: 0.09838 test_loss: 0.11082 \n",
      "[105/200] train_loss: 0.08676 valid_loss: 0.09826 test_loss: 0.10988 \n",
      "[106/200] train_loss: 0.08478 valid_loss: 0.09646 test_loss: 0.10965 \n",
      "[107/200] train_loss: 0.08694 valid_loss: 0.09734 test_loss: 0.11102 \n",
      "[108/200] train_loss: 0.08729 valid_loss: 0.09588 test_loss: 0.11307 \n",
      "[109/200] train_loss: 0.08577 valid_loss: 0.09976 test_loss: 0.10995 \n",
      "[110/200] train_loss: 0.08534 valid_loss: 0.09646 test_loss: 0.10918 \n",
      "[111/200] train_loss: 0.08723 valid_loss: 0.09659 test_loss: 0.10837 \n",
      "[112/200] train_loss: 0.08985 valid_loss: 0.09492 test_loss: 0.11009 \n",
      "Validation loss decreased (0.095193 --> 0.094920).  Saving model ...\n",
      "[113/200] train_loss: 0.08650 valid_loss: 0.09606 test_loss: 0.10908 \n",
      "[114/200] train_loss: 0.08576 valid_loss: 0.09362 test_loss: 0.10918 \n",
      "Validation loss decreased (0.094920 --> 0.093623).  Saving model ...\n",
      "[115/200] train_loss: 0.08593 valid_loss: 0.09508 test_loss: 0.10842 \n",
      "[116/200] train_loss: 0.08527 valid_loss: 0.09367 test_loss: 0.10746 \n",
      "[117/200] train_loss: 0.08225 valid_loss: 0.09090 test_loss: 0.10763 \n",
      "Validation loss decreased (0.093623 --> 0.090901).  Saving model ...\n",
      "[118/200] train_loss: 0.08379 valid_loss: 0.09241 test_loss: 0.10718 \n",
      "[119/200] train_loss: 0.08578 valid_loss: 0.09307 test_loss: 0.10710 \n",
      "[120/200] train_loss: 0.08727 valid_loss: 0.09227 test_loss: 0.10700 \n",
      "[121/200] train_loss: 0.08503 valid_loss: 0.09720 test_loss: 0.10878 \n",
      "[122/200] train_loss: 0.08209 valid_loss: 0.09072 test_loss: 0.10732 \n",
      "Validation loss decreased (0.090901 --> 0.090721).  Saving model ...\n",
      "[123/200] train_loss: 0.08528 valid_loss: 0.09334 test_loss: 0.10675 \n",
      "[124/200] train_loss: 0.08339 valid_loss: 0.09259 test_loss: 0.10751 \n",
      "[125/200] train_loss: 0.08416 valid_loss: 0.09162 test_loss: 0.10517 \n",
      "[126/200] train_loss: 0.08588 valid_loss: 0.09446 test_loss: 0.10663 \n",
      "[127/200] train_loss: 0.08416 valid_loss: 0.09142 test_loss: 0.10553 \n",
      "[128/200] train_loss: 0.08271 valid_loss: 0.09321 test_loss: 0.10664 \n",
      "[129/200] train_loss: 0.08177 valid_loss: 0.09184 test_loss: 0.10579 \n",
      "[130/200] train_loss: 0.08278 valid_loss: 0.09269 test_loss: 0.10493 \n",
      "[131/200] train_loss: 0.08122 valid_loss: 0.09664 test_loss: 0.10808 \n",
      "[132/200] train_loss: 0.08260 valid_loss: 0.09254 test_loss: 0.10460 \n",
      "[133/200] train_loss: 0.08337 valid_loss: 0.09140 test_loss: 0.10513 \n",
      "[134/200] train_loss: 0.08212 valid_loss: 0.09551 test_loss: 0.10611 \n",
      "[135/200] train_loss: 0.08166 valid_loss: 0.09098 test_loss: 0.10518 \n",
      "[136/200] train_loss: 0.08335 valid_loss: 0.08968 test_loss: 0.10437 \n",
      "Validation loss decreased (0.090721 --> 0.089684).  Saving model ...\n",
      "[137/200] train_loss: 0.08107 valid_loss: 0.09196 test_loss: 0.10538 \n",
      "[138/200] train_loss: 0.08146 valid_loss: 0.09090 test_loss: 0.10411 \n",
      "[139/200] train_loss: 0.08068 valid_loss: 0.09244 test_loss: 0.10285 \n",
      "[140/200] train_loss: 0.08077 valid_loss: 0.09197 test_loss: 0.10340 \n",
      "[141/200] train_loss: 0.08256 valid_loss: 0.09221 test_loss: 0.10319 \n",
      "[142/200] train_loss: 0.08397 valid_loss: 0.09088 test_loss: 0.10540 \n",
      "[143/200] train_loss: 0.08092 valid_loss: 0.09320 test_loss: 0.10806 \n",
      "[144/200] train_loss: 0.08072 valid_loss: 0.09236 test_loss: 0.10384 \n",
      "[145/200] train_loss: 0.07747 valid_loss: 0.09274 test_loss: 0.10469 \n",
      "[146/200] train_loss: 0.08283 valid_loss: 0.09040 test_loss: 0.10168 \n",
      "[147/200] train_loss: 0.08085 valid_loss: 0.09687 test_loss: 0.10361 \n",
      "[148/200] train_loss: 0.07945 valid_loss: 0.09287 test_loss: 0.10210 \n",
      "[149/200] train_loss: 0.08057 valid_loss: 0.09063 test_loss: 0.10267 \n",
      "[150/200] train_loss: 0.07972 valid_loss: 0.09115 test_loss: 0.10361 \n",
      "[151/200] train_loss: 0.08007 valid_loss: 0.09021 test_loss: 0.10086 \n",
      "[152/200] train_loss: 0.07984 valid_loss: 0.08862 test_loss: 0.10155 \n",
      "Validation loss decreased (0.089684 --> 0.088623).  Saving model ...\n",
      "[153/200] train_loss: 0.07845 valid_loss: 0.09111 test_loss: 0.10451 \n",
      "[154/200] train_loss: 0.08199 valid_loss: 0.08688 test_loss: 0.10281 \n",
      "Validation loss decreased (0.088623 --> 0.086884).  Saving model ...\n",
      "[155/200] train_loss: 0.08180 valid_loss: 0.08850 test_loss: 0.10123 \n",
      "[156/200] train_loss: 0.08318 valid_loss: 0.09125 test_loss: 0.10216 \n",
      "[157/200] train_loss: 0.08230 valid_loss: 0.09125 test_loss: 0.10156 \n",
      "[158/200] train_loss: 0.07830 valid_loss: 0.09067 test_loss: 0.10310 \n",
      "[159/200] train_loss: 0.07923 valid_loss: 0.09331 test_loss: 0.10098 \n",
      "[160/200] train_loss: 0.07932 valid_loss: 0.08991 test_loss: 0.10379 \n",
      "[161/200] train_loss: 0.07889 valid_loss: 0.09364 test_loss: 0.10405 \n",
      "[162/200] train_loss: 0.08004 valid_loss: 0.08843 test_loss: 0.10152 \n",
      "[163/200] train_loss: 0.07889 valid_loss: 0.09645 test_loss: 0.10271 \n",
      "[164/200] train_loss: 0.07937 valid_loss: 0.09078 test_loss: 0.10279 \n",
      "[165/200] train_loss: 0.07665 valid_loss: 0.08909 test_loss: 0.10258 \n",
      "[166/200] train_loss: 0.07764 valid_loss: 0.08830 test_loss: 0.10161 \n",
      "[167/200] train_loss: 0.07888 valid_loss: 0.08809 test_loss: 0.10035 \n",
      "[168/200] train_loss: 0.07545 valid_loss: 0.09019 test_loss: 0.10156 \n",
      "[169/200] train_loss: 0.07777 valid_loss: 0.08947 test_loss: 0.10128 \n",
      "[170/200] train_loss: 0.07693 valid_loss: 0.08868 test_loss: 0.10156 \n",
      "[171/200] train_loss: 0.07720 valid_loss: 0.08756 test_loss: 0.09891 \n",
      "[172/200] train_loss: 0.07377 valid_loss: 0.09334 test_loss: 0.10068 \n",
      "[173/200] train_loss: 0.07704 valid_loss: 0.08947 test_loss: 0.09926 \n",
      "[174/200] train_loss: 0.07804 valid_loss: 0.08888 test_loss: 0.09885 \n",
      "[175/200] train_loss: 0.07928 valid_loss: 0.08692 test_loss: 0.10034 \n",
      "[176/200] train_loss: 0.07790 valid_loss: 0.08709 test_loss: 0.09967 \n",
      "[177/200] train_loss: 0.07822 valid_loss: 0.08832 test_loss: 0.10048 \n",
      "[178/200] train_loss: 0.07603 valid_loss: 0.08626 test_loss: 0.09911 \n",
      "Validation loss decreased (0.086884 --> 0.086262).  Saving model ...\n",
      "[179/200] train_loss: 0.07735 valid_loss: 0.09000 test_loss: 0.10202 \n",
      "[180/200] train_loss: 0.07767 valid_loss: 0.08682 test_loss: 0.10121 \n",
      "[181/200] train_loss: 0.07493 valid_loss: 0.08513 test_loss: 0.09816 \n",
      "Validation loss decreased (0.086262 --> 0.085127).  Saving model ...\n",
      "[182/200] train_loss: 0.07571 valid_loss: 0.08722 test_loss: 0.09841 \n",
      "[183/200] train_loss: 0.07625 valid_loss: 0.08645 test_loss: 0.09789 \n",
      "[184/200] train_loss: 0.07620 valid_loss: 0.08497 test_loss: 0.09860 \n",
      "Validation loss decreased (0.085127 --> 0.084969).  Saving model ...\n",
      "[185/200] train_loss: 0.07754 valid_loss: 0.09435 test_loss: 0.09956 \n",
      "[186/200] train_loss: 0.07613 valid_loss: 0.08747 test_loss: 0.10049 \n",
      "[187/200] train_loss: 0.07536 valid_loss: 0.08732 test_loss: 0.09906 \n",
      "[188/200] train_loss: 0.07739 valid_loss: 0.08704 test_loss: 0.10017 \n",
      "[189/200] train_loss: 0.07450 valid_loss: 0.08677 test_loss: 0.09955 \n",
      "[190/200] train_loss: 0.07629 valid_loss: 0.08674 test_loss: 0.09812 \n",
      "[191/200] train_loss: 0.07664 valid_loss: 0.08712 test_loss: 0.09927 \n",
      "[192/200] train_loss: 0.07630 valid_loss: 0.08639 test_loss: 0.09834 \n",
      "[193/200] train_loss: 0.07689 valid_loss: 0.08799 test_loss: 0.09849 \n",
      "[194/200] train_loss: 0.07542 valid_loss: 0.08681 test_loss: 0.09839 \n",
      "[195/200] train_loss: 0.07543 valid_loss: 0.09277 test_loss: 0.09892 \n",
      "[196/200] train_loss: 0.07590 valid_loss: 0.08646 test_loss: 0.09762 \n",
      "[197/200] train_loss: 0.07453 valid_loss: 0.08797 test_loss: 0.09888 \n",
      "[198/200] train_loss: 0.07531 valid_loss: 0.08669 test_loss: 0.09806 \n",
      "[199/200] train_loss: 0.07364 valid_loss: 0.08825 test_loss: 0.09720 \n",
      "[200/200] train_loss: 0.07606 valid_loss: 0.09903 test_loss: 0.09769 \n",
      "Model 16 trained for seen data.\n",
      "TRAINING MODEL for seen house 17\n",
      "[  1/200] train_loss: 0.54457 valid_loss: 0.43624 test_loss: 0.44600 \n",
      "Validation loss decreased (inf --> 0.436242).  Saving model ...\n",
      "[  2/200] train_loss: 0.35957 valid_loss: 0.32016 test_loss: 0.33498 \n",
      "Validation loss decreased (0.436242 --> 0.320162).  Saving model ...\n",
      "[  3/200] train_loss: 0.27176 valid_loss: 0.26691 test_loss: 0.28609 \n",
      "Validation loss decreased (0.320162 --> 0.266908).  Saving model ...\n",
      "[  4/200] train_loss: 0.23067 valid_loss: 0.23298 test_loss: 0.24961 \n",
      "Validation loss decreased (0.266908 --> 0.232978).  Saving model ...\n",
      "[  5/200] train_loss: 0.20683 valid_loss: 0.21015 test_loss: 0.22547 \n",
      "Validation loss decreased (0.232978 --> 0.210147).  Saving model ...\n",
      "[  6/200] train_loss: 0.19037 valid_loss: 0.19327 test_loss: 0.20847 \n",
      "Validation loss decreased (0.210147 --> 0.193275).  Saving model ...\n",
      "[  7/200] train_loss: 0.17613 valid_loss: 0.18406 test_loss: 0.19896 \n",
      "Validation loss decreased (0.193275 --> 0.184056).  Saving model ...\n",
      "[  8/200] train_loss: 0.17181 valid_loss: 0.17311 test_loss: 0.18525 \n",
      "Validation loss decreased (0.184056 --> 0.173110).  Saving model ...\n",
      "[  9/200] train_loss: 0.16043 valid_loss: 0.16462 test_loss: 0.17861 \n",
      "Validation loss decreased (0.173110 --> 0.164619).  Saving model ...\n",
      "[ 10/200] train_loss: 0.15287 valid_loss: 0.16304 test_loss: 0.17599 \n",
      "Validation loss decreased (0.164619 --> 0.163039).  Saving model ...\n",
      "[ 11/200] train_loss: 0.14916 valid_loss: 0.15599 test_loss: 0.16983 \n",
      "Validation loss decreased (0.163039 --> 0.155990).  Saving model ...\n",
      "[ 12/200] train_loss: 0.14624 valid_loss: 0.15385 test_loss: 0.16822 \n",
      "Validation loss decreased (0.155990 --> 0.153854).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14129 valid_loss: 0.14825 test_loss: 0.16140 \n",
      "Validation loss decreased (0.153854 --> 0.148249).  Saving model ...\n",
      "[ 14/200] train_loss: 0.13865 valid_loss: 0.14798 test_loss: 0.16199 \n",
      "Validation loss decreased (0.148249 --> 0.147981).  Saving model ...\n",
      "[ 15/200] train_loss: 0.13706 valid_loss: 0.14158 test_loss: 0.15498 \n",
      "Validation loss decreased (0.147981 --> 0.141580).  Saving model ...\n",
      "[ 16/200] train_loss: 0.13362 valid_loss: 0.14264 test_loss: 0.15621 \n",
      "[ 17/200] train_loss: 0.13319 valid_loss: 0.14009 test_loss: 0.15175 \n",
      "Validation loss decreased (0.141580 --> 0.140090).  Saving model ...\n",
      "[ 18/200] train_loss: 0.13242 valid_loss: 0.13931 test_loss: 0.15410 \n",
      "Validation loss decreased (0.140090 --> 0.139307).  Saving model ...\n",
      "[ 19/200] train_loss: 0.12703 valid_loss: 0.14060 test_loss: 0.15188 \n",
      "[ 20/200] train_loss: 0.12461 valid_loss: 0.13213 test_loss: 0.14737 \n",
      "Validation loss decreased (0.139307 --> 0.132131).  Saving model ...\n",
      "[ 21/200] train_loss: 0.12735 valid_loss: 0.13218 test_loss: 0.14856 \n",
      "[ 22/200] train_loss: 0.12285 valid_loss: 0.13073 test_loss: 0.14646 \n",
      "Validation loss decreased (0.132131 --> 0.130732).  Saving model ...\n",
      "[ 23/200] train_loss: 0.12112 valid_loss: 0.12651 test_loss: 0.14270 \n",
      "Validation loss decreased (0.130732 --> 0.126506).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12368 valid_loss: 0.12799 test_loss: 0.14668 \n",
      "[ 25/200] train_loss: 0.11880 valid_loss: 0.12532 test_loss: 0.14172 \n",
      "Validation loss decreased (0.126506 --> 0.125321).  Saving model ...\n",
      "[ 26/200] train_loss: 0.11889 valid_loss: 0.12405 test_loss: 0.14110 \n",
      "Validation loss decreased (0.125321 --> 0.124049).  Saving model ...\n",
      "[ 27/200] train_loss: 0.11834 valid_loss: 0.12397 test_loss: 0.13994 \n",
      "Validation loss decreased (0.124049 --> 0.123973).  Saving model ...\n",
      "[ 28/200] train_loss: 0.11804 valid_loss: 0.12139 test_loss: 0.13851 \n",
      "Validation loss decreased (0.123973 --> 0.121395).  Saving model ...\n",
      "[ 29/200] train_loss: 0.11434 valid_loss: 0.12498 test_loss: 0.13726 \n",
      "[ 30/200] train_loss: 0.11833 valid_loss: 0.12016 test_loss: 0.13723 \n",
      "Validation loss decreased (0.121395 --> 0.120156).  Saving model ...\n",
      "[ 31/200] train_loss: 0.11759 valid_loss: 0.12290 test_loss: 0.13716 \n",
      "[ 32/200] train_loss: 0.11396 valid_loss: 0.12550 test_loss: 0.13624 \n",
      "[ 33/200] train_loss: 0.11099 valid_loss: 0.12269 test_loss: 0.13783 \n",
      "[ 34/200] train_loss: 0.10767 valid_loss: 0.11966 test_loss: 0.13548 \n",
      "Validation loss decreased (0.120156 --> 0.119658).  Saving model ...\n",
      "[ 35/200] train_loss: 0.10979 valid_loss: 0.11790 test_loss: 0.13435 \n",
      "Validation loss decreased (0.119658 --> 0.117898).  Saving model ...\n",
      "[ 36/200] train_loss: 0.11082 valid_loss: 0.11913 test_loss: 0.13374 \n",
      "[ 37/200] train_loss: 0.11071 valid_loss: 0.11540 test_loss: 0.13139 \n",
      "Validation loss decreased (0.117898 --> 0.115401).  Saving model ...\n",
      "[ 38/200] train_loss: 0.10807 valid_loss: 0.11576 test_loss: 0.13093 \n",
      "[ 39/200] train_loss: 0.11033 valid_loss: 0.11675 test_loss: 0.13198 \n",
      "[ 40/200] train_loss: 0.10747 valid_loss: 0.11467 test_loss: 0.13022 \n",
      "Validation loss decreased (0.115401 --> 0.114672).  Saving model ...\n",
      "[ 41/200] train_loss: 0.10658 valid_loss: 0.11302 test_loss: 0.12917 \n",
      "Validation loss decreased (0.114672 --> 0.113015).  Saving model ...\n",
      "[ 42/200] train_loss: 0.10507 valid_loss: 0.11565 test_loss: 0.13239 \n",
      "[ 43/200] train_loss: 0.10645 valid_loss: 0.11295 test_loss: 0.12967 \n",
      "Validation loss decreased (0.113015 --> 0.112951).  Saving model ...\n",
      "[ 44/200] train_loss: 0.10547 valid_loss: 0.11426 test_loss: 0.12863 \n",
      "[ 45/200] train_loss: 0.10605 valid_loss: 0.11315 test_loss: 0.12807 \n",
      "[ 46/200] train_loss: 0.10389 valid_loss: 0.11319 test_loss: 0.12886 \n",
      "[ 47/200] train_loss: 0.10269 valid_loss: 0.11260 test_loss: 0.12673 \n",
      "Validation loss decreased (0.112951 --> 0.112603).  Saving model ...\n",
      "[ 48/200] train_loss: 0.10398 valid_loss: 0.11283 test_loss: 0.12653 \n",
      "[ 49/200] train_loss: 0.10484 valid_loss: 0.10950 test_loss: 0.12586 \n",
      "Validation loss decreased (0.112603 --> 0.109496).  Saving model ...\n",
      "[ 50/200] train_loss: 0.10186 valid_loss: 0.10733 test_loss: 0.12319 \n",
      "Validation loss decreased (0.109496 --> 0.107333).  Saving model ...\n",
      "[ 51/200] train_loss: 0.10211 valid_loss: 0.11108 test_loss: 0.12486 \n",
      "[ 52/200] train_loss: 0.10276 valid_loss: 0.10893 test_loss: 0.12439 \n",
      "[ 53/200] train_loss: 0.09752 valid_loss: 0.11120 test_loss: 0.12397 \n",
      "[ 54/200] train_loss: 0.09891 valid_loss: 0.10863 test_loss: 0.12180 \n",
      "[ 55/200] train_loss: 0.09872 valid_loss: 0.10881 test_loss: 0.12358 \n",
      "[ 56/200] train_loss: 0.09785 valid_loss: 0.10704 test_loss: 0.12172 \n",
      "Validation loss decreased (0.107333 --> 0.107042).  Saving model ...\n",
      "[ 57/200] train_loss: 0.09804 valid_loss: 0.11109 test_loss: 0.12157 \n",
      "[ 58/200] train_loss: 0.09861 valid_loss: 0.10621 test_loss: 0.12052 \n",
      "Validation loss decreased (0.107042 --> 0.106213).  Saving model ...\n",
      "[ 59/200] train_loss: 0.10011 valid_loss: 0.10910 test_loss: 0.12466 \n",
      "[ 60/200] train_loss: 0.09908 valid_loss: 0.10588 test_loss: 0.12011 \n",
      "Validation loss decreased (0.106213 --> 0.105884).  Saving model ...\n",
      "[ 61/200] train_loss: 0.09929 valid_loss: 0.10606 test_loss: 0.11880 \n",
      "[ 62/200] train_loss: 0.09879 valid_loss: 0.10655 test_loss: 0.12104 \n",
      "[ 63/200] train_loss: 0.09572 valid_loss: 0.10549 test_loss: 0.11837 \n",
      "Validation loss decreased (0.105884 --> 0.105485).  Saving model ...\n",
      "[ 64/200] train_loss: 0.09744 valid_loss: 0.10760 test_loss: 0.12151 \n",
      "[ 65/200] train_loss: 0.09813 valid_loss: 0.10626 test_loss: 0.12088 \n",
      "[ 66/200] train_loss: 0.09782 valid_loss: 0.10602 test_loss: 0.11958 \n",
      "[ 67/200] train_loss: 0.09278 valid_loss: 0.10548 test_loss: 0.11800 \n",
      "Validation loss decreased (0.105485 --> 0.105480).  Saving model ...\n",
      "[ 68/200] train_loss: 0.09558 valid_loss: 0.10253 test_loss: 0.11567 \n",
      "Validation loss decreased (0.105480 --> 0.102527).  Saving model ...\n",
      "[ 69/200] train_loss: 0.09668 valid_loss: 0.10415 test_loss: 0.11769 \n",
      "[ 70/200] train_loss: 0.09400 valid_loss: 0.10076 test_loss: 0.11446 \n",
      "Validation loss decreased (0.102527 --> 0.100761).  Saving model ...\n",
      "[ 71/200] train_loss: 0.09468 valid_loss: 0.10110 test_loss: 0.11731 \n",
      "[ 72/200] train_loss: 0.09288 valid_loss: 0.10218 test_loss: 0.11646 \n",
      "[ 73/200] train_loss: 0.09777 valid_loss: 0.10579 test_loss: 0.11757 \n",
      "[ 74/200] train_loss: 0.09094 valid_loss: 0.10237 test_loss: 0.11784 \n",
      "[ 75/200] train_loss: 0.09297 valid_loss: 0.10460 test_loss: 0.11588 \n",
      "[ 76/200] train_loss: 0.09390 valid_loss: 0.10424 test_loss: 0.11689 \n",
      "[ 77/200] train_loss: 0.09402 valid_loss: 0.10455 test_loss: 0.11401 \n",
      "[ 78/200] train_loss: 0.09187 valid_loss: 0.10218 test_loss: 0.11371 \n",
      "[ 79/200] train_loss: 0.09149 valid_loss: 0.10044 test_loss: 0.11551 \n",
      "Validation loss decreased (0.100761 --> 0.100441).  Saving model ...\n",
      "[ 80/200] train_loss: 0.09107 valid_loss: 0.10364 test_loss: 0.11321 \n",
      "[ 81/200] train_loss: 0.09155 valid_loss: 0.10342 test_loss: 0.11525 \n",
      "[ 82/200] train_loss: 0.08999 valid_loss: 0.10122 test_loss: 0.11237 \n",
      "[ 83/200] train_loss: 0.09116 valid_loss: 0.10121 test_loss: 0.11424 \n",
      "[ 84/200] train_loss: 0.09096 valid_loss: 0.09691 test_loss: 0.11164 \n",
      "Validation loss decreased (0.100441 --> 0.096909).  Saving model ...\n",
      "[ 85/200] train_loss: 0.09156 valid_loss: 0.09942 test_loss: 0.11336 \n",
      "[ 86/200] train_loss: 0.09106 valid_loss: 0.10099 test_loss: 0.11170 \n",
      "[ 87/200] train_loss: 0.09039 valid_loss: 0.09942 test_loss: 0.11282 \n",
      "[ 88/200] train_loss: 0.08885 valid_loss: 0.10000 test_loss: 0.11233 \n",
      "[ 89/200] train_loss: 0.09018 valid_loss: 0.09816 test_loss: 0.11238 \n",
      "[ 90/200] train_loss: 0.08888 valid_loss: 0.09675 test_loss: 0.11266 \n",
      "Validation loss decreased (0.096909 --> 0.096753).  Saving model ...\n",
      "[ 91/200] train_loss: 0.08914 valid_loss: 0.09706 test_loss: 0.11031 \n",
      "[ 92/200] train_loss: 0.09228 valid_loss: 0.09911 test_loss: 0.11204 \n",
      "[ 93/200] train_loss: 0.08735 valid_loss: 0.09585 test_loss: 0.10976 \n",
      "Validation loss decreased (0.096753 --> 0.095847).  Saving model ...\n",
      "[ 94/200] train_loss: 0.09109 valid_loss: 0.09557 test_loss: 0.11086 \n",
      "Validation loss decreased (0.095847 --> 0.095571).  Saving model ...\n",
      "[ 95/200] train_loss: 0.08906 valid_loss: 0.09602 test_loss: 0.10945 \n",
      "[ 96/200] train_loss: 0.08758 valid_loss: 0.09685 test_loss: 0.10966 \n",
      "[ 97/200] train_loss: 0.08766 valid_loss: 0.09640 test_loss: 0.10833 \n",
      "[ 98/200] train_loss: 0.08697 valid_loss: 0.09787 test_loss: 0.11023 \n",
      "[ 99/200] train_loss: 0.08990 valid_loss: 0.09668 test_loss: 0.10850 \n",
      "[100/200] train_loss: 0.08919 valid_loss: 0.09593 test_loss: 0.10886 \n",
      "[101/200] train_loss: 0.08376 valid_loss: 0.09699 test_loss: 0.10873 \n",
      "[102/200] train_loss: 0.08672 valid_loss: 0.09642 test_loss: 0.10965 \n",
      "[103/200] train_loss: 0.08683 valid_loss: 0.09491 test_loss: 0.10759 \n",
      "Validation loss decreased (0.095571 --> 0.094908).  Saving model ...\n",
      "[104/200] train_loss: 0.08606 valid_loss: 0.09634 test_loss: 0.10767 \n",
      "[105/200] train_loss: 0.08294 valid_loss: 0.09984 test_loss: 0.10905 \n",
      "[106/200] train_loss: 0.08466 valid_loss: 0.09572 test_loss: 0.10800 \n",
      "[107/200] train_loss: 0.08401 valid_loss: 0.09475 test_loss: 0.10728 \n",
      "Validation loss decreased (0.094908 --> 0.094745).  Saving model ...\n",
      "[108/200] train_loss: 0.08583 valid_loss: 0.09823 test_loss: 0.10869 \n",
      "[109/200] train_loss: 0.08769 valid_loss: 0.09361 test_loss: 0.10722 \n",
      "Validation loss decreased (0.094745 --> 0.093606).  Saving model ...\n",
      "[110/200] train_loss: 0.08665 valid_loss: 0.09535 test_loss: 0.10802 \n",
      "[111/200] train_loss: 0.08430 valid_loss: 0.09479 test_loss: 0.10710 \n",
      "[112/200] train_loss: 0.08293 valid_loss: 0.09479 test_loss: 0.10818 \n",
      "[113/200] train_loss: 0.08274 valid_loss: 0.09476 test_loss: 0.10638 \n",
      "[114/200] train_loss: 0.08371 valid_loss: 0.09099 test_loss: 0.10579 \n",
      "Validation loss decreased (0.093606 --> 0.090989).  Saving model ...\n",
      "[115/200] train_loss: 0.08570 valid_loss: 0.09100 test_loss: 0.10585 \n",
      "[116/200] train_loss: 0.08270 valid_loss: 0.09321 test_loss: 0.10454 \n",
      "[117/200] train_loss: 0.08474 valid_loss: 0.09373 test_loss: 0.10612 \n",
      "[118/200] train_loss: 0.08512 valid_loss: 0.09283 test_loss: 0.10577 \n",
      "[119/200] train_loss: 0.08192 valid_loss: 0.09901 test_loss: 0.10719 \n",
      "[120/200] train_loss: 0.08400 valid_loss: 0.09286 test_loss: 0.10612 \n",
      "[121/200] train_loss: 0.08354 valid_loss: 0.09293 test_loss: 0.10580 \n",
      "[122/200] train_loss: 0.08582 valid_loss: 0.09392 test_loss: 0.10608 \n",
      "[123/200] train_loss: 0.08539 valid_loss: 0.09184 test_loss: 0.10420 \n",
      "[124/200] train_loss: 0.08248 valid_loss: 0.09183 test_loss: 0.10354 \n",
      "[125/200] train_loss: 0.08319 valid_loss: 0.09299 test_loss: 0.10276 \n",
      "[126/200] train_loss: 0.08227 valid_loss: 0.09373 test_loss: 0.10390 \n",
      "[127/200] train_loss: 0.08201 valid_loss: 0.09480 test_loss: 0.10648 \n",
      "[128/200] train_loss: 0.08628 valid_loss: 0.09323 test_loss: 0.10384 \n",
      "[129/200] train_loss: 0.08255 valid_loss: 0.09294 test_loss: 0.10481 \n",
      "[130/200] train_loss: 0.08364 valid_loss: 0.08850 test_loss: 0.10271 \n",
      "Validation loss decreased (0.090989 --> 0.088501).  Saving model ...\n",
      "[131/200] train_loss: 0.08223 valid_loss: 0.09127 test_loss: 0.10399 \n",
      "[132/200] train_loss: 0.08249 valid_loss: 0.09145 test_loss: 0.10205 \n",
      "[133/200] train_loss: 0.07923 valid_loss: 0.09318 test_loss: 0.10290 \n",
      "[134/200] train_loss: 0.08194 valid_loss: 0.09331 test_loss: 0.10333 \n",
      "[135/200] train_loss: 0.08137 valid_loss: 0.09243 test_loss: 0.10303 \n",
      "[136/200] train_loss: 0.08074 valid_loss: 0.08903 test_loss: 0.10236 \n",
      "[137/200] train_loss: 0.08106 valid_loss: 0.09035 test_loss: 0.10314 \n",
      "[138/200] train_loss: 0.08175 valid_loss: 0.08962 test_loss: 0.10131 \n",
      "[139/200] train_loss: 0.08114 valid_loss: 0.09182 test_loss: 0.10342 \n",
      "[140/200] train_loss: 0.07840 valid_loss: 0.09169 test_loss: 0.10243 \n",
      "[141/200] train_loss: 0.08181 valid_loss: 0.08901 test_loss: 0.10116 \n",
      "[142/200] train_loss: 0.08056 valid_loss: 0.08997 test_loss: 0.10182 \n",
      "[143/200] train_loss: 0.08173 valid_loss: 0.08916 test_loss: 0.10169 \n",
      "[144/200] train_loss: 0.08101 valid_loss: 0.08943 test_loss: 0.10126 \n",
      "[145/200] train_loss: 0.08066 valid_loss: 0.08855 test_loss: 0.10071 \n",
      "[146/200] train_loss: 0.07908 valid_loss: 0.08869 test_loss: 0.10102 \n",
      "[147/200] train_loss: 0.08146 valid_loss: 0.08965 test_loss: 0.10229 \n",
      "[148/200] train_loss: 0.08180 valid_loss: 0.09212 test_loss: 0.10315 \n",
      "[149/200] train_loss: 0.08022 valid_loss: 0.08793 test_loss: 0.09954 \n",
      "Validation loss decreased (0.088501 --> 0.087934).  Saving model ...\n",
      "[150/200] train_loss: 0.07922 valid_loss: 0.08764 test_loss: 0.10041 \n",
      "Validation loss decreased (0.087934 --> 0.087642).  Saving model ...\n",
      "[151/200] train_loss: 0.07965 valid_loss: 0.08692 test_loss: 0.09929 \n",
      "Validation loss decreased (0.087642 --> 0.086917).  Saving model ...\n",
      "[152/200] train_loss: 0.07805 valid_loss: 0.08777 test_loss: 0.09919 \n",
      "[153/200] train_loss: 0.07884 valid_loss: 0.08920 test_loss: 0.10052 \n",
      "[154/200] train_loss: 0.07959 valid_loss: 0.09309 test_loss: 0.09953 \n",
      "[155/200] train_loss: 0.07753 valid_loss: 0.08802 test_loss: 0.10108 \n",
      "[156/200] train_loss: 0.07856 valid_loss: 0.08726 test_loss: 0.09998 \n",
      "[157/200] train_loss: 0.07793 valid_loss: 0.09022 test_loss: 0.09991 \n",
      "[158/200] train_loss: 0.07707 valid_loss: 0.09021 test_loss: 0.10132 \n",
      "[159/200] train_loss: 0.07916 valid_loss: 0.08823 test_loss: 0.10010 \n",
      "[160/200] train_loss: 0.07841 valid_loss: 0.08619 test_loss: 0.09909 \n",
      "Validation loss decreased (0.086917 --> 0.086192).  Saving model ...\n",
      "[161/200] train_loss: 0.07857 valid_loss: 0.09022 test_loss: 0.09955 \n",
      "[162/200] train_loss: 0.07687 valid_loss: 0.08754 test_loss: 0.09844 \n",
      "[163/200] train_loss: 0.07882 valid_loss: 0.08822 test_loss: 0.09931 \n",
      "[164/200] train_loss: 0.07747 valid_loss: 0.09511 test_loss: 0.10566 \n",
      "[165/200] train_loss: 0.07814 valid_loss: 0.08689 test_loss: 0.09885 \n",
      "[166/200] train_loss: 0.07826 valid_loss: 0.08594 test_loss: 0.09801 \n",
      "Validation loss decreased (0.086192 --> 0.085942).  Saving model ...\n",
      "[167/200] train_loss: 0.07559 valid_loss: 0.08693 test_loss: 0.09907 \n",
      "[168/200] train_loss: 0.07803 valid_loss: 0.08827 test_loss: 0.09745 \n",
      "[169/200] train_loss: 0.07699 valid_loss: 0.08626 test_loss: 0.09722 \n",
      "[170/200] train_loss: 0.07686 valid_loss: 0.08748 test_loss: 0.09933 \n",
      "[171/200] train_loss: 0.07963 valid_loss: 0.08899 test_loss: 0.09838 \n",
      "[172/200] train_loss: 0.07703 valid_loss: 0.08657 test_loss: 0.09702 \n",
      "[173/200] train_loss: 0.07629 valid_loss: 0.08587 test_loss: 0.09788 \n",
      "Validation loss decreased (0.085942 --> 0.085865).  Saving model ...\n",
      "[174/200] train_loss: 0.07653 valid_loss: 0.08639 test_loss: 0.09807 \n",
      "[175/200] train_loss: 0.07657 valid_loss: 0.08836 test_loss: 0.09860 \n",
      "[176/200] train_loss: 0.07489 valid_loss: 0.08650 test_loss: 0.09765 \n",
      "[177/200] train_loss: 0.07596 valid_loss: 0.08680 test_loss: 0.09917 \n",
      "[178/200] train_loss: 0.07601 valid_loss: 0.08818 test_loss: 0.09933 \n",
      "[179/200] train_loss: 0.07768 valid_loss: 0.08563 test_loss: 0.09740 \n",
      "Validation loss decreased (0.085865 --> 0.085627).  Saving model ...\n",
      "[180/200] train_loss: 0.07687 valid_loss: 0.08679 test_loss: 0.09859 \n",
      "[181/200] train_loss: 0.07606 valid_loss: 0.08604 test_loss: 0.09803 \n",
      "[182/200] train_loss: 0.07570 valid_loss: 0.08856 test_loss: 0.09746 \n",
      "[183/200] train_loss: 0.07426 valid_loss: 0.08625 test_loss: 0.09740 \n",
      "[184/200] train_loss: 0.07849 valid_loss: 0.08809 test_loss: 0.09864 \n",
      "[185/200] train_loss: 0.07579 valid_loss: 0.08657 test_loss: 0.09818 \n",
      "[186/200] train_loss: 0.07602 valid_loss: 0.08599 test_loss: 0.09709 \n",
      "[187/200] train_loss: 0.07691 valid_loss: 0.08354 test_loss: 0.09653 \n",
      "Validation loss decreased (0.085627 --> 0.083537).  Saving model ...\n",
      "[188/200] train_loss: 0.07642 valid_loss: 0.08352 test_loss: 0.09777 \n",
      "Validation loss decreased (0.083537 --> 0.083520).  Saving model ...\n",
      "[189/200] train_loss: 0.07388 valid_loss: 0.08770 test_loss: 0.09935 \n",
      "[190/200] train_loss: 0.07827 valid_loss: 0.08765 test_loss: 0.09686 \n",
      "[191/200] train_loss: 0.07598 valid_loss: 0.08404 test_loss: 0.09681 \n",
      "[192/200] train_loss: 0.07228 valid_loss: 0.08613 test_loss: 0.09815 \n",
      "[193/200] train_loss: 0.07362 valid_loss: 0.08637 test_loss: 0.09635 \n",
      "[194/200] train_loss: 0.07730 valid_loss: 0.08821 test_loss: 0.09656 \n",
      "[195/200] train_loss: 0.07677 valid_loss: 0.08739 test_loss: 0.09505 \n",
      "[196/200] train_loss: 0.07437 valid_loss: 0.08442 test_loss: 0.09671 \n",
      "[197/200] train_loss: 0.07499 valid_loss: 0.08540 test_loss: 0.09618 \n",
      "[198/200] train_loss: 0.07324 valid_loss: 0.08527 test_loss: 0.09681 \n",
      "[199/200] train_loss: 0.07690 valid_loss: 0.08412 test_loss: 0.09625 \n",
      "[200/200] train_loss: 0.07620 valid_loss: 0.08395 test_loss: 0.09646 \n",
      "Model 17 trained for seen data.\n",
      "TRAINING MODEL for seen house 18\n",
      "[  1/200] train_loss: 0.57996 valid_loss: 0.46291 test_loss: 0.45025 \n",
      "Validation loss decreased (inf --> 0.462912).  Saving model ...\n",
      "[  2/200] train_loss: 0.38191 valid_loss: 0.34579 test_loss: 0.34446 \n",
      "Validation loss decreased (0.462912 --> 0.345786).  Saving model ...\n",
      "[  3/200] train_loss: 0.29843 valid_loss: 0.29401 test_loss: 0.30136 \n",
      "Validation loss decreased (0.345786 --> 0.294014).  Saving model ...\n",
      "[  4/200] train_loss: 0.25705 valid_loss: 0.24870 test_loss: 0.26054 \n",
      "Validation loss decreased (0.294014 --> 0.248700).  Saving model ...\n",
      "[  5/200] train_loss: 0.22141 valid_loss: 0.22048 test_loss: 0.23119 \n",
      "Validation loss decreased (0.248700 --> 0.220485).  Saving model ...\n",
      "[  6/200] train_loss: 0.20040 valid_loss: 0.20305 test_loss: 0.21359 \n",
      "Validation loss decreased (0.220485 --> 0.203054).  Saving model ...\n",
      "[  7/200] train_loss: 0.18806 valid_loss: 0.19099 test_loss: 0.19953 \n",
      "Validation loss decreased (0.203054 --> 0.190994).  Saving model ...\n",
      "[  8/200] train_loss: 0.17703 valid_loss: 0.18136 test_loss: 0.19126 \n",
      "Validation loss decreased (0.190994 --> 0.181357).  Saving model ...\n",
      "[  9/200] train_loss: 0.16607 valid_loss: 0.17293 test_loss: 0.18320 \n",
      "Validation loss decreased (0.181357 --> 0.172929).  Saving model ...\n",
      "[ 10/200] train_loss: 0.16203 valid_loss: 0.17039 test_loss: 0.17853 \n",
      "Validation loss decreased (0.172929 --> 0.170391).  Saving model ...\n",
      "[ 11/200] train_loss: 0.15579 valid_loss: 0.16259 test_loss: 0.17364 \n",
      "Validation loss decreased (0.170391 --> 0.162588).  Saving model ...\n",
      "[ 12/200] train_loss: 0.15376 valid_loss: 0.15794 test_loss: 0.17002 \n",
      "Validation loss decreased (0.162588 --> 0.157943).  Saving model ...\n",
      "[ 13/200] train_loss: 0.15334 valid_loss: 0.15748 test_loss: 0.16896 \n",
      "Validation loss decreased (0.157943 --> 0.157476).  Saving model ...\n",
      "[ 14/200] train_loss: 0.14711 valid_loss: 0.15477 test_loss: 0.16481 \n",
      "Validation loss decreased (0.157476 --> 0.154774).  Saving model ...\n",
      "[ 15/200] train_loss: 0.14404 valid_loss: 0.15446 test_loss: 0.16340 \n",
      "Validation loss decreased (0.154774 --> 0.154464).  Saving model ...\n",
      "[ 16/200] train_loss: 0.14076 valid_loss: 0.14826 test_loss: 0.15930 \n",
      "Validation loss decreased (0.154464 --> 0.148255).  Saving model ...\n",
      "[ 17/200] train_loss: 0.14024 valid_loss: 0.15110 test_loss: 0.16177 \n",
      "[ 18/200] train_loss: 0.13618 valid_loss: 0.14937 test_loss: 0.15762 \n",
      "[ 19/200] train_loss: 0.13284 valid_loss: 0.14450 test_loss: 0.15487 \n",
      "Validation loss decreased (0.148255 --> 0.144499).  Saving model ...\n",
      "[ 20/200] train_loss: 0.13281 valid_loss: 0.14591 test_loss: 0.15363 \n",
      "[ 21/200] train_loss: 0.12832 valid_loss: 0.14371 test_loss: 0.15420 \n",
      "Validation loss decreased (0.144499 --> 0.143710).  Saving model ...\n",
      "[ 22/200] train_loss: 0.12893 valid_loss: 0.14286 test_loss: 0.15117 \n",
      "Validation loss decreased (0.143710 --> 0.142856).  Saving model ...\n",
      "[ 23/200] train_loss: 0.13039 valid_loss: 0.14318 test_loss: 0.15218 \n",
      "[ 24/200] train_loss: 0.12772 valid_loss: 0.13561 test_loss: 0.14689 \n",
      "Validation loss decreased (0.142856 --> 0.135605).  Saving model ...\n",
      "[ 25/200] train_loss: 0.12545 valid_loss: 0.14177 test_loss: 0.15016 \n",
      "[ 26/200] train_loss: 0.12598 valid_loss: 0.13449 test_loss: 0.14652 \n",
      "Validation loss decreased (0.135605 --> 0.134494).  Saving model ...\n",
      "[ 27/200] train_loss: 0.12232 valid_loss: 0.13744 test_loss: 0.14585 \n",
      "[ 28/200] train_loss: 0.12248 valid_loss: 0.13486 test_loss: 0.14682 \n",
      "[ 29/200] train_loss: 0.12373 valid_loss: 0.12960 test_loss: 0.14411 \n",
      "Validation loss decreased (0.134494 --> 0.129599).  Saving model ...\n",
      "[ 30/200] train_loss: 0.11991 valid_loss: 0.13176 test_loss: 0.14516 \n",
      "[ 31/200] train_loss: 0.11794 valid_loss: 0.13367 test_loss: 0.14379 \n",
      "[ 32/200] train_loss: 0.11632 valid_loss: 0.12654 test_loss: 0.14205 \n",
      "Validation loss decreased (0.129599 --> 0.126535).  Saving model ...\n",
      "[ 33/200] train_loss: 0.11516 valid_loss: 0.12700 test_loss: 0.14123 \n",
      "[ 34/200] train_loss: 0.11549 valid_loss: 0.12281 test_loss: 0.13932 \n",
      "Validation loss decreased (0.126535 --> 0.122812).  Saving model ...\n",
      "[ 35/200] train_loss: 0.11479 valid_loss: 0.12922 test_loss: 0.14209 \n",
      "[ 36/200] train_loss: 0.11558 valid_loss: 0.12797 test_loss: 0.13878 \n",
      "[ 37/200] train_loss: 0.11054 valid_loss: 0.12127 test_loss: 0.13667 \n",
      "Validation loss decreased (0.122812 --> 0.121275).  Saving model ...\n",
      "[ 38/200] train_loss: 0.11422 valid_loss: 0.12067 test_loss: 0.13691 \n",
      "Validation loss decreased (0.121275 --> 0.120669).  Saving model ...\n",
      "[ 39/200] train_loss: 0.11395 valid_loss: 0.12376 test_loss: 0.13819 \n",
      "[ 40/200] train_loss: 0.10970 valid_loss: 0.12307 test_loss: 0.13946 \n",
      "[ 41/200] train_loss: 0.11118 valid_loss: 0.11858 test_loss: 0.13407 \n",
      "Validation loss decreased (0.120669 --> 0.118578).  Saving model ...\n",
      "[ 42/200] train_loss: 0.11319 valid_loss: 0.11792 test_loss: 0.13401 \n",
      "Validation loss decreased (0.118578 --> 0.117922).  Saving model ...\n",
      "[ 43/200] train_loss: 0.11172 valid_loss: 0.11838 test_loss: 0.13461 \n",
      "[ 44/200] train_loss: 0.10855 valid_loss: 0.12278 test_loss: 0.13428 \n",
      "[ 45/200] train_loss: 0.11016 valid_loss: 0.11672 test_loss: 0.13140 \n",
      "Validation loss decreased (0.117922 --> 0.116718).  Saving model ...\n",
      "[ 46/200] train_loss: 0.10981 valid_loss: 0.11801 test_loss: 0.13127 \n",
      "[ 47/200] train_loss: 0.10771 valid_loss: 0.11992 test_loss: 0.12980 \n",
      "[ 48/200] train_loss: 0.10718 valid_loss: 0.11624 test_loss: 0.12973 \n",
      "Validation loss decreased (0.116718 --> 0.116241).  Saving model ...\n",
      "[ 49/200] train_loss: 0.10651 valid_loss: 0.11476 test_loss: 0.13061 \n",
      "Validation loss decreased (0.116241 --> 0.114764).  Saving model ...\n",
      "[ 50/200] train_loss: 0.10607 valid_loss: 0.11502 test_loss: 0.12958 \n",
      "[ 51/200] train_loss: 0.10412 valid_loss: 0.11773 test_loss: 0.13057 \n",
      "[ 52/200] train_loss: 0.10294 valid_loss: 0.11873 test_loss: 0.12760 \n",
      "[ 53/200] train_loss: 0.10395 valid_loss: 0.11064 test_loss: 0.12719 \n",
      "Validation loss decreased (0.114764 --> 0.110635).  Saving model ...\n",
      "[ 54/200] train_loss: 0.10424 valid_loss: 0.11198 test_loss: 0.12920 \n",
      "[ 55/200] train_loss: 0.10133 valid_loss: 0.11470 test_loss: 0.12656 \n",
      "[ 56/200] train_loss: 0.10163 valid_loss: 0.11326 test_loss: 0.12459 \n",
      "[ 57/200] train_loss: 0.10267 valid_loss: 0.11453 test_loss: 0.12701 \n",
      "[ 58/200] train_loss: 0.10245 valid_loss: 0.10919 test_loss: 0.12336 \n",
      "Validation loss decreased (0.110635 --> 0.109191).  Saving model ...\n",
      "[ 59/200] train_loss: 0.10004 valid_loss: 0.11142 test_loss: 0.12574 \n",
      "[ 60/200] train_loss: 0.10093 valid_loss: 0.11472 test_loss: 0.12398 \n",
      "[ 61/200] train_loss: 0.10349 valid_loss: 0.11393 test_loss: 0.12374 \n",
      "[ 62/200] train_loss: 0.10268 valid_loss: 0.10815 test_loss: 0.12441 \n",
      "Validation loss decreased (0.109191 --> 0.108151).  Saving model ...\n",
      "[ 63/200] train_loss: 0.09854 valid_loss: 0.11480 test_loss: 0.12477 \n",
      "[ 64/200] train_loss: 0.10127 valid_loss: 0.10961 test_loss: 0.12218 \n",
      "[ 65/200] train_loss: 0.10063 valid_loss: 0.10980 test_loss: 0.12507 \n",
      "[ 66/200] train_loss: 0.09921 valid_loss: 0.10628 test_loss: 0.12111 \n",
      "Validation loss decreased (0.108151 --> 0.106281).  Saving model ...\n",
      "[ 67/200] train_loss: 0.10177 valid_loss: 0.10721 test_loss: 0.12157 \n",
      "[ 68/200] train_loss: 0.10149 valid_loss: 0.10687 test_loss: 0.12282 \n",
      "[ 69/200] train_loss: 0.10027 valid_loss: 0.10840 test_loss: 0.12208 \n",
      "[ 70/200] train_loss: 0.09760 valid_loss: 0.11258 test_loss: 0.12578 \n",
      "[ 71/200] train_loss: 0.09730 valid_loss: 0.10509 test_loss: 0.11818 \n",
      "Validation loss decreased (0.106281 --> 0.105091).  Saving model ...\n",
      "[ 72/200] train_loss: 0.09676 valid_loss: 0.10492 test_loss: 0.11911 \n",
      "Validation loss decreased (0.105091 --> 0.104923).  Saving model ...\n",
      "[ 73/200] train_loss: 0.09549 valid_loss: 0.10970 test_loss: 0.12106 \n",
      "[ 74/200] train_loss: 0.09892 valid_loss: 0.10228 test_loss: 0.11803 \n",
      "Validation loss decreased (0.104923 --> 0.102285).  Saving model ...\n",
      "[ 75/200] train_loss: 0.09973 valid_loss: 0.10273 test_loss: 0.11949 \n",
      "[ 76/200] train_loss: 0.09790 valid_loss: 0.10571 test_loss: 0.12027 \n",
      "[ 77/200] train_loss: 0.09459 valid_loss: 0.10620 test_loss: 0.11737 \n",
      "[ 78/200] train_loss: 0.09790 valid_loss: 0.10183 test_loss: 0.11539 \n",
      "Validation loss decreased (0.102285 --> 0.101827).  Saving model ...\n",
      "[ 79/200] train_loss: 0.09358 valid_loss: 0.10205 test_loss: 0.11576 \n",
      "[ 80/200] train_loss: 0.09386 valid_loss: 0.10389 test_loss: 0.11824 \n",
      "[ 81/200] train_loss: 0.09500 valid_loss: 0.10307 test_loss: 0.11836 \n",
      "[ 82/200] train_loss: 0.09534 valid_loss: 0.10347 test_loss: 0.11863 \n",
      "[ 83/200] train_loss: 0.09417 valid_loss: 0.10256 test_loss: 0.11601 \n",
      "[ 84/200] train_loss: 0.09103 valid_loss: 0.10215 test_loss: 0.11585 \n",
      "[ 85/200] train_loss: 0.09239 valid_loss: 0.10401 test_loss: 0.11595 \n",
      "[ 86/200] train_loss: 0.09329 valid_loss: 0.10479 test_loss: 0.11885 \n",
      "[ 87/200] train_loss: 0.09420 valid_loss: 0.10364 test_loss: 0.11478 \n",
      "[ 88/200] train_loss: 0.09199 valid_loss: 0.09923 test_loss: 0.11380 \n",
      "Validation loss decreased (0.101827 --> 0.099228).  Saving model ...\n",
      "[ 89/200] train_loss: 0.09074 valid_loss: 0.09948 test_loss: 0.11416 \n",
      "[ 90/200] train_loss: 0.09089 valid_loss: 0.10551 test_loss: 0.11467 \n",
      "[ 91/200] train_loss: 0.09066 valid_loss: 0.09922 test_loss: 0.11380 \n",
      "Validation loss decreased (0.099228 --> 0.099224).  Saving model ...\n",
      "[ 92/200] train_loss: 0.09122 valid_loss: 0.09791 test_loss: 0.11245 \n",
      "Validation loss decreased (0.099224 --> 0.097909).  Saving model ...\n",
      "[ 93/200] train_loss: 0.09188 valid_loss: 0.10027 test_loss: 0.11412 \n",
      "[ 94/200] train_loss: 0.08843 valid_loss: 0.09877 test_loss: 0.11338 \n",
      "[ 95/200] train_loss: 0.09066 valid_loss: 0.10298 test_loss: 0.11163 \n",
      "[ 96/200] train_loss: 0.09062 valid_loss: 0.09887 test_loss: 0.11318 \n",
      "[ 97/200] train_loss: 0.09231 valid_loss: 0.10156 test_loss: 0.11165 \n",
      "[ 98/200] train_loss: 0.08970 valid_loss: 0.10274 test_loss: 0.11335 \n",
      "[ 99/200] train_loss: 0.09108 valid_loss: 0.09794 test_loss: 0.11119 \n",
      "[100/200] train_loss: 0.09023 valid_loss: 0.10143 test_loss: 0.11130 \n",
      "[101/200] train_loss: 0.09123 valid_loss: 0.09708 test_loss: 0.11160 \n",
      "Validation loss decreased (0.097909 --> 0.097075).  Saving model ...\n",
      "[102/200] train_loss: 0.08998 valid_loss: 0.09712 test_loss: 0.11090 \n",
      "[103/200] train_loss: 0.09177 valid_loss: 0.09995 test_loss: 0.11190 \n",
      "[104/200] train_loss: 0.08926 valid_loss: 0.09745 test_loss: 0.11074 \n",
      "[105/200] train_loss: 0.08726 valid_loss: 0.09702 test_loss: 0.11216 \n",
      "Validation loss decreased (0.097075 --> 0.097017).  Saving model ...\n",
      "[106/200] train_loss: 0.08945 valid_loss: 0.10037 test_loss: 0.11280 \n",
      "[107/200] train_loss: 0.08639 valid_loss: 0.09772 test_loss: 0.11083 \n",
      "[108/200] train_loss: 0.08808 valid_loss: 0.09546 test_loss: 0.11023 \n",
      "Validation loss decreased (0.097017 --> 0.095462).  Saving model ...\n",
      "[109/200] train_loss: 0.08866 valid_loss: 0.09616 test_loss: 0.10974 \n",
      "[110/200] train_loss: 0.08673 valid_loss: 0.09566 test_loss: 0.10960 \n",
      "[111/200] train_loss: 0.08714 valid_loss: 0.09856 test_loss: 0.11165 \n",
      "[112/200] train_loss: 0.08735 valid_loss: 0.09856 test_loss: 0.10996 \n",
      "[113/200] train_loss: 0.08680 valid_loss: 0.09996 test_loss: 0.10896 \n",
      "[114/200] train_loss: 0.08667 valid_loss: 0.09926 test_loss: 0.10942 \n",
      "[115/200] train_loss: 0.08835 valid_loss: 0.10009 test_loss: 0.10921 \n",
      "[116/200] train_loss: 0.08622 valid_loss: 0.09851 test_loss: 0.10889 \n",
      "[117/200] train_loss: 0.08726 valid_loss: 0.09600 test_loss: 0.10865 \n",
      "[118/200] train_loss: 0.08670 valid_loss: 0.09623 test_loss: 0.10887 \n",
      "[119/200] train_loss: 0.08512 valid_loss: 0.09726 test_loss: 0.10634 \n",
      "[120/200] train_loss: 0.08629 valid_loss: 0.09605 test_loss: 0.10786 \n",
      "[121/200] train_loss: 0.08745 valid_loss: 0.09798 test_loss: 0.10716 \n",
      "[122/200] train_loss: 0.08611 valid_loss: 0.09557 test_loss: 0.10701 \n",
      "[123/200] train_loss: 0.08621 valid_loss: 0.09998 test_loss: 0.10726 \n",
      "[124/200] train_loss: 0.08786 valid_loss: 0.09824 test_loss: 0.10871 \n",
      "[125/200] train_loss: 0.08331 valid_loss: 0.09732 test_loss: 0.10954 \n",
      "[126/200] train_loss: 0.08329 valid_loss: 0.09740 test_loss: 0.10693 \n",
      "[127/200] train_loss: 0.08752 valid_loss: 0.09722 test_loss: 0.10622 \n",
      "[128/200] train_loss: 0.08534 valid_loss: 0.09489 test_loss: 0.10618 \n",
      "Validation loss decreased (0.095462 --> 0.094885).  Saving model ...\n",
      "[129/200] train_loss: 0.08649 valid_loss: 0.09291 test_loss: 0.10792 \n",
      "Validation loss decreased (0.094885 --> 0.092907).  Saving model ...\n",
      "[130/200] train_loss: 0.08730 valid_loss: 0.09386 test_loss: 0.10665 \n",
      "[131/200] train_loss: 0.08523 valid_loss: 0.09512 test_loss: 0.10751 \n",
      "[132/200] train_loss: 0.08455 valid_loss: 0.09281 test_loss: 0.10673 \n",
      "Validation loss decreased (0.092907 --> 0.092807).  Saving model ...\n",
      "[133/200] train_loss: 0.08512 valid_loss: 0.09712 test_loss: 0.10600 \n",
      "[134/200] train_loss: 0.08211 valid_loss: 0.09259 test_loss: 0.10507 \n",
      "Validation loss decreased (0.092807 --> 0.092591).  Saving model ...\n",
      "[135/200] train_loss: 0.08380 valid_loss: 0.09060 test_loss: 0.10438 \n",
      "Validation loss decreased (0.092591 --> 0.090596).  Saving model ...\n",
      "[136/200] train_loss: 0.08236 valid_loss: 0.09257 test_loss: 0.10613 \n",
      "[137/200] train_loss: 0.08130 valid_loss: 0.09256 test_loss: 0.10560 \n",
      "[138/200] train_loss: 0.08368 valid_loss: 0.09372 test_loss: 0.10656 \n",
      "[139/200] train_loss: 0.08184 valid_loss: 0.09518 test_loss: 0.10527 \n",
      "[140/200] train_loss: 0.08471 valid_loss: 0.09195 test_loss: 0.10486 \n",
      "[141/200] train_loss: 0.08070 valid_loss: 0.09369 test_loss: 0.10593 \n",
      "[142/200] train_loss: 0.08256 valid_loss: 0.09188 test_loss: 0.10594 \n",
      "[143/200] train_loss: 0.08169 valid_loss: 0.09302 test_loss: 0.10364 \n",
      "[144/200] train_loss: 0.08239 valid_loss: 0.09615 test_loss: 0.10581 \n",
      "[145/200] train_loss: 0.08046 valid_loss: 0.09503 test_loss: 0.10601 \n",
      "[146/200] train_loss: 0.08078 valid_loss: 0.09349 test_loss: 0.10334 \n",
      "[147/200] train_loss: 0.08072 valid_loss: 0.09536 test_loss: 0.10447 \n",
      "[148/200] train_loss: 0.08241 valid_loss: 0.09013 test_loss: 0.10283 \n",
      "Validation loss decreased (0.090596 --> 0.090128).  Saving model ...\n",
      "[149/200] train_loss: 0.08145 valid_loss: 0.09708 test_loss: 0.10575 \n",
      "[150/200] train_loss: 0.08099 valid_loss: 0.09302 test_loss: 0.10337 \n",
      "[151/200] train_loss: 0.08096 valid_loss: 0.09245 test_loss: 0.10367 \n",
      "[152/200] train_loss: 0.07901 valid_loss: 0.09275 test_loss: 0.10295 \n",
      "[153/200] train_loss: 0.08025 valid_loss: 0.09136 test_loss: 0.10281 \n",
      "[154/200] train_loss: 0.07835 valid_loss: 0.09249 test_loss: 0.10174 \n",
      "[155/200] train_loss: 0.08259 valid_loss: 0.09085 test_loss: 0.10266 \n",
      "[156/200] train_loss: 0.08238 valid_loss: 0.09085 test_loss: 0.10237 \n",
      "[157/200] train_loss: 0.08018 valid_loss: 0.08804 test_loss: 0.10178 \n",
      "Validation loss decreased (0.090128 --> 0.088039).  Saving model ...\n",
      "[158/200] train_loss: 0.08134 valid_loss: 0.09053 test_loss: 0.10250 \n",
      "[159/200] train_loss: 0.08017 valid_loss: 0.08964 test_loss: 0.10204 \n",
      "[160/200] train_loss: 0.08113 valid_loss: 0.08984 test_loss: 0.10264 \n",
      "[161/200] train_loss: 0.07990 valid_loss: 0.08816 test_loss: 0.10162 \n",
      "[162/200] train_loss: 0.08080 valid_loss: 0.08914 test_loss: 0.10291 \n",
      "[163/200] train_loss: 0.08154 valid_loss: 0.08900 test_loss: 0.10238 \n",
      "[164/200] train_loss: 0.07936 valid_loss: 0.08864 test_loss: 0.10231 \n",
      "[165/200] train_loss: 0.07746 valid_loss: 0.08862 test_loss: 0.10230 \n",
      "[166/200] train_loss: 0.08034 valid_loss: 0.08780 test_loss: 0.10209 \n",
      "Validation loss decreased (0.088039 --> 0.087799).  Saving model ...\n",
      "[167/200] train_loss: 0.08190 valid_loss: 0.09194 test_loss: 0.10094 \n",
      "[168/200] train_loss: 0.07780 valid_loss: 0.08968 test_loss: 0.10254 \n",
      "[169/200] train_loss: 0.07913 valid_loss: 0.09937 test_loss: 0.10217 \n",
      "[170/200] train_loss: 0.08026 valid_loss: 0.09054 test_loss: 0.10106 \n",
      "[171/200] train_loss: 0.07943 valid_loss: 0.08910 test_loss: 0.10161 \n",
      "[172/200] train_loss: 0.07873 valid_loss: 0.09069 test_loss: 0.10146 \n",
      "[173/200] train_loss: 0.07848 valid_loss: 0.08975 test_loss: 0.10022 \n",
      "[174/200] train_loss: 0.08059 valid_loss: 0.08750 test_loss: 0.09964 \n",
      "Validation loss decreased (0.087799 --> 0.087495).  Saving model ...\n",
      "[175/200] train_loss: 0.07970 valid_loss: 0.08669 test_loss: 0.09941 \n",
      "Validation loss decreased (0.087495 --> 0.086690).  Saving model ...\n",
      "[176/200] train_loss: 0.07661 valid_loss: 0.08698 test_loss: 0.10125 \n",
      "[177/200] train_loss: 0.07926 valid_loss: 0.08845 test_loss: 0.10112 \n",
      "[178/200] train_loss: 0.07748 valid_loss: 0.08749 test_loss: 0.09969 \n",
      "[179/200] train_loss: 0.07772 valid_loss: 0.09012 test_loss: 0.10191 \n",
      "[180/200] train_loss: 0.07731 valid_loss: 0.08868 test_loss: 0.10048 \n",
      "[181/200] train_loss: 0.07831 valid_loss: 0.08674 test_loss: 0.09988 \n",
      "[182/200] train_loss: 0.07772 valid_loss: 0.08828 test_loss: 0.09996 \n",
      "[183/200] train_loss: 0.08034 valid_loss: 0.08831 test_loss: 0.10155 \n",
      "[184/200] train_loss: 0.07889 valid_loss: 0.08816 test_loss: 0.10097 \n",
      "[185/200] train_loss: 0.07635 valid_loss: 0.08702 test_loss: 0.10006 \n",
      "[186/200] train_loss: 0.07732 valid_loss: 0.08661 test_loss: 0.09964 \n",
      "Validation loss decreased (0.086690 --> 0.086606).  Saving model ...\n",
      "[187/200] train_loss: 0.07925 valid_loss: 0.09381 test_loss: 0.10193 \n",
      "[188/200] train_loss: 0.07788 valid_loss: 0.08618 test_loss: 0.09941 \n",
      "Validation loss decreased (0.086606 --> 0.086176).  Saving model ...\n",
      "[189/200] train_loss: 0.07763 valid_loss: 0.08538 test_loss: 0.09898 \n",
      "Validation loss decreased (0.086176 --> 0.085382).  Saving model ...\n",
      "[190/200] train_loss: 0.07910 valid_loss: 0.09221 test_loss: 0.09836 \n",
      "[191/200] train_loss: 0.07959 valid_loss: 0.08958 test_loss: 0.10111 \n",
      "[192/200] train_loss: 0.07785 valid_loss: 0.08773 test_loss: 0.09939 \n",
      "[193/200] train_loss: 0.07610 valid_loss: 0.08692 test_loss: 0.09898 \n",
      "[194/200] train_loss: 0.07432 valid_loss: 0.08702 test_loss: 0.09923 \n",
      "[195/200] train_loss: 0.07556 valid_loss: 0.08940 test_loss: 0.09867 \n",
      "[196/200] train_loss: 0.07768 valid_loss: 0.09389 test_loss: 0.10066 \n",
      "[197/200] train_loss: 0.07649 valid_loss: 0.08673 test_loss: 0.09853 \n",
      "[198/200] train_loss: 0.07644 valid_loss: 0.09075 test_loss: 0.09934 \n",
      "[199/200] train_loss: 0.07556 valid_loss: 0.08672 test_loss: 0.09872 \n",
      "[200/200] train_loss: 0.07679 valid_loss: 0.08635 test_loss: 0.09801 \n",
      "Model 18 trained for seen data.\n",
      "TRAINING MODEL for seen house 19\n",
      "[  1/200] train_loss: 0.51054 valid_loss: 0.40183 test_loss: 0.40591 \n",
      "Validation loss decreased (inf --> 0.401829).  Saving model ...\n",
      "[  2/200] train_loss: 0.32966 valid_loss: 0.30672 test_loss: 0.31941 \n",
      "Validation loss decreased (0.401829 --> 0.306717).  Saving model ...\n",
      "[  3/200] train_loss: 0.26449 valid_loss: 0.25889 test_loss: 0.27666 \n",
      "Validation loss decreased (0.306717 --> 0.258887).  Saving model ...\n",
      "[  4/200] train_loss: 0.22869 valid_loss: 0.22461 test_loss: 0.24215 \n",
      "Validation loss decreased (0.258887 --> 0.224610).  Saving model ...\n",
      "[  5/200] train_loss: 0.20109 valid_loss: 0.20167 test_loss: 0.21726 \n",
      "Validation loss decreased (0.224610 --> 0.201670).  Saving model ...\n",
      "[  6/200] train_loss: 0.18389 valid_loss: 0.18558 test_loss: 0.19797 \n",
      "Validation loss decreased (0.201670 --> 0.185578).  Saving model ...\n",
      "[  7/200] train_loss: 0.17354 valid_loss: 0.17519 test_loss: 0.18747 \n",
      "Validation loss decreased (0.185578 --> 0.175187).  Saving model ...\n",
      "[  8/200] train_loss: 0.16521 valid_loss: 0.16842 test_loss: 0.17955 \n",
      "Validation loss decreased (0.175187 --> 0.168416).  Saving model ...\n",
      "[  9/200] train_loss: 0.15785 valid_loss: 0.16348 test_loss: 0.17437 \n",
      "Validation loss decreased (0.168416 --> 0.163481).  Saving model ...\n",
      "[ 10/200] train_loss: 0.15848 valid_loss: 0.15995 test_loss: 0.17304 \n",
      "Validation loss decreased (0.163481 --> 0.159951).  Saving model ...\n",
      "[ 11/200] train_loss: 0.14748 valid_loss: 0.15575 test_loss: 0.16642 \n",
      "Validation loss decreased (0.159951 --> 0.155746).  Saving model ...\n",
      "[ 12/200] train_loss: 0.14679 valid_loss: 0.15178 test_loss: 0.16292 \n",
      "Validation loss decreased (0.155746 --> 0.151783).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14384 valid_loss: 0.15158 test_loss: 0.16085 \n",
      "Validation loss decreased (0.151783 --> 0.151584).  Saving model ...\n",
      "[ 14/200] train_loss: 0.14274 valid_loss: 0.14906 test_loss: 0.15937 \n",
      "Validation loss decreased (0.151584 --> 0.149064).  Saving model ...\n",
      "[ 15/200] train_loss: 0.13630 valid_loss: 0.14478 test_loss: 0.15444 \n",
      "Validation loss decreased (0.149064 --> 0.144776).  Saving model ...\n",
      "[ 16/200] train_loss: 0.13761 valid_loss: 0.14722 test_loss: 0.15416 \n",
      "[ 17/200] train_loss: 0.13354 valid_loss: 0.13966 test_loss: 0.15217 \n",
      "Validation loss decreased (0.144776 --> 0.139656).  Saving model ...\n",
      "[ 18/200] train_loss: 0.13269 valid_loss: 0.14311 test_loss: 0.15217 \n",
      "[ 19/200] train_loss: 0.13165 valid_loss: 0.13997 test_loss: 0.14935 \n",
      "[ 20/200] train_loss: 0.12888 valid_loss: 0.13784 test_loss: 0.14829 \n",
      "Validation loss decreased (0.139656 --> 0.137845).  Saving model ...\n",
      "[ 21/200] train_loss: 0.12801 valid_loss: 0.13104 test_loss: 0.14536 \n",
      "Validation loss decreased (0.137845 --> 0.131037).  Saving model ...\n",
      "[ 22/200] train_loss: 0.12552 valid_loss: 0.13506 test_loss: 0.14759 \n",
      "[ 23/200] train_loss: 0.12256 valid_loss: 0.12990 test_loss: 0.14302 \n",
      "Validation loss decreased (0.131037 --> 0.129903).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12324 valid_loss: 0.13203 test_loss: 0.14348 \n",
      "[ 25/200] train_loss: 0.11962 valid_loss: 0.12959 test_loss: 0.14228 \n",
      "Validation loss decreased (0.129903 --> 0.129586).  Saving model ...\n",
      "[ 26/200] train_loss: 0.12064 valid_loss: 0.13260 test_loss: 0.14289 \n",
      "[ 27/200] train_loss: 0.12237 valid_loss: 0.12944 test_loss: 0.13993 \n",
      "Validation loss decreased (0.129586 --> 0.129438).  Saving model ...\n",
      "[ 28/200] train_loss: 0.12165 valid_loss: 0.12888 test_loss: 0.14103 \n",
      "Validation loss decreased (0.129438 --> 0.128882).  Saving model ...\n",
      "[ 29/200] train_loss: 0.12066 valid_loss: 0.12742 test_loss: 0.14030 \n",
      "Validation loss decreased (0.128882 --> 0.127417).  Saving model ...\n",
      "[ 30/200] train_loss: 0.11540 valid_loss: 0.12381 test_loss: 0.13739 \n",
      "Validation loss decreased (0.127417 --> 0.123810).  Saving model ...\n",
      "[ 31/200] train_loss: 0.11673 valid_loss: 0.12647 test_loss: 0.13968 \n",
      "[ 32/200] train_loss: 0.11334 valid_loss: 0.12427 test_loss: 0.13691 \n",
      "[ 33/200] train_loss: 0.11884 valid_loss: 0.12571 test_loss: 0.13792 \n",
      "[ 34/200] train_loss: 0.11775 valid_loss: 0.12191 test_loss: 0.13540 \n",
      "Validation loss decreased (0.123810 --> 0.121909).  Saving model ...\n",
      "[ 35/200] train_loss: 0.11237 valid_loss: 0.12203 test_loss: 0.13420 \n",
      "[ 36/200] train_loss: 0.11045 valid_loss: 0.12073 test_loss: 0.13372 \n",
      "Validation loss decreased (0.121909 --> 0.120727).  Saving model ...\n",
      "[ 37/200] train_loss: 0.11205 valid_loss: 0.12423 test_loss: 0.13402 \n",
      "[ 38/200] train_loss: 0.11292 valid_loss: 0.12034 test_loss: 0.13425 \n",
      "Validation loss decreased (0.120727 --> 0.120336).  Saving model ...\n",
      "[ 39/200] train_loss: 0.10855 valid_loss: 0.11966 test_loss: 0.13359 \n",
      "Validation loss decreased (0.120336 --> 0.119659).  Saving model ...\n",
      "[ 40/200] train_loss: 0.11032 valid_loss: 0.12486 test_loss: 0.13292 \n",
      "[ 41/200] train_loss: 0.10925 valid_loss: 0.11978 test_loss: 0.13335 \n",
      "[ 42/200] train_loss: 0.10607 valid_loss: 0.11739 test_loss: 0.13170 \n",
      "Validation loss decreased (0.119659 --> 0.117393).  Saving model ...\n",
      "[ 43/200] train_loss: 0.10716 valid_loss: 0.12135 test_loss: 0.13105 \n",
      "[ 44/200] train_loss: 0.10703 valid_loss: 0.11422 test_loss: 0.12975 \n",
      "Validation loss decreased (0.117393 --> 0.114220).  Saving model ...\n",
      "[ 45/200] train_loss: 0.10747 valid_loss: 0.11667 test_loss: 0.12858 \n",
      "[ 46/200] train_loss: 0.10893 valid_loss: 0.11913 test_loss: 0.13299 \n",
      "[ 47/200] train_loss: 0.10861 valid_loss: 0.11817 test_loss: 0.13053 \n",
      "[ 48/200] train_loss: 0.10571 valid_loss: 0.11504 test_loss: 0.12864 \n",
      "[ 49/200] train_loss: 0.10949 valid_loss: 0.11696 test_loss: 0.13250 \n",
      "[ 50/200] train_loss: 0.10535 valid_loss: 0.11382 test_loss: 0.12911 \n",
      "Validation loss decreased (0.114220 --> 0.113824).  Saving model ...\n",
      "[ 51/200] train_loss: 0.10502 valid_loss: 0.11780 test_loss: 0.13202 \n",
      "[ 52/200] train_loss: 0.10515 valid_loss: 0.11763 test_loss: 0.12937 \n",
      "[ 53/200] train_loss: 0.10451 valid_loss: 0.11267 test_loss: 0.12663 \n",
      "Validation loss decreased (0.113824 --> 0.112670).  Saving model ...\n",
      "[ 54/200] train_loss: 0.10306 valid_loss: 0.11517 test_loss: 0.12698 \n",
      "[ 55/200] train_loss: 0.10074 valid_loss: 0.11480 test_loss: 0.12525 \n",
      "[ 56/200] train_loss: 0.10296 valid_loss: 0.11504 test_loss: 0.12597 \n",
      "[ 57/200] train_loss: 0.10378 valid_loss: 0.11106 test_loss: 0.12676 \n",
      "Validation loss decreased (0.112670 --> 0.111060).  Saving model ...\n",
      "[ 58/200] train_loss: 0.10238 valid_loss: 0.11287 test_loss: 0.12655 \n",
      "[ 59/200] train_loss: 0.10182 valid_loss: 0.10652 test_loss: 0.12144 \n",
      "Validation loss decreased (0.111060 --> 0.106518).  Saving model ...\n",
      "[ 60/200] train_loss: 0.10099 valid_loss: 0.11181 test_loss: 0.12356 \n",
      "[ 61/200] train_loss: 0.10189 valid_loss: 0.10930 test_loss: 0.12402 \n",
      "[ 62/200] train_loss: 0.10270 valid_loss: 0.11278 test_loss: 0.12529 \n",
      "[ 63/200] train_loss: 0.09985 valid_loss: 0.11374 test_loss: 0.12259 \n",
      "[ 64/200] train_loss: 0.09743 valid_loss: 0.11339 test_loss: 0.12209 \n",
      "[ 65/200] train_loss: 0.10031 valid_loss: 0.11051 test_loss: 0.12029 \n",
      "[ 66/200] train_loss: 0.09866 valid_loss: 0.11127 test_loss: 0.12084 \n",
      "[ 67/200] train_loss: 0.09902 valid_loss: 0.10944 test_loss: 0.12144 \n",
      "[ 68/200] train_loss: 0.09922 valid_loss: 0.10983 test_loss: 0.12212 \n",
      "[ 69/200] train_loss: 0.09658 valid_loss: 0.10847 test_loss: 0.12233 \n",
      "[ 70/200] train_loss: 0.09609 valid_loss: 0.10672 test_loss: 0.12034 \n",
      "[ 71/200] train_loss: 0.09648 valid_loss: 0.10601 test_loss: 0.12044 \n",
      "Validation loss decreased (0.106518 --> 0.106014).  Saving model ...\n",
      "[ 72/200] train_loss: 0.09685 valid_loss: 0.10646 test_loss: 0.11963 \n",
      "[ 73/200] train_loss: 0.09691 valid_loss: 0.10601 test_loss: 0.12087 \n",
      "Validation loss decreased (0.106014 --> 0.106005).  Saving model ...\n",
      "[ 74/200] train_loss: 0.09801 valid_loss: 0.10365 test_loss: 0.11919 \n",
      "Validation loss decreased (0.106005 --> 0.103654).  Saving model ...\n",
      "[ 75/200] train_loss: 0.09740 valid_loss: 0.10401 test_loss: 0.11887 \n",
      "[ 76/200] train_loss: 0.09516 valid_loss: 0.10168 test_loss: 0.11759 \n",
      "Validation loss decreased (0.103654 --> 0.101677).  Saving model ...\n",
      "[ 77/200] train_loss: 0.09629 valid_loss: 0.10431 test_loss: 0.11822 \n",
      "[ 78/200] train_loss: 0.09535 valid_loss: 0.10376 test_loss: 0.11737 \n",
      "[ 79/200] train_loss: 0.09526 valid_loss: 0.10495 test_loss: 0.11719 \n",
      "[ 80/200] train_loss: 0.09647 valid_loss: 0.10440 test_loss: 0.11626 \n",
      "[ 81/200] train_loss: 0.09353 valid_loss: 0.10430 test_loss: 0.11731 \n",
      "[ 82/200] train_loss: 0.09362 valid_loss: 0.10220 test_loss: 0.11549 \n",
      "[ 83/200] train_loss: 0.09453 valid_loss: 0.10526 test_loss: 0.11607 \n",
      "[ 84/200] train_loss: 0.09180 valid_loss: 0.10453 test_loss: 0.11552 \n",
      "[ 85/200] train_loss: 0.09412 valid_loss: 0.10218 test_loss: 0.11704 \n",
      "[ 86/200] train_loss: 0.09220 valid_loss: 0.10869 test_loss: 0.11671 \n",
      "[ 87/200] train_loss: 0.09135 valid_loss: 0.10317 test_loss: 0.11534 \n",
      "[ 88/200] train_loss: 0.09482 valid_loss: 0.10596 test_loss: 0.11390 \n",
      "[ 89/200] train_loss: 0.09573 valid_loss: 0.10433 test_loss: 0.11611 \n",
      "[ 90/200] train_loss: 0.09544 valid_loss: 0.10684 test_loss: 0.11558 \n",
      "[ 91/200] train_loss: 0.09068 valid_loss: 0.10469 test_loss: 0.11372 \n",
      "[ 92/200] train_loss: 0.09076 valid_loss: 0.10416 test_loss: 0.11350 \n",
      "[ 93/200] train_loss: 0.09435 valid_loss: 0.10421 test_loss: 0.11438 \n",
      "[ 94/200] train_loss: 0.09059 valid_loss: 0.10557 test_loss: 0.11489 \n",
      "[ 95/200] train_loss: 0.09117 valid_loss: 0.10426 test_loss: 0.11318 \n",
      "[ 96/200] train_loss: 0.09279 valid_loss: 0.10487 test_loss: 0.11443 \n",
      "[ 97/200] train_loss: 0.08995 valid_loss: 0.09802 test_loss: 0.11212 \n",
      "Validation loss decreased (0.101677 --> 0.098022).  Saving model ...\n",
      "[ 98/200] train_loss: 0.09137 valid_loss: 0.10594 test_loss: 0.11241 \n",
      "[ 99/200] train_loss: 0.09074 valid_loss: 0.09977 test_loss: 0.11208 \n",
      "[100/200] train_loss: 0.08852 valid_loss: 0.10099 test_loss: 0.11217 \n",
      "[101/200] train_loss: 0.08861 valid_loss: 0.10179 test_loss: 0.11306 \n",
      "[102/200] train_loss: 0.09302 valid_loss: 0.09929 test_loss: 0.11145 \n",
      "[103/200] train_loss: 0.08846 valid_loss: 0.10307 test_loss: 0.11356 \n",
      "[104/200] train_loss: 0.08980 valid_loss: 0.09915 test_loss: 0.11134 \n",
      "[105/200] train_loss: 0.08834 valid_loss: 0.09853 test_loss: 0.11125 \n",
      "[106/200] train_loss: 0.08959 valid_loss: 0.10242 test_loss: 0.11161 \n",
      "[107/200] train_loss: 0.09031 valid_loss: 0.10363 test_loss: 0.11184 \n",
      "[108/200] train_loss: 0.08973 valid_loss: 0.10114 test_loss: 0.10995 \n",
      "[109/200] train_loss: 0.08678 valid_loss: 0.10286 test_loss: 0.11219 \n",
      "[110/200] train_loss: 0.08794 valid_loss: 0.09911 test_loss: 0.11162 \n",
      "[111/200] train_loss: 0.08820 valid_loss: 0.09908 test_loss: 0.11075 \n",
      "[112/200] train_loss: 0.08877 valid_loss: 0.10020 test_loss: 0.10989 \n",
      "[113/200] train_loss: 0.08864 valid_loss: 0.10233 test_loss: 0.11021 \n",
      "[114/200] train_loss: 0.08741 valid_loss: 0.09912 test_loss: 0.10981 \n",
      "[115/200] train_loss: 0.08573 valid_loss: 0.09758 test_loss: 0.10992 \n",
      "Validation loss decreased (0.098022 --> 0.097579).  Saving model ...\n",
      "[116/200] train_loss: 0.08425 valid_loss: 0.09719 test_loss: 0.10766 \n",
      "Validation loss decreased (0.097579 --> 0.097188).  Saving model ...\n",
      "[117/200] train_loss: 0.08578 valid_loss: 0.09568 test_loss: 0.10611 \n",
      "Validation loss decreased (0.097188 --> 0.095682).  Saving model ...\n",
      "[118/200] train_loss: 0.08634 valid_loss: 0.09741 test_loss: 0.10783 \n",
      "[119/200] train_loss: 0.08490 valid_loss: 0.09711 test_loss: 0.10778 \n",
      "[120/200] train_loss: 0.08767 valid_loss: 0.09779 test_loss: 0.10990 \n",
      "[121/200] train_loss: 0.08530 valid_loss: 0.09789 test_loss: 0.10930 \n",
      "[122/200] train_loss: 0.08662 valid_loss: 0.10012 test_loss: 0.10882 \n",
      "[123/200] train_loss: 0.08589 valid_loss: 0.09575 test_loss: 0.10792 \n",
      "[124/200] train_loss: 0.08558 valid_loss: 0.09704 test_loss: 0.10665 \n",
      "[125/200] train_loss: 0.08491 valid_loss: 0.09632 test_loss: 0.10737 \n",
      "[126/200] train_loss: 0.08217 valid_loss: 0.09508 test_loss: 0.10801 \n",
      "Validation loss decreased (0.095682 --> 0.095083).  Saving model ...\n",
      "[127/200] train_loss: 0.08265 valid_loss: 0.09424 test_loss: 0.10699 \n",
      "Validation loss decreased (0.095083 --> 0.094245).  Saving model ...\n",
      "[128/200] train_loss: 0.08636 valid_loss: 0.09522 test_loss: 0.10758 \n",
      "[129/200] train_loss: 0.08386 valid_loss: 0.09551 test_loss: 0.10566 \n",
      "[130/200] train_loss: 0.08287 valid_loss: 0.09650 test_loss: 0.10688 \n",
      "[131/200] train_loss: 0.08505 valid_loss: 0.09425 test_loss: 0.10575 \n",
      "[132/200] train_loss: 0.08507 valid_loss: 0.09519 test_loss: 0.10496 \n",
      "[133/200] train_loss: 0.08237 valid_loss: 0.09591 test_loss: 0.10727 \n",
      "[134/200] train_loss: 0.08233 valid_loss: 0.09460 test_loss: 0.10492 \n",
      "[135/200] train_loss: 0.08416 valid_loss: 0.09550 test_loss: 0.10523 \n",
      "[136/200] train_loss: 0.08420 valid_loss: 0.09408 test_loss: 0.10546 \n",
      "Validation loss decreased (0.094245 --> 0.094081).  Saving model ...\n",
      "[137/200] train_loss: 0.08493 valid_loss: 0.09269 test_loss: 0.10466 \n",
      "Validation loss decreased (0.094081 --> 0.092695).  Saving model ...\n",
      "[138/200] train_loss: 0.08166 valid_loss: 0.09784 test_loss: 0.10811 \n",
      "[139/200] train_loss: 0.08542 valid_loss: 0.09665 test_loss: 0.10414 \n",
      "[140/200] train_loss: 0.08116 valid_loss: 0.10045 test_loss: 0.10572 \n",
      "[141/200] train_loss: 0.08409 valid_loss: 0.09221 test_loss: 0.10449 \n",
      "Validation loss decreased (0.092695 --> 0.092207).  Saving model ...\n",
      "[142/200] train_loss: 0.08402 valid_loss: 0.09148 test_loss: 0.10335 \n",
      "Validation loss decreased (0.092207 --> 0.091476).  Saving model ...\n",
      "[143/200] train_loss: 0.08408 valid_loss: 0.09467 test_loss: 0.10355 \n",
      "[144/200] train_loss: 0.08479 valid_loss: 0.09737 test_loss: 0.10500 \n",
      "[145/200] train_loss: 0.08322 valid_loss: 0.09710 test_loss: 0.10525 \n",
      "[146/200] train_loss: 0.08494 valid_loss: 0.09207 test_loss: 0.10384 \n",
      "[147/200] train_loss: 0.08095 valid_loss: 0.09259 test_loss: 0.10400 \n",
      "[148/200] train_loss: 0.08201 valid_loss: 0.09232 test_loss: 0.10373 \n",
      "[149/200] train_loss: 0.08081 valid_loss: 0.09250 test_loss: 0.10448 \n",
      "[150/200] train_loss: 0.08258 valid_loss: 0.09393 test_loss: 0.10603 \n",
      "[151/200] train_loss: 0.08100 valid_loss: 0.09151 test_loss: 0.10340 \n",
      "[152/200] train_loss: 0.08168 valid_loss: 0.09040 test_loss: 0.10331 \n",
      "Validation loss decreased (0.091476 --> 0.090399).  Saving model ...\n",
      "[153/200] train_loss: 0.08129 valid_loss: 0.09865 test_loss: 0.10483 \n",
      "[154/200] train_loss: 0.08048 valid_loss: 0.09045 test_loss: 0.10308 \n",
      "[155/200] train_loss: 0.08010 valid_loss: 0.09184 test_loss: 0.10360 \n",
      "[156/200] train_loss: 0.07914 valid_loss: 0.09422 test_loss: 0.10437 \n",
      "[157/200] train_loss: 0.08288 valid_loss: 0.09107 test_loss: 0.10321 \n",
      "[158/200] train_loss: 0.08078 valid_loss: 0.09017 test_loss: 0.10105 \n",
      "Validation loss decreased (0.090399 --> 0.090174).  Saving model ...\n",
      "[159/200] train_loss: 0.08170 valid_loss: 0.09083 test_loss: 0.10200 \n",
      "[160/200] train_loss: 0.07810 valid_loss: 0.09016 test_loss: 0.10128 \n",
      "Validation loss decreased (0.090174 --> 0.090163).  Saving model ...\n",
      "[161/200] train_loss: 0.08150 valid_loss: 0.09591 test_loss: 0.10156 \n",
      "[162/200] train_loss: 0.08207 valid_loss: 0.09169 test_loss: 0.10065 \n",
      "[163/200] train_loss: 0.08060 valid_loss: 0.09283 test_loss: 0.10257 \n",
      "[164/200] train_loss: 0.07876 valid_loss: 0.09391 test_loss: 0.10218 \n",
      "[165/200] train_loss: 0.08119 valid_loss: 0.09146 test_loss: 0.10206 \n",
      "[166/200] train_loss: 0.08040 valid_loss: 0.09250 test_loss: 0.10311 \n",
      "[167/200] train_loss: 0.08137 valid_loss: 0.09220 test_loss: 0.10246 \n",
      "[168/200] train_loss: 0.08131 valid_loss: 0.08974 test_loss: 0.10142 \n",
      "Validation loss decreased (0.090163 --> 0.089738).  Saving model ...\n",
      "[169/200] train_loss: 0.07956 valid_loss: 0.09060 test_loss: 0.10161 \n",
      "[170/200] train_loss: 0.08109 valid_loss: 0.08872 test_loss: 0.10072 \n",
      "Validation loss decreased (0.089738 --> 0.088717).  Saving model ...\n",
      "[171/200] train_loss: 0.07876 valid_loss: 0.08951 test_loss: 0.10128 \n",
      "[172/200] train_loss: 0.07931 valid_loss: 0.09223 test_loss: 0.10261 \n",
      "[173/200] train_loss: 0.07905 valid_loss: 0.08981 test_loss: 0.10028 \n",
      "[174/200] train_loss: 0.07857 valid_loss: 0.08934 test_loss: 0.09904 \n",
      "[175/200] train_loss: 0.07874 valid_loss: 0.08840 test_loss: 0.10123 \n",
      "Validation loss decreased (0.088717 --> 0.088401).  Saving model ...\n",
      "[176/200] train_loss: 0.07689 valid_loss: 0.09160 test_loss: 0.10271 \n",
      "[177/200] train_loss: 0.07889 valid_loss: 0.09024 test_loss: 0.10044 \n",
      "[178/200] train_loss: 0.07810 valid_loss: 0.09252 test_loss: 0.10067 \n",
      "[179/200] train_loss: 0.08039 valid_loss: 0.09089 test_loss: 0.10072 \n",
      "[180/200] train_loss: 0.07673 valid_loss: 0.08800 test_loss: 0.09955 \n",
      "Validation loss decreased (0.088401 --> 0.088003).  Saving model ...\n",
      "[181/200] train_loss: 0.07628 valid_loss: 0.08734 test_loss: 0.09911 \n",
      "Validation loss decreased (0.088003 --> 0.087341).  Saving model ...\n",
      "[182/200] train_loss: 0.07772 valid_loss: 0.08982 test_loss: 0.10101 \n",
      "[183/200] train_loss: 0.07878 valid_loss: 0.09305 test_loss: 0.10065 \n",
      "[184/200] train_loss: 0.07535 valid_loss: 0.08730 test_loss: 0.09944 \n",
      "Validation loss decreased (0.087341 --> 0.087297).  Saving model ...\n",
      "[185/200] train_loss: 0.07904 valid_loss: 0.08799 test_loss: 0.09993 \n",
      "[186/200] train_loss: 0.07714 valid_loss: 0.08904 test_loss: 0.10032 \n",
      "[187/200] train_loss: 0.07647 valid_loss: 0.08764 test_loss: 0.09890 \n",
      "[188/200] train_loss: 0.07726 valid_loss: 0.08919 test_loss: 0.09974 \n",
      "[189/200] train_loss: 0.07600 valid_loss: 0.08829 test_loss: 0.09917 \n",
      "[190/200] train_loss: 0.07573 valid_loss: 0.08779 test_loss: 0.09833 \n",
      "[191/200] train_loss: 0.07738 valid_loss: 0.08803 test_loss: 0.09904 \n",
      "[192/200] train_loss: 0.07512 valid_loss: 0.08602 test_loss: 0.09729 \n",
      "Validation loss decreased (0.087297 --> 0.086018).  Saving model ...\n",
      "[193/200] train_loss: 0.07764 valid_loss: 0.08991 test_loss: 0.09932 \n",
      "[194/200] train_loss: 0.07680 valid_loss: 0.08864 test_loss: 0.09912 \n",
      "[195/200] train_loss: 0.07634 valid_loss: 0.08805 test_loss: 0.09927 \n",
      "[196/200] train_loss: 0.07555 valid_loss: 0.09441 test_loss: 0.09825 \n",
      "[197/200] train_loss: 0.07543 valid_loss: 0.09315 test_loss: 0.10014 \n",
      "[198/200] train_loss: 0.07839 valid_loss: 0.08759 test_loss: 0.09983 \n",
      "[199/200] train_loss: 0.07647 valid_loss: 0.08695 test_loss: 0.09789 \n",
      "[200/200] train_loss: 0.07531 valid_loss: 0.08708 test_loss: 0.09827 \n",
      "Model 19 trained for seen data.\n",
      "TRAINING MODEL for seen house 20\n",
      "[  1/200] train_loss: 0.50002 valid_loss: 0.38556 test_loss: 0.39194 \n",
      "Validation loss decreased (inf --> 0.385562).  Saving model ...\n",
      "[  2/200] train_loss: 0.32434 valid_loss: 0.30399 test_loss: 0.31539 \n",
      "Validation loss decreased (0.385562 --> 0.303988).  Saving model ...\n",
      "[  3/200] train_loss: 0.26500 valid_loss: 0.26322 test_loss: 0.28082 \n",
      "Validation loss decreased (0.303988 --> 0.263219).  Saving model ...\n",
      "[  4/200] train_loss: 0.22923 valid_loss: 0.22813 test_loss: 0.24540 \n",
      "Validation loss decreased (0.263219 --> 0.228132).  Saving model ...\n",
      "[  5/200] train_loss: 0.20592 valid_loss: 0.20176 test_loss: 0.21861 \n",
      "Validation loss decreased (0.228132 --> 0.201755).  Saving model ...\n",
      "[  6/200] train_loss: 0.18563 valid_loss: 0.18618 test_loss: 0.20097 \n",
      "Validation loss decreased (0.201755 --> 0.186180).  Saving model ...\n",
      "[  7/200] train_loss: 0.17467 valid_loss: 0.17816 test_loss: 0.19278 \n",
      "Validation loss decreased (0.186180 --> 0.178158).  Saving model ...\n",
      "[  8/200] train_loss: 0.16533 valid_loss: 0.17068 test_loss: 0.18257 \n",
      "Validation loss decreased (0.178158 --> 0.170678).  Saving model ...\n",
      "[  9/200] train_loss: 0.15574 valid_loss: 0.16724 test_loss: 0.17719 \n",
      "Validation loss decreased (0.170678 --> 0.167241).  Saving model ...\n",
      "[ 10/200] train_loss: 0.15345 valid_loss: 0.15829 test_loss: 0.17069 \n",
      "Validation loss decreased (0.167241 --> 0.158290).  Saving model ...\n",
      "[ 11/200] train_loss: 0.15163 valid_loss: 0.15509 test_loss: 0.16641 \n",
      "Validation loss decreased (0.158290 --> 0.155091).  Saving model ...\n",
      "[ 12/200] train_loss: 0.14712 valid_loss: 0.15383 test_loss: 0.16516 \n",
      "Validation loss decreased (0.155091 --> 0.153827).  Saving model ...\n",
      "[ 13/200] train_loss: 0.14568 valid_loss: 0.14857 test_loss: 0.16134 \n",
      "Validation loss decreased (0.153827 --> 0.148572).  Saving model ...\n",
      "[ 14/200] train_loss: 0.14010 valid_loss: 0.15180 test_loss: 0.15975 \n",
      "[ 15/200] train_loss: 0.13971 valid_loss: 0.14720 test_loss: 0.15719 \n",
      "Validation loss decreased (0.148572 --> 0.147196).  Saving model ...\n",
      "[ 16/200] train_loss: 0.13497 valid_loss: 0.14531 test_loss: 0.15627 \n",
      "Validation loss decreased (0.147196 --> 0.145310).  Saving model ...\n",
      "[ 17/200] train_loss: 0.13177 valid_loss: 0.14381 test_loss: 0.15425 \n",
      "Validation loss decreased (0.145310 --> 0.143810).  Saving model ...\n",
      "[ 18/200] train_loss: 0.13082 valid_loss: 0.14225 test_loss: 0.15328 \n",
      "Validation loss decreased (0.143810 --> 0.142254).  Saving model ...\n",
      "[ 19/200] train_loss: 0.12867 valid_loss: 0.14133 test_loss: 0.15573 \n",
      "Validation loss decreased (0.142254 --> 0.141326).  Saving model ...\n",
      "[ 20/200] train_loss: 0.12944 valid_loss: 0.13598 test_loss: 0.14854 \n",
      "Validation loss decreased (0.141326 --> 0.135982).  Saving model ...\n",
      "[ 21/200] train_loss: 0.12995 valid_loss: 0.13342 test_loss: 0.14759 \n",
      "Validation loss decreased (0.135982 --> 0.133421).  Saving model ...\n",
      "[ 22/200] train_loss: 0.13140 valid_loss: 0.13784 test_loss: 0.14914 \n",
      "[ 23/200] train_loss: 0.12641 valid_loss: 0.13181 test_loss: 0.14534 \n",
      "Validation loss decreased (0.133421 --> 0.131808).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12400 valid_loss: 0.13406 test_loss: 0.14562 \n",
      "[ 25/200] train_loss: 0.12391 valid_loss: 0.13411 test_loss: 0.14411 \n",
      "[ 26/200] train_loss: 0.12127 valid_loss: 0.12858 test_loss: 0.14333 \n",
      "Validation loss decreased (0.131808 --> 0.128576).  Saving model ...\n",
      "[ 27/200] train_loss: 0.11949 valid_loss: 0.12724 test_loss: 0.14114 \n",
      "Validation loss decreased (0.128576 --> 0.127242).  Saving model ...\n",
      "[ 28/200] train_loss: 0.11837 valid_loss: 0.12800 test_loss: 0.14111 \n",
      "[ 29/200] train_loss: 0.11524 valid_loss: 0.12801 test_loss: 0.14295 \n",
      "[ 30/200] train_loss: 0.12023 valid_loss: 0.12383 test_loss: 0.14164 \n",
      "Validation loss decreased (0.127242 --> 0.123830).  Saving model ...\n",
      "[ 31/200] train_loss: 0.11772 valid_loss: 0.12764 test_loss: 0.13999 \n",
      "[ 32/200] train_loss: 0.11583 valid_loss: 0.12447 test_loss: 0.13758 \n",
      "[ 33/200] train_loss: 0.11066 valid_loss: 0.12325 test_loss: 0.13690 \n",
      "Validation loss decreased (0.123830 --> 0.123255).  Saving model ...\n",
      "[ 34/200] train_loss: 0.11738 valid_loss: 0.12570 test_loss: 0.13760 \n",
      "[ 35/200] train_loss: 0.11041 valid_loss: 0.12645 test_loss: 0.13918 \n",
      "[ 36/200] train_loss: 0.11324 valid_loss: 0.12025 test_loss: 0.13505 \n",
      "Validation loss decreased (0.123255 --> 0.120252).  Saving model ...\n",
      "[ 37/200] train_loss: 0.11426 valid_loss: 0.12357 test_loss: 0.13668 \n",
      "[ 38/200] train_loss: 0.11072 valid_loss: 0.11769 test_loss: 0.13350 \n",
      "Validation loss decreased (0.120252 --> 0.117690).  Saving model ...\n",
      "[ 39/200] train_loss: 0.11165 valid_loss: 0.12161 test_loss: 0.13420 \n",
      "[ 40/200] train_loss: 0.10856 valid_loss: 0.12025 test_loss: 0.13389 \n",
      "[ 41/200] train_loss: 0.11245 valid_loss: 0.11844 test_loss: 0.13269 \n",
      "[ 42/200] train_loss: 0.11138 valid_loss: 0.11919 test_loss: 0.13242 \n",
      "[ 43/200] train_loss: 0.10851 valid_loss: 0.11933 test_loss: 0.13316 \n",
      "[ 44/200] train_loss: 0.10695 valid_loss: 0.11697 test_loss: 0.13128 \n",
      "Validation loss decreased (0.117690 --> 0.116966).  Saving model ...\n",
      "[ 45/200] train_loss: 0.10809 valid_loss: 0.11836 test_loss: 0.13275 \n",
      "[ 46/200] train_loss: 0.10925 valid_loss: 0.11543 test_loss: 0.13050 \n",
      "Validation loss decreased (0.116966 --> 0.115429).  Saving model ...\n",
      "[ 47/200] train_loss: 0.10616 valid_loss: 0.11701 test_loss: 0.12904 \n",
      "[ 48/200] train_loss: 0.10774 valid_loss: 0.11369 test_loss: 0.12945 \n",
      "Validation loss decreased (0.115429 --> 0.113691).  Saving model ...\n",
      "[ 49/200] train_loss: 0.10542 valid_loss: 0.11325 test_loss: 0.12796 \n",
      "Validation loss decreased (0.113691 --> 0.113251).  Saving model ...\n",
      "[ 50/200] train_loss: 0.10428 valid_loss: 0.11464 test_loss: 0.12943 \n",
      "[ 51/200] train_loss: 0.10602 valid_loss: 0.11307 test_loss: 0.12774 \n",
      "Validation loss decreased (0.113251 --> 0.113069).  Saving model ...\n",
      "[ 52/200] train_loss: 0.10284 valid_loss: 0.11466 test_loss: 0.12742 \n",
      "[ 53/200] train_loss: 0.10305 valid_loss: 0.11168 test_loss: 0.12642 \n",
      "Validation loss decreased (0.113069 --> 0.111683).  Saving model ...\n",
      "[ 54/200] train_loss: 0.10346 valid_loss: 0.11236 test_loss: 0.12501 \n",
      "[ 55/200] train_loss: 0.10427 valid_loss: 0.10914 test_loss: 0.12438 \n",
      "Validation loss decreased (0.111683 --> 0.109139).  Saving model ...\n",
      "[ 56/200] train_loss: 0.10252 valid_loss: 0.11666 test_loss: 0.12713 \n",
      "[ 57/200] train_loss: 0.10114 valid_loss: 0.11517 test_loss: 0.12503 \n",
      "[ 58/200] train_loss: 0.10209 valid_loss: 0.11126 test_loss: 0.12464 \n",
      "[ 59/200] train_loss: 0.09929 valid_loss: 0.11018 test_loss: 0.12396 \n",
      "[ 60/200] train_loss: 0.09870 valid_loss: 0.10950 test_loss: 0.12422 \n",
      "[ 61/200] train_loss: 0.10012 valid_loss: 0.10703 test_loss: 0.12283 \n",
      "Validation loss decreased (0.109139 --> 0.107027).  Saving model ...\n",
      "[ 62/200] train_loss: 0.10070 valid_loss: 0.11492 test_loss: 0.12295 \n",
      "[ 63/200] train_loss: 0.10210 valid_loss: 0.10785 test_loss: 0.12227 \n",
      "[ 64/200] train_loss: 0.09969 valid_loss: 0.10979 test_loss: 0.12264 \n",
      "[ 65/200] train_loss: 0.09660 valid_loss: 0.10872 test_loss: 0.12317 \n",
      "[ 66/200] train_loss: 0.09966 valid_loss: 0.10949 test_loss: 0.12067 \n",
      "[ 67/200] train_loss: 0.09818 valid_loss: 0.10806 test_loss: 0.12197 \n",
      "[ 68/200] train_loss: 0.09818 valid_loss: 0.10601 test_loss: 0.12085 \n",
      "Validation loss decreased (0.107027 --> 0.106008).  Saving model ...\n",
      "[ 69/200] train_loss: 0.10012 valid_loss: 0.10577 test_loss: 0.12024 \n",
      "Validation loss decreased (0.106008 --> 0.105772).  Saving model ...\n",
      "[ 70/200] train_loss: 0.09514 valid_loss: 0.11026 test_loss: 0.12088 \n",
      "[ 71/200] train_loss: 0.09819 valid_loss: 0.10629 test_loss: 0.11997 \n",
      "[ 72/200] train_loss: 0.09684 valid_loss: 0.10755 test_loss: 0.11792 \n",
      "[ 73/200] train_loss: 0.09452 valid_loss: 0.10465 test_loss: 0.12055 \n",
      "Validation loss decreased (0.105772 --> 0.104650).  Saving model ...\n",
      "[ 74/200] train_loss: 0.09726 valid_loss: 0.10405 test_loss: 0.11741 \n",
      "Validation loss decreased (0.104650 --> 0.104050).  Saving model ...\n",
      "[ 75/200] train_loss: 0.09671 valid_loss: 0.10247 test_loss: 0.11661 \n",
      "Validation loss decreased (0.104050 --> 0.102467).  Saving model ...\n",
      "[ 76/200] train_loss: 0.09427 valid_loss: 0.10652 test_loss: 0.11958 \n",
      "[ 77/200] train_loss: 0.09682 valid_loss: 0.10380 test_loss: 0.11962 \n",
      "[ 78/200] train_loss: 0.09348 valid_loss: 0.10306 test_loss: 0.11705 \n",
      "[ 79/200] train_loss: 0.09326 valid_loss: 0.10669 test_loss: 0.11860 \n",
      "[ 80/200] train_loss: 0.09250 valid_loss: 0.10147 test_loss: 0.11680 \n",
      "Validation loss decreased (0.102467 --> 0.101466).  Saving model ...\n",
      "[ 81/200] train_loss: 0.09367 valid_loss: 0.10386 test_loss: 0.11693 \n",
      "[ 82/200] train_loss: 0.09335 valid_loss: 0.10031 test_loss: 0.11524 \n",
      "Validation loss decreased (0.101466 --> 0.100311).  Saving model ...\n",
      "[ 83/200] train_loss: 0.09394 valid_loss: 0.10015 test_loss: 0.11478 \n",
      "Validation loss decreased (0.100311 --> 0.100150).  Saving model ...\n",
      "[ 84/200] train_loss: 0.09540 valid_loss: 0.10134 test_loss: 0.11549 \n",
      "[ 85/200] train_loss: 0.09215 valid_loss: 0.10468 test_loss: 0.11469 \n",
      "[ 86/200] train_loss: 0.09222 valid_loss: 0.10462 test_loss: 0.11405 \n",
      "[ 87/200] train_loss: 0.09428 valid_loss: 0.10432 test_loss: 0.11512 \n",
      "[ 88/200] train_loss: 0.09416 valid_loss: 0.10181 test_loss: 0.11426 \n",
      "[ 89/200] train_loss: 0.09166 valid_loss: 0.10157 test_loss: 0.11518 \n",
      "[ 90/200] train_loss: 0.09035 valid_loss: 0.10398 test_loss: 0.11599 \n",
      "[ 91/200] train_loss: 0.08903 valid_loss: 0.10313 test_loss: 0.11277 \n",
      "[ 92/200] train_loss: 0.09227 valid_loss: 0.09831 test_loss: 0.11258 \n",
      "Validation loss decreased (0.100150 --> 0.098306).  Saving model ...\n",
      "[ 93/200] train_loss: 0.09200 valid_loss: 0.09924 test_loss: 0.11318 \n",
      "[ 94/200] train_loss: 0.09151 valid_loss: 0.09730 test_loss: 0.11243 \n",
      "Validation loss decreased (0.098306 --> 0.097297).  Saving model ...\n",
      "[ 95/200] train_loss: 0.09003 valid_loss: 0.09995 test_loss: 0.11285 \n",
      "[ 96/200] train_loss: 0.09262 valid_loss: 0.10060 test_loss: 0.11469 \n",
      "[ 97/200] train_loss: 0.09126 valid_loss: 0.09859 test_loss: 0.11364 \n",
      "[ 98/200] train_loss: 0.08834 valid_loss: 0.09934 test_loss: 0.11298 \n",
      "[ 99/200] train_loss: 0.09095 valid_loss: 0.10056 test_loss: 0.11366 \n",
      "[100/200] train_loss: 0.08616 valid_loss: 0.10089 test_loss: 0.11139 \n",
      "[101/200] train_loss: 0.09038 valid_loss: 0.09657 test_loss: 0.11123 \n",
      "Validation loss decreased (0.097297 --> 0.096575).  Saving model ...\n",
      "[102/200] train_loss: 0.08703 valid_loss: 0.09524 test_loss: 0.11009 \n",
      "Validation loss decreased (0.096575 --> 0.095241).  Saving model ...\n",
      "[103/200] train_loss: 0.08756 valid_loss: 0.09867 test_loss: 0.11130 \n",
      "[104/200] train_loss: 0.08648 valid_loss: 0.09720 test_loss: 0.11083 \n",
      "[105/200] train_loss: 0.08697 valid_loss: 0.09790 test_loss: 0.11167 \n",
      "[106/200] train_loss: 0.08699 valid_loss: 0.09697 test_loss: 0.11006 \n",
      "[107/200] train_loss: 0.08880 valid_loss: 0.09694 test_loss: 0.11075 \n",
      "[108/200] train_loss: 0.08669 valid_loss: 0.09692 test_loss: 0.11025 \n",
      "[109/200] train_loss: 0.08839 valid_loss: 0.09531 test_loss: 0.10915 \n",
      "[110/200] train_loss: 0.08676 valid_loss: 0.09616 test_loss: 0.10986 \n",
      "[111/200] train_loss: 0.08724 valid_loss: 0.09751 test_loss: 0.11100 \n",
      "[112/200] train_loss: 0.08630 valid_loss: 0.09496 test_loss: 0.10816 \n",
      "Validation loss decreased (0.095241 --> 0.094958).  Saving model ...\n",
      "[113/200] train_loss: 0.08444 valid_loss: 0.10691 test_loss: 0.11035 \n",
      "[114/200] train_loss: 0.08444 valid_loss: 0.09465 test_loss: 0.10859 \n",
      "Validation loss decreased (0.094958 --> 0.094650).  Saving model ...\n",
      "[115/200] train_loss: 0.08612 valid_loss: 0.09489 test_loss: 0.10756 \n",
      "[116/200] train_loss: 0.08652 valid_loss: 0.09947 test_loss: 0.10791 \n",
      "[117/200] train_loss: 0.08980 valid_loss: 0.09614 test_loss: 0.10763 \n",
      "[118/200] train_loss: 0.08728 valid_loss: 0.09570 test_loss: 0.10835 \n",
      "[119/200] train_loss: 0.08749 valid_loss: 0.09329 test_loss: 0.10759 \n",
      "Validation loss decreased (0.094650 --> 0.093295).  Saving model ...\n",
      "[120/200] train_loss: 0.08462 valid_loss: 0.09528 test_loss: 0.10926 \n",
      "[121/200] train_loss: 0.08689 valid_loss: 0.09432 test_loss: 0.10673 \n",
      "[122/200] train_loss: 0.08799 valid_loss: 0.09324 test_loss: 0.10666 \n",
      "Validation loss decreased (0.093295 --> 0.093242).  Saving model ...\n",
      "[123/200] train_loss: 0.08506 valid_loss: 0.09556 test_loss: 0.10739 \n",
      "[124/200] train_loss: 0.08499 valid_loss: 0.09503 test_loss: 0.10639 \n",
      "[125/200] train_loss: 0.08569 valid_loss: 0.09498 test_loss: 0.10637 \n",
      "[126/200] train_loss: 0.08528 valid_loss: 0.09450 test_loss: 0.10696 \n",
      "[127/200] train_loss: 0.08300 valid_loss: 0.09362 test_loss: 0.10503 \n",
      "[128/200] train_loss: 0.08554 valid_loss: 0.09966 test_loss: 0.10764 \n",
      "[129/200] train_loss: 0.08272 valid_loss: 0.09657 test_loss: 0.10653 \n",
      "[130/200] train_loss: 0.08303 valid_loss: 0.09730 test_loss: 0.10734 \n",
      "[131/200] train_loss: 0.08481 valid_loss: 0.09551 test_loss: 0.10583 \n",
      "[132/200] train_loss: 0.08438 valid_loss: 0.09491 test_loss: 0.10548 \n",
      "[133/200] train_loss: 0.08138 valid_loss: 0.09473 test_loss: 0.10558 \n",
      "[134/200] train_loss: 0.08340 valid_loss: 0.09183 test_loss: 0.10527 \n",
      "Validation loss decreased (0.093242 --> 0.091831).  Saving model ...\n",
      "[135/200] train_loss: 0.08380 valid_loss: 0.09850 test_loss: 0.10682 \n",
      "[136/200] train_loss: 0.08578 valid_loss: 0.09084 test_loss: 0.10413 \n",
      "Validation loss decreased (0.091831 --> 0.090838).  Saving model ...\n",
      "[137/200] train_loss: 0.08345 valid_loss: 0.09255 test_loss: 0.10454 \n",
      "[138/200] train_loss: 0.08399 valid_loss: 0.09227 test_loss: 0.10475 \n",
      "[139/200] train_loss: 0.08216 valid_loss: 0.09122 test_loss: 0.10421 \n",
      "[140/200] train_loss: 0.08393 valid_loss: 0.09134 test_loss: 0.10428 \n",
      "[141/200] train_loss: 0.08290 valid_loss: 0.09465 test_loss: 0.10489 \n",
      "[142/200] train_loss: 0.08072 valid_loss: 0.09186 test_loss: 0.10443 \n",
      "[143/200] train_loss: 0.07993 valid_loss: 0.09134 test_loss: 0.10356 \n",
      "[144/200] train_loss: 0.08308 valid_loss: 0.09158 test_loss: 0.10354 \n",
      "[145/200] train_loss: 0.07974 valid_loss: 0.09313 test_loss: 0.10414 \n",
      "[146/200] train_loss: 0.08187 valid_loss: 0.09142 test_loss: 0.10287 \n",
      "[147/200] train_loss: 0.08293 valid_loss: 0.09248 test_loss: 0.10344 \n",
      "[148/200] train_loss: 0.08222 valid_loss: 0.08995 test_loss: 0.10421 \n",
      "Validation loss decreased (0.090838 --> 0.089952).  Saving model ...\n",
      "[149/200] train_loss: 0.08118 valid_loss: 0.09032 test_loss: 0.10535 \n",
      "[150/200] train_loss: 0.07934 valid_loss: 0.08947 test_loss: 0.10320 \n",
      "Validation loss decreased (0.089952 --> 0.089474).  Saving model ...\n",
      "[151/200] train_loss: 0.07908 valid_loss: 0.09169 test_loss: 0.10317 \n",
      "[152/200] train_loss: 0.07949 valid_loss: 0.08946 test_loss: 0.10195 \n",
      "Validation loss decreased (0.089474 --> 0.089465).  Saving model ...\n",
      "[153/200] train_loss: 0.08097 valid_loss: 0.08922 test_loss: 0.10226 \n",
      "Validation loss decreased (0.089465 --> 0.089219).  Saving model ...\n",
      "[154/200] train_loss: 0.08025 valid_loss: 0.09032 test_loss: 0.10248 \n",
      "[155/200] train_loss: 0.08185 valid_loss: 0.09273 test_loss: 0.10313 \n",
      "[156/200] train_loss: 0.07916 valid_loss: 0.09229 test_loss: 0.10118 \n",
      "[157/200] train_loss: 0.07993 valid_loss: 0.09038 test_loss: 0.10256 \n",
      "[158/200] train_loss: 0.07836 valid_loss: 0.09390 test_loss: 0.10216 \n",
      "[159/200] train_loss: 0.08003 valid_loss: 0.08733 test_loss: 0.10077 \n",
      "Validation loss decreased (0.089219 --> 0.087333).  Saving model ...\n",
      "[160/200] train_loss: 0.07835 valid_loss: 0.08831 test_loss: 0.10090 \n",
      "[161/200] train_loss: 0.07924 valid_loss: 0.09517 test_loss: 0.10078 \n",
      "[162/200] train_loss: 0.07950 valid_loss: 0.09109 test_loss: 0.10367 \n",
      "[163/200] train_loss: 0.07907 valid_loss: 0.09060 test_loss: 0.10157 \n",
      "[164/200] train_loss: 0.08012 valid_loss: 0.09196 test_loss: 0.10213 \n",
      "[165/200] train_loss: 0.08028 valid_loss: 0.08926 test_loss: 0.10170 \n",
      "[166/200] train_loss: 0.07907 valid_loss: 0.09097 test_loss: 0.10115 \n",
      "[167/200] train_loss: 0.07647 valid_loss: 0.08781 test_loss: 0.10147 \n",
      "[168/200] train_loss: 0.08155 valid_loss: 0.08947 test_loss: 0.10106 \n",
      "[169/200] train_loss: 0.07797 valid_loss: 0.09025 test_loss: 0.10065 \n",
      "[170/200] train_loss: 0.07836 valid_loss: 0.08908 test_loss: 0.10207 \n",
      "[171/200] train_loss: 0.07708 valid_loss: 0.08608 test_loss: 0.10018 \n",
      "Validation loss decreased (0.087333 --> 0.086084).  Saving model ...\n",
      "[172/200] train_loss: 0.07545 valid_loss: 0.09318 test_loss: 0.10128 \n",
      "[173/200] train_loss: 0.07818 valid_loss: 0.08877 test_loss: 0.10022 \n",
      "[174/200] train_loss: 0.07630 valid_loss: 0.08965 test_loss: 0.10046 \n",
      "[175/200] train_loss: 0.07814 valid_loss: 0.08763 test_loss: 0.10012 \n",
      "[176/200] train_loss: 0.07675 valid_loss: 0.08732 test_loss: 0.10026 \n",
      "[177/200] train_loss: 0.08092 valid_loss: 0.08816 test_loss: 0.10016 \n",
      "[178/200] train_loss: 0.07805 valid_loss: 0.08802 test_loss: 0.09965 \n",
      "[179/200] train_loss: 0.07664 valid_loss: 0.08756 test_loss: 0.09968 \n",
      "[180/200] train_loss: 0.07709 valid_loss: 0.08825 test_loss: 0.10022 \n",
      "[181/200] train_loss: 0.08073 valid_loss: 0.08829 test_loss: 0.10017 \n",
      "[182/200] train_loss: 0.07649 valid_loss: 0.08754 test_loss: 0.09868 \n",
      "[183/200] train_loss: 0.07489 valid_loss: 0.09117 test_loss: 0.09888 \n",
      "[184/200] train_loss: 0.07484 valid_loss: 0.09054 test_loss: 0.10121 \n",
      "[185/200] train_loss: 0.07664 valid_loss: 0.09044 test_loss: 0.09926 \n",
      "[186/200] train_loss: 0.07740 valid_loss: 0.08704 test_loss: 0.09906 \n",
      "[187/200] train_loss: 0.07776 valid_loss: 0.08852 test_loss: 0.09901 \n",
      "[188/200] train_loss: 0.07872 valid_loss: 0.09370 test_loss: 0.09934 \n",
      "[189/200] train_loss: 0.07762 valid_loss: 0.08716 test_loss: 0.09889 \n",
      "[190/200] train_loss: 0.07685 valid_loss: 0.08735 test_loss: 0.09825 \n",
      "[191/200] train_loss: 0.07685 valid_loss: 0.08964 test_loss: 0.09781 \n",
      "[192/200] train_loss: 0.07625 valid_loss: 0.08607 test_loss: 0.09863 \n",
      "Validation loss decreased (0.086084 --> 0.086074).  Saving model ...\n",
      "[193/200] train_loss: 0.07556 valid_loss: 0.08882 test_loss: 0.09847 \n",
      "[194/200] train_loss: 0.07361 valid_loss: 0.08600 test_loss: 0.09822 \n",
      "Validation loss decreased (0.086074 --> 0.086002).  Saving model ...\n",
      "[195/200] train_loss: 0.07565 valid_loss: 0.08860 test_loss: 0.09808 \n",
      "[196/200] train_loss: 0.07621 valid_loss: 0.08744 test_loss: 0.09938 \n",
      "[197/200] train_loss: 0.07664 valid_loss: 0.08491 test_loss: 0.09839 \n",
      "Validation loss decreased (0.086002 --> 0.084909).  Saving model ...\n",
      "[198/200] train_loss: 0.07476 valid_loss: 0.08808 test_loss: 0.09836 \n",
      "[199/200] train_loss: 0.07545 valid_loss: 0.08921 test_loss: 0.09822 \n",
      "[200/200] train_loss: 0.07583 valid_loss: 0.08727 test_loss: 0.09720 \n",
      "Model 20 trained for seen data.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the training process for both seen and unseen data\n",
    "# Seen data training and validation\n",
    "train_loader = dl_train_seen\n",
    "valid_loader = dl_valid_seen\n",
    "test_loader = dl_test_seen\n",
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 200\n",
    "\n",
    "\n",
    "\n",
    "# Train models for seen data \n",
    "for i in range(20):\n",
    "    print('TRAINING MODEL for seen house %d' % (i + 1))\n",
    "    model = PTPNet(1, 3, 32)  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    model_filename = f'UKDALE_seen_model_{i+1}.pth'\n",
    "    model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, model_filename)\n",
    "    print(f\"Model {i+1} trained for seen data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL for unseen house 2\n",
      "[  1/100] train_loss: 0.61289 valid_loss: 0.50132 test_loss: 0.46517 \n",
      "Validation loss decreased (inf --> 0.501322).  Saving model ...\n",
      "[  2/100] train_loss: 0.41727 valid_loss: 0.36538 test_loss: 0.32224 \n",
      "Validation loss decreased (0.501322 --> 0.365378).  Saving model ...\n",
      "[  3/100] train_loss: 0.32942 valid_loss: 0.31635 test_loss: 0.27149 \n",
      "Validation loss decreased (0.365378 --> 0.316346).  Saving model ...\n",
      "[  4/100] train_loss: 0.28558 valid_loss: 0.27743 test_loss: 0.23492 \n",
      "Validation loss decreased (0.316346 --> 0.277432).  Saving model ...\n",
      "[  5/100] train_loss: 0.24645 valid_loss: 0.25212 test_loss: 0.20553 \n",
      "Validation loss decreased (0.277432 --> 0.252117).  Saving model ...\n",
      "[  6/100] train_loss: 0.22434 valid_loss: 0.22620 test_loss: 0.18322 \n",
      "Validation loss decreased (0.252117 --> 0.226203).  Saving model ...\n",
      "[  7/100] train_loss: 0.20371 valid_loss: 0.20924 test_loss: 0.16374 \n",
      "Validation loss decreased (0.226203 --> 0.209241).  Saving model ...\n",
      "[  8/100] train_loss: 0.18810 valid_loss: 0.19977 test_loss: 0.15672 \n",
      "Validation loss decreased (0.209241 --> 0.199765).  Saving model ...\n",
      "[  9/100] train_loss: 0.18247 valid_loss: 0.19077 test_loss: 0.14691 \n",
      "Validation loss decreased (0.199765 --> 0.190775).  Saving model ...\n",
      "[ 10/100] train_loss: 0.17081 valid_loss: 0.18609 test_loss: 0.13914 \n",
      "Validation loss decreased (0.190775 --> 0.186087).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16624 valid_loss: 0.18022 test_loss: 0.13508 \n",
      "Validation loss decreased (0.186087 --> 0.180215).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16086 valid_loss: 0.17287 test_loss: 0.12721 \n",
      "Validation loss decreased (0.180215 --> 0.172869).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15566 valid_loss: 0.17063 test_loss: 0.12365 \n",
      "Validation loss decreased (0.172869 --> 0.170629).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15293 valid_loss: 0.16744 test_loss: 0.12360 \n",
      "Validation loss decreased (0.170629 --> 0.167441).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14777 valid_loss: 0.16497 test_loss: 0.11951 \n",
      "Validation loss decreased (0.167441 --> 0.164968).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14658 valid_loss: 0.16041 test_loss: 0.11715 \n",
      "Validation loss decreased (0.164968 --> 0.160409).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14490 valid_loss: 0.15997 test_loss: 0.11726 \n",
      "Validation loss decreased (0.160409 --> 0.159966).  Saving model ...\n",
      "[ 18/100] train_loss: 0.14069 valid_loss: 0.15908 test_loss: 0.11478 \n",
      "Validation loss decreased (0.159966 --> 0.159081).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13651 valid_loss: 0.15441 test_loss: 0.11406 \n",
      "Validation loss decreased (0.159081 --> 0.154410).  Saving model ...\n",
      "[ 20/100] train_loss: 0.14036 valid_loss: 0.15430 test_loss: 0.11478 \n",
      "Validation loss decreased (0.154410 --> 0.154301).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13859 valid_loss: 0.15521 test_loss: 0.11285 \n",
      "[ 22/100] train_loss: 0.13319 valid_loss: 0.15043 test_loss: 0.11118 \n",
      "Validation loss decreased (0.154301 --> 0.150433).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13057 valid_loss: 0.14699 test_loss: 0.10861 \n",
      "Validation loss decreased (0.150433 --> 0.146994).  Saving model ...\n",
      "[ 24/100] train_loss: 0.12919 valid_loss: 0.14847 test_loss: 0.11115 \n",
      "[ 25/100] train_loss: 0.13055 valid_loss: 0.14763 test_loss: 0.11067 \n",
      "[ 26/100] train_loss: 0.12676 valid_loss: 0.14563 test_loss: 0.11030 \n",
      "Validation loss decreased (0.146994 --> 0.145631).  Saving model ...\n",
      "[ 27/100] train_loss: 0.12701 valid_loss: 0.14011 test_loss: 0.10584 \n",
      "Validation loss decreased (0.145631 --> 0.140114).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12909 valid_loss: 0.13794 test_loss: 0.10704 \n",
      "Validation loss decreased (0.140114 --> 0.137938).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12349 valid_loss: 0.14266 test_loss: 0.10475 \n",
      "[ 30/100] train_loss: 0.12484 valid_loss: 0.13696 test_loss: 0.10260 \n",
      "Validation loss decreased (0.137938 --> 0.136961).  Saving model ...\n",
      "[ 31/100] train_loss: 0.12367 valid_loss: 0.13551 test_loss: 0.10141 \n",
      "Validation loss decreased (0.136961 --> 0.135514).  Saving model ...\n",
      "[ 32/100] train_loss: 0.12147 valid_loss: 0.13674 test_loss: 0.10535 \n",
      "[ 33/100] train_loss: 0.12163 valid_loss: 0.13425 test_loss: 0.10241 \n",
      "Validation loss decreased (0.135514 --> 0.134248).  Saving model ...\n",
      "[ 34/100] train_loss: 0.12018 valid_loss: 0.13312 test_loss: 0.10270 \n",
      "Validation loss decreased (0.134248 --> 0.133124).  Saving model ...\n",
      "[ 35/100] train_loss: 0.11660 valid_loss: 0.13827 test_loss: 0.10626 \n",
      "[ 36/100] train_loss: 0.12019 valid_loss: 0.13377 test_loss: 0.10106 \n",
      "[ 37/100] train_loss: 0.11930 valid_loss: 0.12909 test_loss: 0.09912 \n",
      "Validation loss decreased (0.133124 --> 0.129094).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11842 valid_loss: 0.13103 test_loss: 0.09892 \n",
      "[ 39/100] train_loss: 0.11624 valid_loss: 0.13370 test_loss: 0.10457 \n",
      "[ 40/100] train_loss: 0.11862 valid_loss: 0.12561 test_loss: 0.09655 \n",
      "Validation loss decreased (0.129094 --> 0.125605).  Saving model ...\n",
      "[ 41/100] train_loss: 0.11423 valid_loss: 0.12867 test_loss: 0.09992 \n",
      "[ 42/100] train_loss: 0.11262 valid_loss: 0.13041 test_loss: 0.10093 \n",
      "[ 43/100] train_loss: 0.11453 valid_loss: 0.12739 test_loss: 0.09855 \n",
      "[ 44/100] train_loss: 0.11309 valid_loss: 0.12442 test_loss: 0.09844 \n",
      "Validation loss decreased (0.125605 --> 0.124416).  Saving model ...\n",
      "[ 45/100] train_loss: 0.11022 valid_loss: 0.12651 test_loss: 0.09855 \n",
      "[ 46/100] train_loss: 0.11291 valid_loss: 0.12606 test_loss: 0.10058 \n",
      "[ 47/100] train_loss: 0.11210 valid_loss: 0.12732 test_loss: 0.09946 \n",
      "[ 48/100] train_loss: 0.11278 valid_loss: 0.12312 test_loss: 0.09934 \n",
      "Validation loss decreased (0.124416 --> 0.123121).  Saving model ...\n",
      "[ 49/100] train_loss: 0.11069 valid_loss: 0.12915 test_loss: 0.10286 \n",
      "[ 50/100] train_loss: 0.11375 valid_loss: 0.12293 test_loss: 0.09712 \n",
      "Validation loss decreased (0.123121 --> 0.122927).  Saving model ...\n",
      "[ 51/100] train_loss: 0.10998 valid_loss: 0.12787 test_loss: 0.09844 \n",
      "[ 52/100] train_loss: 0.10913 valid_loss: 0.12059 test_loss: 0.09330 \n",
      "Validation loss decreased (0.122927 --> 0.120585).  Saving model ...\n",
      "[ 53/100] train_loss: 0.11009 valid_loss: 0.12288 test_loss: 0.09825 \n",
      "[ 54/100] train_loss: 0.10763 valid_loss: 0.12209 test_loss: 0.09664 \n",
      "[ 55/100] train_loss: 0.10559 valid_loss: 0.12183 test_loss: 0.09736 \n",
      "[ 56/100] train_loss: 0.10738 valid_loss: 0.11782 test_loss: 0.09460 \n",
      "Validation loss decreased (0.120585 --> 0.117819).  Saving model ...\n",
      "[ 57/100] train_loss: 0.10472 valid_loss: 0.12030 test_loss: 0.09505 \n",
      "[ 58/100] train_loss: 0.10554 valid_loss: 0.11989 test_loss: 0.09482 \n",
      "[ 59/100] train_loss: 0.10687 valid_loss: 0.12259 test_loss: 0.09729 \n",
      "[ 60/100] train_loss: 0.10252 valid_loss: 0.11993 test_loss: 0.09428 \n",
      "[ 61/100] train_loss: 0.10845 valid_loss: 0.12153 test_loss: 0.09565 \n",
      "[ 62/100] train_loss: 0.10224 valid_loss: 0.11686 test_loss: 0.09358 \n",
      "Validation loss decreased (0.117819 --> 0.116863).  Saving model ...\n",
      "[ 63/100] train_loss: 0.10743 valid_loss: 0.11861 test_loss: 0.09451 \n",
      "[ 64/100] train_loss: 0.10338 valid_loss: 0.12441 test_loss: 0.10072 \n",
      "[ 65/100] train_loss: 0.10740 valid_loss: 0.11501 test_loss: 0.09267 \n",
      "Validation loss decreased (0.116863 --> 0.115013).  Saving model ...\n",
      "[ 66/100] train_loss: 0.10144 valid_loss: 0.11522 test_loss: 0.09268 \n",
      "[ 67/100] train_loss: 0.10234 valid_loss: 0.12102 test_loss: 0.09435 \n",
      "[ 68/100] train_loss: 0.10104 valid_loss: 0.11524 test_loss: 0.08976 \n",
      "[ 69/100] train_loss: 0.09999 valid_loss: 0.11938 test_loss: 0.09260 \n",
      "[ 70/100] train_loss: 0.10299 valid_loss: 0.11794 test_loss: 0.09517 \n",
      "[ 71/100] train_loss: 0.10000 valid_loss: 0.11793 test_loss: 0.09410 \n",
      "[ 72/100] train_loss: 0.10288 valid_loss: 0.11718 test_loss: 0.09252 \n",
      "[ 73/100] train_loss: 0.09995 valid_loss: 0.11400 test_loss: 0.08945 \n",
      "Validation loss decreased (0.115013 --> 0.113997).  Saving model ...\n",
      "[ 74/100] train_loss: 0.09826 valid_loss: 0.11495 test_loss: 0.09072 \n",
      "[ 75/100] train_loss: 0.09839 valid_loss: 0.11861 test_loss: 0.09086 \n",
      "[ 76/100] train_loss: 0.09830 valid_loss: 0.11784 test_loss: 0.09384 \n",
      "[ 77/100] train_loss: 0.10010 valid_loss: 0.11540 test_loss: 0.08953 \n",
      "[ 78/100] train_loss: 0.09964 valid_loss: 0.11643 test_loss: 0.08860 \n",
      "[ 79/100] train_loss: 0.10193 valid_loss: 0.11720 test_loss: 0.09083 \n",
      "[ 80/100] train_loss: 0.09634 valid_loss: 0.11499 test_loss: 0.08994 \n",
      "[ 81/100] train_loss: 0.09772 valid_loss: 0.11644 test_loss: 0.08953 \n",
      "[ 82/100] train_loss: 0.09827 valid_loss: 0.11239 test_loss: 0.08971 \n",
      "Validation loss decreased (0.113997 --> 0.112388).  Saving model ...\n",
      "[ 83/100] train_loss: 0.09897 valid_loss: 0.11400 test_loss: 0.08859 \n",
      "[ 84/100] train_loss: 0.09667 valid_loss: 0.11220 test_loss: 0.08690 \n",
      "Validation loss decreased (0.112388 --> 0.112205).  Saving model ...\n",
      "[ 85/100] train_loss: 0.09753 valid_loss: 0.11250 test_loss: 0.08833 \n",
      "[ 86/100] train_loss: 0.09549 valid_loss: 0.11230 test_loss: 0.08574 \n",
      "[ 87/100] train_loss: 0.09637 valid_loss: 0.11273 test_loss: 0.08696 \n",
      "[ 88/100] train_loss: 0.09589 valid_loss: 0.11373 test_loss: 0.08743 \n",
      "[ 89/100] train_loss: 0.09708 valid_loss: 0.11359 test_loss: 0.08870 \n",
      "[ 90/100] train_loss: 0.09592 valid_loss: 0.11307 test_loss: 0.08769 \n",
      "[ 91/100] train_loss: 0.09310 valid_loss: 0.11364 test_loss: 0.08721 \n",
      "[ 92/100] train_loss: 0.09574 valid_loss: 0.11473 test_loss: 0.08771 \n",
      "[ 93/100] train_loss: 0.09493 valid_loss: 0.11148 test_loss: 0.08544 \n",
      "Validation loss decreased (0.112205 --> 0.111484).  Saving model ...\n",
      "[ 94/100] train_loss: 0.09503 valid_loss: 0.11605 test_loss: 0.09234 \n",
      "[ 95/100] train_loss: 0.09626 valid_loss: 0.11179 test_loss: 0.08699 \n",
      "[ 96/100] train_loss: 0.09708 valid_loss: 0.11202 test_loss: 0.08936 \n",
      "[ 97/100] train_loss: 0.09286 valid_loss: 0.11272 test_loss: 0.08941 \n",
      "[ 98/100] train_loss: 0.09690 valid_loss: 0.11092 test_loss: 0.08881 \n",
      "Validation loss decreased (0.111484 --> 0.110921).  Saving model ...\n",
      "[ 99/100] train_loss: 0.09257 valid_loss: 0.11036 test_loss: 0.08603 \n",
      "Validation loss decreased (0.110921 --> 0.110360).  Saving model ...\n",
      "[100/100] train_loss: 0.09458 valid_loss: 0.10774 test_loss: 0.08425 \n",
      "Validation loss decreased (0.110360 --> 0.107738).  Saving model ...\n",
      "Model 2 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 3\n",
      "[  1/100] train_loss: 0.58900 valid_loss: 0.49015 test_loss: 0.44955 \n",
      "Validation loss decreased (inf --> 0.490146).  Saving model ...\n",
      "[  2/100] train_loss: 0.40671 valid_loss: 0.36213 test_loss: 0.31164 \n",
      "Validation loss decreased (0.490146 --> 0.362133).  Saving model ...\n",
      "[  3/100] train_loss: 0.31791 valid_loss: 0.30534 test_loss: 0.25942 \n",
      "Validation loss decreased (0.362133 --> 0.305335).  Saving model ...\n",
      "[  4/100] train_loss: 0.26956 valid_loss: 0.26906 test_loss: 0.21776 \n",
      "Validation loss decreased (0.305335 --> 0.269064).  Saving model ...\n",
      "[  5/100] train_loss: 0.23671 valid_loss: 0.23812 test_loss: 0.18738 \n",
      "Validation loss decreased (0.269064 --> 0.238119).  Saving model ...\n",
      "[  6/100] train_loss: 0.21102 valid_loss: 0.21840 test_loss: 0.17156 \n",
      "Validation loss decreased (0.238119 --> 0.218404).  Saving model ...\n",
      "[  7/100] train_loss: 0.19425 valid_loss: 0.20610 test_loss: 0.15276 \n",
      "Validation loss decreased (0.218404 --> 0.206097).  Saving model ...\n",
      "[  8/100] train_loss: 0.18227 valid_loss: 0.19476 test_loss: 0.14553 \n",
      "Validation loss decreased (0.206097 --> 0.194760).  Saving model ...\n",
      "[  9/100] train_loss: 0.17225 valid_loss: 0.18751 test_loss: 0.13851 \n",
      "Validation loss decreased (0.194760 --> 0.187509).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16514 valid_loss: 0.18000 test_loss: 0.13199 \n",
      "Validation loss decreased (0.187509 --> 0.179997).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16360 valid_loss: 0.17695 test_loss: 0.12833 \n",
      "Validation loss decreased (0.179997 --> 0.176949).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15938 valid_loss: 0.18107 test_loss: 0.12834 \n",
      "[ 13/100] train_loss: 0.15605 valid_loss: 0.17735 test_loss: 0.12482 \n",
      "[ 14/100] train_loss: 0.14795 valid_loss: 0.17241 test_loss: 0.12408 \n",
      "Validation loss decreased (0.176949 --> 0.172414).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14922 valid_loss: 0.16894 test_loss: 0.12000 \n",
      "Validation loss decreased (0.172414 --> 0.168943).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14555 valid_loss: 0.16764 test_loss: 0.11968 \n",
      "Validation loss decreased (0.168943 --> 0.167638).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14239 valid_loss: 0.16064 test_loss: 0.11621 \n",
      "Validation loss decreased (0.167638 --> 0.160639).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13927 valid_loss: 0.15776 test_loss: 0.11341 \n",
      "Validation loss decreased (0.160639 --> 0.157758).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13892 valid_loss: 0.16248 test_loss: 0.11695 \n",
      "[ 20/100] train_loss: 0.13826 valid_loss: 0.15386 test_loss: 0.11238 \n",
      "Validation loss decreased (0.157758 --> 0.153858).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13493 valid_loss: 0.15217 test_loss: 0.11089 \n",
      "Validation loss decreased (0.153858 --> 0.152174).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13758 valid_loss: 0.15413 test_loss: 0.11312 \n",
      "[ 23/100] train_loss: 0.13287 valid_loss: 0.14979 test_loss: 0.11063 \n",
      "Validation loss decreased (0.152174 --> 0.149789).  Saving model ...\n",
      "[ 24/100] train_loss: 0.13109 valid_loss: 0.15293 test_loss: 0.11353 \n",
      "[ 25/100] train_loss: 0.13103 valid_loss: 0.14854 test_loss: 0.10994 \n",
      "Validation loss decreased (0.149789 --> 0.148537).  Saving model ...\n",
      "[ 26/100] train_loss: 0.12995 valid_loss: 0.14640 test_loss: 0.10877 \n",
      "Validation loss decreased (0.148537 --> 0.146396).  Saving model ...\n",
      "[ 27/100] train_loss: 0.12434 valid_loss: 0.14827 test_loss: 0.11014 \n",
      "[ 28/100] train_loss: 0.12499 valid_loss: 0.14028 test_loss: 0.10597 \n",
      "Validation loss decreased (0.146396 --> 0.140280).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12352 valid_loss: 0.14579 test_loss: 0.11030 \n",
      "[ 30/100] train_loss: 0.12686 valid_loss: 0.14277 test_loss: 0.10715 \n",
      "[ 31/100] train_loss: 0.12241 valid_loss: 0.14286 test_loss: 0.10881 \n",
      "[ 32/100] train_loss: 0.12437 valid_loss: 0.14703 test_loss: 0.10970 \n",
      "[ 33/100] train_loss: 0.12360 valid_loss: 0.13402 test_loss: 0.10387 \n",
      "Validation loss decreased (0.140280 --> 0.134015).  Saving model ...\n",
      "[ 34/100] train_loss: 0.11812 valid_loss: 0.13352 test_loss: 0.10203 \n",
      "Validation loss decreased (0.134015 --> 0.133516).  Saving model ...\n",
      "[ 35/100] train_loss: 0.11706 valid_loss: 0.12998 test_loss: 0.10148 \n",
      "Validation loss decreased (0.133516 --> 0.129979).  Saving model ...\n",
      "[ 36/100] train_loss: 0.11804 valid_loss: 0.13415 test_loss: 0.10354 \n",
      "[ 37/100] train_loss: 0.11529 valid_loss: 0.12894 test_loss: 0.10093 \n",
      "Validation loss decreased (0.129979 --> 0.128941).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11710 valid_loss: 0.13237 test_loss: 0.10337 \n",
      "[ 39/100] train_loss: 0.11857 valid_loss: 0.12975 test_loss: 0.10399 \n",
      "[ 40/100] train_loss: 0.11378 valid_loss: 0.13369 test_loss: 0.10352 \n",
      "[ 41/100] train_loss: 0.11372 valid_loss: 0.13514 test_loss: 0.10341 \n",
      "[ 42/100] train_loss: 0.11455 valid_loss: 0.12793 test_loss: 0.09904 \n",
      "Validation loss decreased (0.128941 --> 0.127925).  Saving model ...\n",
      "[ 43/100] train_loss: 0.11298 valid_loss: 0.13001 test_loss: 0.09966 \n",
      "[ 44/100] train_loss: 0.11188 valid_loss: 0.12814 test_loss: 0.09875 \n",
      "[ 45/100] train_loss: 0.11105 valid_loss: 0.12918 test_loss: 0.09907 \n",
      "[ 46/100] train_loss: 0.10985 valid_loss: 0.13234 test_loss: 0.10407 \n",
      "[ 47/100] train_loss: 0.10803 valid_loss: 0.12502 test_loss: 0.09710 \n",
      "Validation loss decreased (0.127925 --> 0.125020).  Saving model ...\n",
      "[ 48/100] train_loss: 0.10738 valid_loss: 0.12604 test_loss: 0.09677 \n",
      "[ 49/100] train_loss: 0.11140 valid_loss: 0.12602 test_loss: 0.09817 \n",
      "[ 50/100] train_loss: 0.11071 valid_loss: 0.12630 test_loss: 0.09817 \n",
      "[ 51/100] train_loss: 0.10864 valid_loss: 0.12541 test_loss: 0.09597 \n",
      "[ 52/100] train_loss: 0.11022 valid_loss: 0.12611 test_loss: 0.09792 \n",
      "[ 53/100] train_loss: 0.10665 valid_loss: 0.12343 test_loss: 0.09881 \n",
      "Validation loss decreased (0.125020 --> 0.123427).  Saving model ...\n",
      "[ 54/100] train_loss: 0.10675 valid_loss: 0.12036 test_loss: 0.09355 \n",
      "Validation loss decreased (0.123427 --> 0.120362).  Saving model ...\n",
      "[ 55/100] train_loss: 0.10942 valid_loss: 0.12266 test_loss: 0.09687 \n",
      "[ 56/100] train_loss: 0.10746 valid_loss: 0.11862 test_loss: 0.09300 \n",
      "Validation loss decreased (0.120362 --> 0.118618).  Saving model ...\n",
      "[ 57/100] train_loss: 0.10779 valid_loss: 0.12243 test_loss: 0.09579 \n",
      "[ 58/100] train_loss: 0.10901 valid_loss: 0.12044 test_loss: 0.09573 \n",
      "[ 59/100] train_loss: 0.10419 valid_loss: 0.11905 test_loss: 0.09463 \n",
      "[ 60/100] train_loss: 0.10352 valid_loss: 0.12307 test_loss: 0.09862 \n",
      "[ 61/100] train_loss: 0.10281 valid_loss: 0.11892 test_loss: 0.09290 \n",
      "[ 62/100] train_loss: 0.10268 valid_loss: 0.11929 test_loss: 0.09173 \n",
      "[ 63/100] train_loss: 0.10462 valid_loss: 0.12144 test_loss: 0.09652 \n",
      "[ 64/100] train_loss: 0.10218 valid_loss: 0.12238 test_loss: 0.09603 \n",
      "[ 65/100] train_loss: 0.10236 valid_loss: 0.11804 test_loss: 0.09698 \n",
      "Validation loss decreased (0.118618 --> 0.118037).  Saving model ...\n",
      "[ 66/100] train_loss: 0.09975 valid_loss: 0.11505 test_loss: 0.09152 \n",
      "Validation loss decreased (0.118037 --> 0.115054).  Saving model ...\n",
      "[ 67/100] train_loss: 0.10149 valid_loss: 0.11959 test_loss: 0.09360 \n",
      "[ 68/100] train_loss: 0.10152 valid_loss: 0.11811 test_loss: 0.09498 \n",
      "[ 69/100] train_loss: 0.10062 valid_loss: 0.11745 test_loss: 0.09160 \n",
      "[ 70/100] train_loss: 0.10271 valid_loss: 0.11549 test_loss: 0.09079 \n",
      "[ 71/100] train_loss: 0.10042 valid_loss: 0.11414 test_loss: 0.09087 \n",
      "Validation loss decreased (0.115054 --> 0.114136).  Saving model ...\n",
      "[ 72/100] train_loss: 0.10186 valid_loss: 0.11571 test_loss: 0.09223 \n",
      "[ 73/100] train_loss: 0.10291 valid_loss: 0.11535 test_loss: 0.08898 \n",
      "[ 74/100] train_loss: 0.09876 valid_loss: 0.11198 test_loss: 0.08940 \n",
      "Validation loss decreased (0.114136 --> 0.111980).  Saving model ...\n",
      "[ 75/100] train_loss: 0.10126 valid_loss: 0.11280 test_loss: 0.08835 \n",
      "[ 76/100] train_loss: 0.09896 valid_loss: 0.11209 test_loss: 0.09102 \n",
      "[ 77/100] train_loss: 0.09967 valid_loss: 0.11143 test_loss: 0.08804 \n",
      "Validation loss decreased (0.111980 --> 0.111425).  Saving model ...\n",
      "[ 78/100] train_loss: 0.09965 valid_loss: 0.11373 test_loss: 0.09102 \n",
      "[ 79/100] train_loss: 0.09691 valid_loss: 0.10995 test_loss: 0.08842 \n",
      "Validation loss decreased (0.111425 --> 0.109952).  Saving model ...\n",
      "[ 80/100] train_loss: 0.09683 valid_loss: 0.11278 test_loss: 0.09120 \n",
      "[ 81/100] train_loss: 0.09844 valid_loss: 0.10973 test_loss: 0.09432 \n",
      "Validation loss decreased (0.109952 --> 0.109726).  Saving model ...\n",
      "[ 82/100] train_loss: 0.09619 valid_loss: 0.11103 test_loss: 0.09127 \n",
      "[ 83/100] train_loss: 0.09968 valid_loss: 0.11239 test_loss: 0.08968 \n",
      "[ 84/100] train_loss: 0.09787 valid_loss: 0.11015 test_loss: 0.08702 \n",
      "[ 85/100] train_loss: 0.09602 valid_loss: 0.11247 test_loss: 0.08869 \n",
      "[ 86/100] train_loss: 0.09818 valid_loss: 0.10990 test_loss: 0.08800 \n",
      "[ 87/100] train_loss: 0.09729 valid_loss: 0.11105 test_loss: 0.08962 \n",
      "[ 88/100] train_loss: 0.09395 valid_loss: 0.11050 test_loss: 0.08638 \n",
      "[ 89/100] train_loss: 0.09607 valid_loss: 0.11231 test_loss: 0.08657 \n",
      "[ 90/100] train_loss: 0.09572 valid_loss: 0.11359 test_loss: 0.09095 \n",
      "[ 91/100] train_loss: 0.09694 valid_loss: 0.10918 test_loss: 0.08635 \n",
      "Validation loss decreased (0.109726 --> 0.109184).  Saving model ...\n",
      "[ 92/100] train_loss: 0.09433 valid_loss: 0.11055 test_loss: 0.08809 \n",
      "[ 93/100] train_loss: 0.09511 valid_loss: 0.11295 test_loss: 0.09012 \n",
      "[ 94/100] train_loss: 0.09437 valid_loss: 0.10910 test_loss: 0.08769 \n",
      "Validation loss decreased (0.109184 --> 0.109102).  Saving model ...\n",
      "[ 95/100] train_loss: 0.09713 valid_loss: 0.10917 test_loss: 0.08786 \n",
      "[ 96/100] train_loss: 0.09575 valid_loss: 0.10773 test_loss: 0.08888 \n",
      "Validation loss decreased (0.109102 --> 0.107733).  Saving model ...\n",
      "[ 97/100] train_loss: 0.09228 valid_loss: 0.10967 test_loss: 0.08685 \n",
      "[ 98/100] train_loss: 0.09495 valid_loss: 0.11137 test_loss: 0.09083 \n",
      "[ 99/100] train_loss: 0.09417 valid_loss: 0.10758 test_loss: 0.08733 \n",
      "Validation loss decreased (0.107733 --> 0.107584).  Saving model ...\n",
      "[100/100] train_loss: 0.09320 valid_loss: 0.10765 test_loss: 0.08691 \n",
      "Model 3 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 4\n",
      "[  1/100] train_loss: 0.55654 valid_loss: 0.45370 test_loss: 0.40989 \n",
      "Validation loss decreased (inf --> 0.453705).  Saving model ...\n",
      "[  2/100] train_loss: 0.37590 valid_loss: 0.33711 test_loss: 0.29005 \n",
      "Validation loss decreased (0.453705 --> 0.337107).  Saving model ...\n",
      "[  3/100] train_loss: 0.29927 valid_loss: 0.28416 test_loss: 0.24117 \n",
      "Validation loss decreased (0.337107 --> 0.284164).  Saving model ...\n",
      "[  4/100] train_loss: 0.25304 valid_loss: 0.25522 test_loss: 0.21014 \n",
      "Validation loss decreased (0.284164 --> 0.255221).  Saving model ...\n",
      "[  5/100] train_loss: 0.22226 valid_loss: 0.23274 test_loss: 0.18508 \n",
      "Validation loss decreased (0.255221 --> 0.232743).  Saving model ...\n",
      "[  6/100] train_loss: 0.20604 valid_loss: 0.21923 test_loss: 0.16931 \n",
      "Validation loss decreased (0.232743 --> 0.219227).  Saving model ...\n",
      "[  7/100] train_loss: 0.19497 valid_loss: 0.20635 test_loss: 0.15717 \n",
      "Validation loss decreased (0.219227 --> 0.206351).  Saving model ...\n",
      "[  8/100] train_loss: 0.18448 valid_loss: 0.19440 test_loss: 0.14442 \n",
      "Validation loss decreased (0.206351 --> 0.194404).  Saving model ...\n",
      "[  9/100] train_loss: 0.17471 valid_loss: 0.19046 test_loss: 0.13814 \n",
      "Validation loss decreased (0.194404 --> 0.190459).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16908 valid_loss: 0.18003 test_loss: 0.13194 \n",
      "Validation loss decreased (0.190459 --> 0.180027).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16303 valid_loss: 0.18290 test_loss: 0.12661 \n",
      "[ 12/100] train_loss: 0.15475 valid_loss: 0.17422 test_loss: 0.12552 \n",
      "Validation loss decreased (0.180027 --> 0.174217).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15362 valid_loss: 0.16748 test_loss: 0.11946 \n",
      "Validation loss decreased (0.174217 --> 0.167475).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14886 valid_loss: 0.17099 test_loss: 0.12073 \n",
      "[ 15/100] train_loss: 0.14300 valid_loss: 0.16224 test_loss: 0.11667 \n",
      "Validation loss decreased (0.167475 --> 0.162242).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14344 valid_loss: 0.15619 test_loss: 0.11464 \n",
      "Validation loss decreased (0.162242 --> 0.156190).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13906 valid_loss: 0.15830 test_loss: 0.11427 \n",
      "[ 18/100] train_loss: 0.13773 valid_loss: 0.15484 test_loss: 0.11139 \n",
      "Validation loss decreased (0.156190 --> 0.154837).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13631 valid_loss: 0.15653 test_loss: 0.11396 \n",
      "[ 20/100] train_loss: 0.13598 valid_loss: 0.14872 test_loss: 0.11032 \n",
      "Validation loss decreased (0.154837 --> 0.148721).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13497 valid_loss: 0.15449 test_loss: 0.11331 \n",
      "[ 22/100] train_loss: 0.13347 valid_loss: 0.14317 test_loss: 0.10649 \n",
      "Validation loss decreased (0.148721 --> 0.143170).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13023 valid_loss: 0.14639 test_loss: 0.10738 \n",
      "[ 24/100] train_loss: 0.12995 valid_loss: 0.14258 test_loss: 0.10674 \n",
      "Validation loss decreased (0.143170 --> 0.142580).  Saving model ...\n",
      "[ 25/100] train_loss: 0.12757 valid_loss: 0.14315 test_loss: 0.10500 \n",
      "[ 26/100] train_loss: 0.12524 valid_loss: 0.14322 test_loss: 0.10530 \n",
      "[ 27/100] train_loss: 0.12334 valid_loss: 0.13769 test_loss: 0.10712 \n",
      "Validation loss decreased (0.142580 --> 0.137689).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12320 valid_loss: 0.14151 test_loss: 0.10787 \n",
      "[ 29/100] train_loss: 0.12130 valid_loss: 0.13187 test_loss: 0.10266 \n",
      "Validation loss decreased (0.137689 --> 0.131871).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12034 valid_loss: 0.13982 test_loss: 0.10450 \n",
      "[ 31/100] train_loss: 0.12257 valid_loss: 0.13236 test_loss: 0.10306 \n",
      "[ 32/100] train_loss: 0.12197 valid_loss: 0.13094 test_loss: 0.10182 \n",
      "Validation loss decreased (0.131871 --> 0.130936).  Saving model ...\n",
      "[ 33/100] train_loss: 0.11607 valid_loss: 0.13058 test_loss: 0.10309 \n",
      "Validation loss decreased (0.130936 --> 0.130578).  Saving model ...\n",
      "[ 34/100] train_loss: 0.11694 valid_loss: 0.13482 test_loss: 0.10175 \n",
      "[ 35/100] train_loss: 0.11792 valid_loss: 0.12905 test_loss: 0.09774 \n",
      "Validation loss decreased (0.130578 --> 0.129049).  Saving model ...\n",
      "[ 36/100] train_loss: 0.11417 valid_loss: 0.12667 test_loss: 0.09759 \n",
      "Validation loss decreased (0.129049 --> 0.126665).  Saving model ...\n",
      "[ 37/100] train_loss: 0.11623 valid_loss: 0.12504 test_loss: 0.09912 \n",
      "Validation loss decreased (0.126665 --> 0.125041).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11488 valid_loss: 0.12674 test_loss: 0.09907 \n",
      "[ 39/100] train_loss: 0.11708 valid_loss: 0.12317 test_loss: 0.09598 \n",
      "Validation loss decreased (0.125041 --> 0.123167).  Saving model ...\n",
      "[ 40/100] train_loss: 0.11423 valid_loss: 0.12635 test_loss: 0.09809 \n",
      "[ 41/100] train_loss: 0.11331 valid_loss: 0.12643 test_loss: 0.09876 \n",
      "[ 42/100] train_loss: 0.11109 valid_loss: 0.13279 test_loss: 0.10289 \n",
      "[ 43/100] train_loss: 0.11114 valid_loss: 0.12498 test_loss: 0.09768 \n",
      "[ 44/100] train_loss: 0.11034 valid_loss: 0.11950 test_loss: 0.09744 \n",
      "Validation loss decreased (0.123167 --> 0.119497).  Saving model ...\n",
      "[ 45/100] train_loss: 0.11037 valid_loss: 0.12055 test_loss: 0.09819 \n",
      "[ 46/100] train_loss: 0.11225 valid_loss: 0.11883 test_loss: 0.09689 \n",
      "Validation loss decreased (0.119497 --> 0.118826).  Saving model ...\n",
      "[ 47/100] train_loss: 0.10862 valid_loss: 0.11980 test_loss: 0.09504 \n",
      "[ 48/100] train_loss: 0.10705 valid_loss: 0.11939 test_loss: 0.09350 \n",
      "[ 49/100] train_loss: 0.10717 valid_loss: 0.11633 test_loss: 0.10038 \n",
      "Validation loss decreased (0.118826 --> 0.116330).  Saving model ...\n",
      "[ 50/100] train_loss: 0.10901 valid_loss: 0.11824 test_loss: 0.09528 \n",
      "[ 51/100] train_loss: 0.10627 valid_loss: 0.11652 test_loss: 0.09899 \n",
      "[ 52/100] train_loss: 0.10390 valid_loss: 0.11556 test_loss: 0.10034 \n",
      "Validation loss decreased (0.116330 --> 0.115565).  Saving model ...\n",
      "[ 53/100] train_loss: 0.10629 valid_loss: 0.11815 test_loss: 0.09387 \n",
      "[ 54/100] train_loss: 0.10581 valid_loss: 0.11779 test_loss: 0.09262 \n",
      "[ 55/100] train_loss: 0.10369 valid_loss: 0.11581 test_loss: 0.09221 \n",
      "[ 56/100] train_loss: 0.10274 valid_loss: 0.11580 test_loss: 0.09171 \n",
      "[ 57/100] train_loss: 0.10407 valid_loss: 0.11683 test_loss: 0.09109 \n",
      "[ 58/100] train_loss: 0.10701 valid_loss: 0.11387 test_loss: 0.09015 \n",
      "Validation loss decreased (0.115565 --> 0.113873).  Saving model ...\n",
      "[ 59/100] train_loss: 0.10442 valid_loss: 0.11735 test_loss: 0.09179 \n",
      "[ 60/100] train_loss: 0.10042 valid_loss: 0.11442 test_loss: 0.09095 \n",
      "[ 61/100] train_loss: 0.10169 valid_loss: 0.11336 test_loss: 0.09022 \n",
      "Validation loss decreased (0.113873 --> 0.113357).  Saving model ...\n",
      "[ 62/100] train_loss: 0.10205 valid_loss: 0.11397 test_loss: 0.09035 \n",
      "[ 63/100] train_loss: 0.09889 valid_loss: 0.11228 test_loss: 0.08842 \n",
      "Validation loss decreased (0.113357 --> 0.112277).  Saving model ...\n",
      "[ 64/100] train_loss: 0.09993 valid_loss: 0.11414 test_loss: 0.09121 \n",
      "[ 65/100] train_loss: 0.10246 valid_loss: 0.11238 test_loss: 0.08937 \n",
      "[ 66/100] train_loss: 0.10178 valid_loss: 0.11222 test_loss: 0.09107 \n",
      "Validation loss decreased (0.112277 --> 0.112217).  Saving model ...\n",
      "[ 67/100] train_loss: 0.09816 valid_loss: 0.11154 test_loss: 0.08957 \n",
      "Validation loss decreased (0.112217 --> 0.111544).  Saving model ...\n",
      "[ 68/100] train_loss: 0.09831 valid_loss: 0.11118 test_loss: 0.08879 \n",
      "Validation loss decreased (0.111544 --> 0.111179).  Saving model ...\n",
      "[ 69/100] train_loss: 0.09760 valid_loss: 0.11132 test_loss: 0.08791 \n",
      "[ 70/100] train_loss: 0.09853 valid_loss: 0.11214 test_loss: 0.09007 \n",
      "[ 71/100] train_loss: 0.09875 valid_loss: 0.11494 test_loss: 0.09146 \n",
      "[ 72/100] train_loss: 0.09700 valid_loss: 0.11346 test_loss: 0.08901 \n",
      "[ 73/100] train_loss: 0.09759 valid_loss: 0.11249 test_loss: 0.08988 \n",
      "[ 74/100] train_loss: 0.09737 valid_loss: 0.11517 test_loss: 0.09428 \n",
      "[ 75/100] train_loss: 0.09853 valid_loss: 0.11096 test_loss: 0.08770 \n",
      "Validation loss decreased (0.111179 --> 0.110965).  Saving model ...\n",
      "[ 76/100] train_loss: 0.09891 valid_loss: 0.11104 test_loss: 0.08924 \n",
      "[ 77/100] train_loss: 0.09673 valid_loss: 0.10894 test_loss: 0.08560 \n",
      "Validation loss decreased (0.110965 --> 0.108941).  Saving model ...\n",
      "[ 78/100] train_loss: 0.09812 valid_loss: 0.10852 test_loss: 0.08572 \n",
      "Validation loss decreased (0.108941 --> 0.108516).  Saving model ...\n",
      "[ 79/100] train_loss: 0.09965 valid_loss: 0.10945 test_loss: 0.08612 \n",
      "[ 80/100] train_loss: 0.09798 valid_loss: 0.10762 test_loss: 0.08397 \n",
      "Validation loss decreased (0.108516 --> 0.107624).  Saving model ...\n",
      "[ 81/100] train_loss: 0.09384 valid_loss: 0.10918 test_loss: 0.08701 \n",
      "[ 82/100] train_loss: 0.09652 valid_loss: 0.10821 test_loss: 0.08569 \n",
      "[ 83/100] train_loss: 0.09624 valid_loss: 0.10915 test_loss: 0.08618 \n",
      "[ 84/100] train_loss: 0.09707 valid_loss: 0.10927 test_loss: 0.08697 \n",
      "[ 85/100] train_loss: 0.09272 valid_loss: 0.10853 test_loss: 0.08795 \n",
      "[ 86/100] train_loss: 0.09238 valid_loss: 0.10617 test_loss: 0.08526 \n",
      "Validation loss decreased (0.107624 --> 0.106173).  Saving model ...\n",
      "[ 87/100] train_loss: 0.09343 valid_loss: 0.11017 test_loss: 0.08701 \n",
      "[ 88/100] train_loss: 0.09374 valid_loss: 0.10767 test_loss: 0.08570 \n",
      "[ 89/100] train_loss: 0.09331 valid_loss: 0.10968 test_loss: 0.08753 \n",
      "[ 90/100] train_loss: 0.09187 valid_loss: 0.10548 test_loss: 0.08440 \n",
      "Validation loss decreased (0.106173 --> 0.105478).  Saving model ...\n",
      "[ 91/100] train_loss: 0.09275 valid_loss: 0.10733 test_loss: 0.08570 \n",
      "[ 92/100] train_loss: 0.09576 valid_loss: 0.10623 test_loss: 0.08496 \n",
      "[ 93/100] train_loss: 0.09411 valid_loss: 0.10778 test_loss: 0.08715 \n",
      "[ 94/100] train_loss: 0.09383 valid_loss: 0.10703 test_loss: 0.08573 \n",
      "[ 95/100] train_loss: 0.09305 valid_loss: 0.10522 test_loss: 0.08368 \n",
      "Validation loss decreased (0.105478 --> 0.105222).  Saving model ...\n",
      "[ 96/100] train_loss: 0.09352 valid_loss: 0.10658 test_loss: 0.08682 \n",
      "[ 97/100] train_loss: 0.09372 valid_loss: 0.10406 test_loss: 0.08258 \n",
      "Validation loss decreased (0.105222 --> 0.104055).  Saving model ...\n",
      "[ 98/100] train_loss: 0.09329 valid_loss: 0.10196 test_loss: 0.08435 \n",
      "Validation loss decreased (0.104055 --> 0.101957).  Saving model ...\n",
      "[ 99/100] train_loss: 0.09153 valid_loss: 0.10431 test_loss: 0.08507 \n",
      "[100/100] train_loss: 0.08996 valid_loss: 0.10542 test_loss: 0.08241 \n",
      "Model 4 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 5\n",
      "[  1/100] train_loss: 0.58899 valid_loss: 0.51611 test_loss: 0.47847 \n",
      "Validation loss decreased (inf --> 0.516115).  Saving model ...\n",
      "[  2/100] train_loss: 0.41092 valid_loss: 0.35300 test_loss: 0.30645 \n",
      "Validation loss decreased (0.516115 --> 0.352995).  Saving model ...\n",
      "[  3/100] train_loss: 0.31345 valid_loss: 0.30114 test_loss: 0.25030 \n",
      "Validation loss decreased (0.352995 --> 0.301136).  Saving model ...\n",
      "[  4/100] train_loss: 0.26204 valid_loss: 0.26610 test_loss: 0.20889 \n",
      "Validation loss decreased (0.301136 --> 0.266097).  Saving model ...\n",
      "[  5/100] train_loss: 0.23196 valid_loss: 0.24257 test_loss: 0.18271 \n",
      "Validation loss decreased (0.266097 --> 0.242568).  Saving model ...\n",
      "[  6/100] train_loss: 0.20907 valid_loss: 0.22268 test_loss: 0.16744 \n",
      "Validation loss decreased (0.242568 --> 0.222684).  Saving model ...\n",
      "[  7/100] train_loss: 0.19350 valid_loss: 0.20119 test_loss: 0.15469 \n",
      "Validation loss decreased (0.222684 --> 0.201187).  Saving model ...\n",
      "[  8/100] train_loss: 0.18354 valid_loss: 0.19627 test_loss: 0.14092 \n",
      "Validation loss decreased (0.201187 --> 0.196268).  Saving model ...\n",
      "[  9/100] train_loss: 0.17340 valid_loss: 0.18444 test_loss: 0.13223 \n",
      "Validation loss decreased (0.196268 --> 0.184442).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16632 valid_loss: 0.17994 test_loss: 0.12882 \n",
      "Validation loss decreased (0.184442 --> 0.179942).  Saving model ...\n",
      "[ 11/100] train_loss: 0.15709 valid_loss: 0.17957 test_loss: 0.12437 \n",
      "Validation loss decreased (0.179942 --> 0.179568).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15777 valid_loss: 0.16653 test_loss: 0.11867 \n",
      "Validation loss decreased (0.179568 --> 0.166528).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15446 valid_loss: 0.17112 test_loss: 0.11916 \n",
      "[ 14/100] train_loss: 0.14877 valid_loss: 0.16083 test_loss: 0.11436 \n",
      "Validation loss decreased (0.166528 --> 0.160828).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14764 valid_loss: 0.16435 test_loss: 0.11687 \n",
      "[ 16/100] train_loss: 0.14071 valid_loss: 0.15477 test_loss: 0.11274 \n",
      "Validation loss decreased (0.160828 --> 0.154771).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13973 valid_loss: 0.15881 test_loss: 0.11354 \n",
      "[ 18/100] train_loss: 0.13838 valid_loss: 0.15544 test_loss: 0.11143 \n",
      "[ 19/100] train_loss: 0.13524 valid_loss: 0.14842 test_loss: 0.10964 \n",
      "Validation loss decreased (0.154771 --> 0.148421).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13283 valid_loss: 0.14878 test_loss: 0.10744 \n",
      "[ 21/100] train_loss: 0.13479 valid_loss: 0.14119 test_loss: 0.10365 \n",
      "Validation loss decreased (0.148421 --> 0.141194).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13192 valid_loss: 0.14560 test_loss: 0.10673 \n",
      "[ 23/100] train_loss: 0.13050 valid_loss: 0.14347 test_loss: 0.10547 \n",
      "[ 24/100] train_loss: 0.12546 valid_loss: 0.14197 test_loss: 0.10686 \n",
      "[ 25/100] train_loss: 0.12868 valid_loss: 0.13854 test_loss: 0.10335 \n",
      "Validation loss decreased (0.141194 --> 0.138537).  Saving model ...\n",
      "[ 26/100] train_loss: 0.12498 valid_loss: 0.13711 test_loss: 0.10398 \n",
      "Validation loss decreased (0.138537 --> 0.137107).  Saving model ...\n",
      "[ 27/100] train_loss: 0.12454 valid_loss: 0.13496 test_loss: 0.10113 \n",
      "Validation loss decreased (0.137107 --> 0.134965).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12258 valid_loss: 0.13376 test_loss: 0.10586 \n",
      "Validation loss decreased (0.134965 --> 0.133762).  Saving model ...\n",
      "[ 29/100] train_loss: 0.11969 valid_loss: 0.13495 test_loss: 0.10110 \n",
      "[ 30/100] train_loss: 0.12033 valid_loss: 0.13212 test_loss: 0.10047 \n",
      "Validation loss decreased (0.133762 --> 0.132120).  Saving model ...\n",
      "[ 31/100] train_loss: 0.11989 valid_loss: 0.12992 test_loss: 0.09915 \n",
      "Validation loss decreased (0.132120 --> 0.129925).  Saving model ...\n",
      "[ 32/100] train_loss: 0.11856 valid_loss: 0.13190 test_loss: 0.10095 \n",
      "[ 33/100] train_loss: 0.11776 valid_loss: 0.13008 test_loss: 0.09782 \n",
      "[ 34/100] train_loss: 0.11661 valid_loss: 0.13112 test_loss: 0.09922 \n",
      "[ 35/100] train_loss: 0.11610 valid_loss: 0.12574 test_loss: 0.09642 \n",
      "Validation loss decreased (0.129925 --> 0.125738).  Saving model ...\n",
      "[ 36/100] train_loss: 0.11989 valid_loss: 0.13061 test_loss: 0.09952 \n",
      "[ 37/100] train_loss: 0.11199 valid_loss: 0.12630 test_loss: 0.09743 \n",
      "[ 38/100] train_loss: 0.11314 valid_loss: 0.12627 test_loss: 0.09682 \n",
      "[ 39/100] train_loss: 0.11116 valid_loss: 0.12727 test_loss: 0.09870 \n",
      "[ 40/100] train_loss: 0.11163 valid_loss: 0.12419 test_loss: 0.09548 \n",
      "Validation loss decreased (0.125738 --> 0.124186).  Saving model ...\n",
      "[ 41/100] train_loss: 0.11431 valid_loss: 0.12416 test_loss: 0.09496 \n",
      "Validation loss decreased (0.124186 --> 0.124162).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11243 valid_loss: 0.12736 test_loss: 0.09616 \n",
      "[ 43/100] train_loss: 0.10982 valid_loss: 0.12553 test_loss: 0.09630 \n",
      "[ 44/100] train_loss: 0.10886 valid_loss: 0.12450 test_loss: 0.09644 \n",
      "[ 45/100] train_loss: 0.10729 valid_loss: 0.12282 test_loss: 0.09413 \n",
      "Validation loss decreased (0.124162 --> 0.122819).  Saving model ...\n",
      "[ 46/100] train_loss: 0.11044 valid_loss: 0.12211 test_loss: 0.09410 \n",
      "Validation loss decreased (0.122819 --> 0.122107).  Saving model ...\n",
      "[ 47/100] train_loss: 0.10893 valid_loss: 0.11980 test_loss: 0.09356 \n",
      "Validation loss decreased (0.122107 --> 0.119797).  Saving model ...\n",
      "[ 48/100] train_loss: 0.10879 valid_loss: 0.12006 test_loss: 0.09205 \n",
      "[ 49/100] train_loss: 0.10863 valid_loss: 0.11997 test_loss: 0.09272 \n",
      "[ 50/100] train_loss: 0.10985 valid_loss: 0.11903 test_loss: 0.09205 \n",
      "Validation loss decreased (0.119797 --> 0.119035).  Saving model ...\n",
      "[ 51/100] train_loss: 0.10598 valid_loss: 0.11888 test_loss: 0.09117 \n",
      "Validation loss decreased (0.119035 --> 0.118882).  Saving model ...\n",
      "[ 52/100] train_loss: 0.10741 valid_loss: 0.12164 test_loss: 0.09272 \n",
      "[ 53/100] train_loss: 0.10707 valid_loss: 0.11823 test_loss: 0.09260 \n",
      "Validation loss decreased (0.118882 --> 0.118232).  Saving model ...\n",
      "[ 54/100] train_loss: 0.10726 valid_loss: 0.12126 test_loss: 0.09394 \n",
      "[ 55/100] train_loss: 0.10316 valid_loss: 0.11804 test_loss: 0.09133 \n",
      "Validation loss decreased (0.118232 --> 0.118041).  Saving model ...\n",
      "[ 56/100] train_loss: 0.10832 valid_loss: 0.12252 test_loss: 0.09634 \n",
      "[ 57/100] train_loss: 0.10437 valid_loss: 0.11870 test_loss: 0.09361 \n",
      "[ 58/100] train_loss: 0.10578 valid_loss: 0.11908 test_loss: 0.09373 \n",
      "[ 59/100] train_loss: 0.10178 valid_loss: 0.11877 test_loss: 0.09195 \n",
      "[ 60/100] train_loss: 0.10907 valid_loss: 0.11601 test_loss: 0.09072 \n",
      "Validation loss decreased (0.118041 --> 0.116010).  Saving model ...\n",
      "[ 61/100] train_loss: 0.10354 valid_loss: 0.11634 test_loss: 0.09150 \n",
      "[ 62/100] train_loss: 0.09899 valid_loss: 0.11577 test_loss: 0.08981 \n",
      "Validation loss decreased (0.116010 --> 0.115770).  Saving model ...\n",
      "[ 63/100] train_loss: 0.10315 valid_loss: 0.11543 test_loss: 0.09148 \n",
      "Validation loss decreased (0.115770 --> 0.115431).  Saving model ...\n",
      "[ 64/100] train_loss: 0.10059 valid_loss: 0.11584 test_loss: 0.09166 \n",
      "[ 65/100] train_loss: 0.09941 valid_loss: 0.11576 test_loss: 0.09013 \n",
      "[ 66/100] train_loss: 0.09876 valid_loss: 0.11401 test_loss: 0.08812 \n",
      "Validation loss decreased (0.115431 --> 0.114007).  Saving model ...\n",
      "[ 67/100] train_loss: 0.10121 valid_loss: 0.11890 test_loss: 0.09249 \n",
      "[ 68/100] train_loss: 0.10069 valid_loss: 0.11534 test_loss: 0.09161 \n",
      "[ 69/100] train_loss: 0.09926 valid_loss: 0.11550 test_loss: 0.09085 \n",
      "[ 70/100] train_loss: 0.09977 valid_loss: 0.11464 test_loss: 0.09062 \n",
      "[ 71/100] train_loss: 0.09994 valid_loss: 0.11412 test_loss: 0.08959 \n",
      "[ 72/100] train_loss: 0.09902 valid_loss: 0.11096 test_loss: 0.08764 \n",
      "Validation loss decreased (0.114007 --> 0.110955).  Saving model ...\n",
      "[ 73/100] train_loss: 0.09642 valid_loss: 0.11732 test_loss: 0.09127 \n",
      "[ 74/100] train_loss: 0.09908 valid_loss: 0.11188 test_loss: 0.08657 \n",
      "[ 75/100] train_loss: 0.09668 valid_loss: 0.11092 test_loss: 0.08612 \n",
      "Validation loss decreased (0.110955 --> 0.110920).  Saving model ...\n",
      "[ 76/100] train_loss: 0.09603 valid_loss: 0.11246 test_loss: 0.08845 \n",
      "[ 77/100] train_loss: 0.09430 valid_loss: 0.11383 test_loss: 0.08784 \n",
      "[ 78/100] train_loss: 0.09257 valid_loss: 0.11001 test_loss: 0.08552 \n",
      "Validation loss decreased (0.110920 --> 0.110013).  Saving model ...\n",
      "[ 79/100] train_loss: 0.09488 valid_loss: 0.11012 test_loss: 0.08547 \n",
      "[ 80/100] train_loss: 0.09536 valid_loss: 0.11132 test_loss: 0.08915 \n",
      "[ 81/100] train_loss: 0.09555 valid_loss: 0.11186 test_loss: 0.08752 \n",
      "[ 82/100] train_loss: 0.09437 valid_loss: 0.11060 test_loss: 0.08741 \n",
      "[ 83/100] train_loss: 0.09396 valid_loss: 0.10963 test_loss: 0.08622 \n",
      "Validation loss decreased (0.110013 --> 0.109633).  Saving model ...\n",
      "[ 84/100] train_loss: 0.09553 valid_loss: 0.11043 test_loss: 0.08756 \n",
      "[ 85/100] train_loss: 0.09503 valid_loss: 0.10971 test_loss: 0.08613 \n",
      "[ 86/100] train_loss: 0.09596 valid_loss: 0.11209 test_loss: 0.08830 \n",
      "[ 87/100] train_loss: 0.09754 valid_loss: 0.11248 test_loss: 0.08843 \n",
      "[ 88/100] train_loss: 0.09614 valid_loss: 0.11120 test_loss: 0.08929 \n",
      "[ 89/100] train_loss: 0.09599 valid_loss: 0.10933 test_loss: 0.08451 \n",
      "Validation loss decreased (0.109633 --> 0.109330).  Saving model ...\n",
      "[ 90/100] train_loss: 0.09476 valid_loss: 0.10871 test_loss: 0.08552 \n",
      "Validation loss decreased (0.109330 --> 0.108712).  Saving model ...\n",
      "[ 91/100] train_loss: 0.09166 valid_loss: 0.10804 test_loss: 0.08243 \n",
      "Validation loss decreased (0.108712 --> 0.108045).  Saving model ...\n",
      "[ 92/100] train_loss: 0.09411 valid_loss: 0.10740 test_loss: 0.08418 \n",
      "Validation loss decreased (0.108045 --> 0.107399).  Saving model ...\n",
      "[ 93/100] train_loss: 0.09678 valid_loss: 0.10950 test_loss: 0.08931 \n",
      "[ 94/100] train_loss: 0.09211 valid_loss: 0.11010 test_loss: 0.08639 \n",
      "[ 95/100] train_loss: 0.09080 valid_loss: 0.11121 test_loss: 0.08558 \n",
      "[ 96/100] train_loss: 0.09241 valid_loss: 0.10944 test_loss: 0.08656 \n",
      "[ 97/100] train_loss: 0.09335 valid_loss: 0.10806 test_loss: 0.08952 \n",
      "[ 98/100] train_loss: 0.09387 valid_loss: 0.10504 test_loss: 0.08507 \n",
      "Validation loss decreased (0.107399 --> 0.105043).  Saving model ...\n",
      "[ 99/100] train_loss: 0.09136 valid_loss: 0.10403 test_loss: 0.08137 \n",
      "Validation loss decreased (0.105043 --> 0.104031).  Saving model ...\n",
      "[100/100] train_loss: 0.09125 valid_loss: 0.10651 test_loss: 0.08392 \n",
      "Model 5 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 6\n",
      "[  1/100] train_loss: 0.68576 valid_loss: 0.60807 test_loss: 0.56445 \n",
      "Validation loss decreased (inf --> 0.608074).  Saving model ...\n",
      "[  2/100] train_loss: 0.51733 valid_loss: 0.45594 test_loss: 0.40260 \n",
      "Validation loss decreased (0.608074 --> 0.455937).  Saving model ...\n",
      "[  3/100] train_loss: 0.39575 valid_loss: 0.36494 test_loss: 0.31604 \n",
      "Validation loss decreased (0.455937 --> 0.364937).  Saving model ...\n",
      "[  4/100] train_loss: 0.32255 valid_loss: 0.31336 test_loss: 0.26343 \n",
      "Validation loss decreased (0.364937 --> 0.313363).  Saving model ...\n",
      "[  5/100] train_loss: 0.27727 valid_loss: 0.27687 test_loss: 0.22728 \n",
      "Validation loss decreased (0.313363 --> 0.276865).  Saving model ...\n",
      "[  6/100] train_loss: 0.24749 valid_loss: 0.24781 test_loss: 0.20076 \n",
      "Validation loss decreased (0.276865 --> 0.247813).  Saving model ...\n",
      "[  7/100] train_loss: 0.22539 valid_loss: 0.23163 test_loss: 0.18167 \n",
      "Validation loss decreased (0.247813 --> 0.231630).  Saving model ...\n",
      "[  8/100] train_loss: 0.20500 valid_loss: 0.21395 test_loss: 0.16624 \n",
      "Validation loss decreased (0.231630 --> 0.213953).  Saving model ...\n",
      "[  9/100] train_loss: 0.19094 valid_loss: 0.20235 test_loss: 0.15080 \n",
      "Validation loss decreased (0.213953 --> 0.202353).  Saving model ...\n",
      "[ 10/100] train_loss: 0.18298 valid_loss: 0.19627 test_loss: 0.14314 \n",
      "Validation loss decreased (0.202353 --> 0.196265).  Saving model ...\n",
      "[ 11/100] train_loss: 0.17377 valid_loss: 0.18533 test_loss: 0.13530 \n",
      "Validation loss decreased (0.196265 --> 0.185334).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16583 valid_loss: 0.18268 test_loss: 0.13250 \n",
      "Validation loss decreased (0.185334 --> 0.182677).  Saving model ...\n",
      "[ 13/100] train_loss: 0.16112 valid_loss: 0.17835 test_loss: 0.12663 \n",
      "Validation loss decreased (0.182677 --> 0.178353).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15843 valid_loss: 0.17468 test_loss: 0.12518 \n",
      "Validation loss decreased (0.178353 --> 0.174685).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15468 valid_loss: 0.17034 test_loss: 0.12070 \n",
      "Validation loss decreased (0.174685 --> 0.170337).  Saving model ...\n",
      "[ 16/100] train_loss: 0.15133 valid_loss: 0.16326 test_loss: 0.11786 \n",
      "Validation loss decreased (0.170337 --> 0.163259).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14514 valid_loss: 0.17267 test_loss: 0.12359 \n",
      "[ 18/100] train_loss: 0.14341 valid_loss: 0.16186 test_loss: 0.11757 \n",
      "Validation loss decreased (0.163259 --> 0.161863).  Saving model ...\n",
      "[ 19/100] train_loss: 0.14078 valid_loss: 0.16181 test_loss: 0.11715 \n",
      "Validation loss decreased (0.161863 --> 0.161811).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13951 valid_loss: 0.16050 test_loss: 0.11484 \n",
      "Validation loss decreased (0.161811 --> 0.160500).  Saving model ...\n",
      "[ 21/100] train_loss: 0.14333 valid_loss: 0.15519 test_loss: 0.11413 \n",
      "Validation loss decreased (0.160500 --> 0.155185).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13726 valid_loss: 0.15083 test_loss: 0.11120 \n",
      "Validation loss decreased (0.155185 --> 0.150833).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13463 valid_loss: 0.14820 test_loss: 0.10994 \n",
      "Validation loss decreased (0.150833 --> 0.148205).  Saving model ...\n",
      "[ 24/100] train_loss: 0.13270 valid_loss: 0.14691 test_loss: 0.11120 \n",
      "Validation loss decreased (0.148205 --> 0.146915).  Saving model ...\n",
      "[ 25/100] train_loss: 0.13175 valid_loss: 0.14751 test_loss: 0.11026 \n",
      "[ 26/100] train_loss: 0.12845 valid_loss: 0.14352 test_loss: 0.10856 \n",
      "Validation loss decreased (0.146915 --> 0.143518).  Saving model ...\n",
      "[ 27/100] train_loss: 0.13009 valid_loss: 0.15012 test_loss: 0.11271 \n",
      "[ 28/100] train_loss: 0.12810 valid_loss: 0.13970 test_loss: 0.10407 \n",
      "Validation loss decreased (0.143518 --> 0.139699).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12467 valid_loss: 0.13752 test_loss: 0.10294 \n",
      "Validation loss decreased (0.139699 --> 0.137521).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12529 valid_loss: 0.14063 test_loss: 0.10577 \n",
      "[ 31/100] train_loss: 0.12474 valid_loss: 0.13863 test_loss: 0.10835 \n",
      "[ 32/100] train_loss: 0.12193 valid_loss: 0.13732 test_loss: 0.10453 \n",
      "Validation loss decreased (0.137521 --> 0.137320).  Saving model ...\n",
      "[ 33/100] train_loss: 0.12375 valid_loss: 0.13158 test_loss: 0.09921 \n",
      "Validation loss decreased (0.137320 --> 0.131576).  Saving model ...\n",
      "[ 34/100] train_loss: 0.12169 valid_loss: 0.13673 test_loss: 0.10440 \n",
      "[ 35/100] train_loss: 0.11842 valid_loss: 0.13783 test_loss: 0.10639 \n",
      "[ 36/100] train_loss: 0.11751 valid_loss: 0.13218 test_loss: 0.10208 \n",
      "[ 37/100] train_loss: 0.11924 valid_loss: 0.12914 test_loss: 0.10116 \n",
      "Validation loss decreased (0.131576 --> 0.129139).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11871 valid_loss: 0.13135 test_loss: 0.10291 \n",
      "[ 39/100] train_loss: 0.11770 valid_loss: 0.13000 test_loss: 0.10130 \n",
      "[ 40/100] train_loss: 0.11628 valid_loss: 0.12679 test_loss: 0.09896 \n",
      "Validation loss decreased (0.129139 --> 0.126787).  Saving model ...\n",
      "[ 41/100] train_loss: 0.11721 valid_loss: 0.12718 test_loss: 0.09789 \n",
      "[ 42/100] train_loss: 0.11329 valid_loss: 0.12904 test_loss: 0.09994 \n",
      "[ 43/100] train_loss: 0.11419 valid_loss: 0.12854 test_loss: 0.10200 \n",
      "[ 44/100] train_loss: 0.11328 valid_loss: 0.13146 test_loss: 0.10478 \n",
      "[ 45/100] train_loss: 0.11414 valid_loss: 0.12497 test_loss: 0.09860 \n",
      "Validation loss decreased (0.126787 --> 0.124969).  Saving model ...\n",
      "[ 46/100] train_loss: 0.11539 valid_loss: 0.12484 test_loss: 0.10066 \n",
      "Validation loss decreased (0.124969 --> 0.124838).  Saving model ...\n",
      "[ 47/100] train_loss: 0.11583 valid_loss: 0.12535 test_loss: 0.09961 \n",
      "[ 48/100] train_loss: 0.11044 valid_loss: 0.12514 test_loss: 0.09871 \n",
      "[ 49/100] train_loss: 0.10926 valid_loss: 0.12427 test_loss: 0.09933 \n",
      "Validation loss decreased (0.124838 --> 0.124269).  Saving model ...\n",
      "[ 50/100] train_loss: 0.11100 valid_loss: 0.12322 test_loss: 0.09973 \n",
      "Validation loss decreased (0.124269 --> 0.123224).  Saving model ...\n",
      "[ 51/100] train_loss: 0.11017 valid_loss: 0.12361 test_loss: 0.10191 \n",
      "[ 52/100] train_loss: 0.10707 valid_loss: 0.12218 test_loss: 0.10050 \n",
      "Validation loss decreased (0.123224 --> 0.122184).  Saving model ...\n",
      "[ 53/100] train_loss: 0.10750 valid_loss: 0.12295 test_loss: 0.09648 \n",
      "[ 54/100] train_loss: 0.10752 valid_loss: 0.12062 test_loss: 0.09880 \n",
      "Validation loss decreased (0.122184 --> 0.120621).  Saving model ...\n",
      "[ 55/100] train_loss: 0.11024 valid_loss: 0.12294 test_loss: 0.09834 \n",
      "[ 56/100] train_loss: 0.10798 valid_loss: 0.12057 test_loss: 0.09778 \n",
      "Validation loss decreased (0.120621 --> 0.120566).  Saving model ...\n",
      "[ 57/100] train_loss: 0.10822 valid_loss: 0.12244 test_loss: 0.10081 \n",
      "[ 58/100] train_loss: 0.10656 valid_loss: 0.11656 test_loss: 0.09422 \n",
      "Validation loss decreased (0.120566 --> 0.116556).  Saving model ...\n",
      "[ 59/100] train_loss: 0.10668 valid_loss: 0.11998 test_loss: 0.09483 \n",
      "[ 60/100] train_loss: 0.10326 valid_loss: 0.11621 test_loss: 0.09669 \n",
      "Validation loss decreased (0.116556 --> 0.116207).  Saving model ...\n",
      "[ 61/100] train_loss: 0.10461 valid_loss: 0.12037 test_loss: 0.09573 \n",
      "[ 62/100] train_loss: 0.10644 valid_loss: 0.11693 test_loss: 0.09661 \n",
      "[ 63/100] train_loss: 0.10566 valid_loss: 0.11975 test_loss: 0.09871 \n",
      "[ 64/100] train_loss: 0.10433 valid_loss: 0.11717 test_loss: 0.09844 \n",
      "[ 65/100] train_loss: 0.10335 valid_loss: 0.11504 test_loss: 0.09365 \n",
      "Validation loss decreased (0.116207 --> 0.115040).  Saving model ...\n",
      "[ 66/100] train_loss: 0.10561 valid_loss: 0.11799 test_loss: 0.09744 \n",
      "[ 67/100] train_loss: 0.10185 valid_loss: 0.11766 test_loss: 0.09477 \n",
      "[ 68/100] train_loss: 0.10428 valid_loss: 0.11823 test_loss: 0.09527 \n",
      "[ 69/100] train_loss: 0.09947 valid_loss: 0.11775 test_loss: 0.09479 \n",
      "[ 70/100] train_loss: 0.10493 valid_loss: 0.11627 test_loss: 0.09545 \n",
      "[ 71/100] train_loss: 0.10306 valid_loss: 0.11719 test_loss: 0.09812 \n",
      "[ 72/100] train_loss: 0.09824 valid_loss: 0.11441 test_loss: 0.09190 \n",
      "Validation loss decreased (0.115040 --> 0.114410).  Saving model ...\n",
      "[ 73/100] train_loss: 0.10299 valid_loss: 0.11914 test_loss: 0.09712 \n",
      "[ 74/100] train_loss: 0.09892 valid_loss: 0.11611 test_loss: 0.09496 \n",
      "[ 75/100] train_loss: 0.10061 valid_loss: 0.11568 test_loss: 0.09521 \n",
      "[ 76/100] train_loss: 0.10015 valid_loss: 0.11611 test_loss: 0.09437 \n",
      "[ 77/100] train_loss: 0.10286 valid_loss: 0.11614 test_loss: 0.09554 \n",
      "[ 78/100] train_loss: 0.10165 valid_loss: 0.11480 test_loss: 0.09454 \n",
      "[ 79/100] train_loss: 0.10142 valid_loss: 0.11187 test_loss: 0.09012 \n",
      "Validation loss decreased (0.114410 --> 0.111875).  Saving model ...\n",
      "[ 80/100] train_loss: 0.10015 valid_loss: 0.11352 test_loss: 0.09317 \n",
      "[ 81/100] train_loss: 0.09768 valid_loss: 0.11289 test_loss: 0.09327 \n",
      "[ 82/100] train_loss: 0.09891 valid_loss: 0.11265 test_loss: 0.09186 \n",
      "[ 83/100] train_loss: 0.10035 valid_loss: 0.11310 test_loss: 0.09247 \n",
      "[ 84/100] train_loss: 0.10082 valid_loss: 0.11616 test_loss: 0.09882 \n",
      "[ 85/100] train_loss: 0.09957 valid_loss: 0.11188 test_loss: 0.09076 \n",
      "[ 86/100] train_loss: 0.09868 valid_loss: 0.11097 test_loss: 0.09339 \n",
      "Validation loss decreased (0.111875 --> 0.110965).  Saving model ...\n",
      "[ 87/100] train_loss: 0.09657 valid_loss: 0.11049 test_loss: 0.09177 \n",
      "Validation loss decreased (0.110965 --> 0.110495).  Saving model ...\n",
      "[ 88/100] train_loss: 0.09470 valid_loss: 0.10886 test_loss: 0.08879 \n",
      "Validation loss decreased (0.110495 --> 0.108858).  Saving model ...\n",
      "[ 89/100] train_loss: 0.09615 valid_loss: 0.11317 test_loss: 0.09346 \n",
      "[ 90/100] train_loss: 0.09973 valid_loss: 0.11939 test_loss: 0.09868 \n",
      "[ 91/100] train_loss: 0.09873 valid_loss: 0.10941 test_loss: 0.08953 \n",
      "[ 92/100] train_loss: 0.09512 valid_loss: 0.10884 test_loss: 0.08947 \n",
      "Validation loss decreased (0.108858 --> 0.108835).  Saving model ...\n",
      "[ 93/100] train_loss: 0.09344 valid_loss: 0.11020 test_loss: 0.09644 \n",
      "[ 94/100] train_loss: 0.09460 valid_loss: 0.10767 test_loss: 0.08947 \n",
      "Validation loss decreased (0.108835 --> 0.107673).  Saving model ...\n",
      "[ 95/100] train_loss: 0.09507 valid_loss: 0.11014 test_loss: 0.09183 \n",
      "[ 96/100] train_loss: 0.09295 valid_loss: 0.10741 test_loss: 0.09122 \n",
      "Validation loss decreased (0.107673 --> 0.107411).  Saving model ...\n",
      "[ 97/100] train_loss: 0.09542 valid_loss: 0.10852 test_loss: 0.08954 \n",
      "[ 98/100] train_loss: 0.09573 valid_loss: 0.11064 test_loss: 0.08742 \n",
      "[ 99/100] train_loss: 0.09244 valid_loss: 0.10908 test_loss: 0.09318 \n",
      "[100/100] train_loss: 0.09207 valid_loss: 0.10886 test_loss: 0.08818 \n",
      "Model 6 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 7\n",
      "[  1/100] train_loss: 0.56880 valid_loss: 0.47530 test_loss: 0.42259 \n",
      "Validation loss decreased (inf --> 0.475301).  Saving model ...\n",
      "[  2/100] train_loss: 0.38984 valid_loss: 0.34609 test_loss: 0.29863 \n",
      "Validation loss decreased (0.475301 --> 0.346088).  Saving model ...\n",
      "[  3/100] train_loss: 0.30735 valid_loss: 0.29374 test_loss: 0.24832 \n",
      "Validation loss decreased (0.346088 --> 0.293737).  Saving model ...\n",
      "[  4/100] train_loss: 0.25908 valid_loss: 0.26169 test_loss: 0.20970 \n",
      "Validation loss decreased (0.293737 --> 0.261690).  Saving model ...\n",
      "[  5/100] train_loss: 0.23173 valid_loss: 0.23751 test_loss: 0.18506 \n",
      "Validation loss decreased (0.261690 --> 0.237512).  Saving model ...\n",
      "[  6/100] train_loss: 0.20881 valid_loss: 0.21728 test_loss: 0.16664 \n",
      "Validation loss decreased (0.237512 --> 0.217279).  Saving model ...\n",
      "[  7/100] train_loss: 0.19251 valid_loss: 0.20668 test_loss: 0.15329 \n",
      "Validation loss decreased (0.217279 --> 0.206684).  Saving model ...\n",
      "[  8/100] train_loss: 0.18169 valid_loss: 0.19871 test_loss: 0.14318 \n",
      "Validation loss decreased (0.206684 --> 0.198706).  Saving model ...\n",
      "[  9/100] train_loss: 0.17550 valid_loss: 0.18838 test_loss: 0.13389 \n",
      "Validation loss decreased (0.198706 --> 0.188377).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16903 valid_loss: 0.17693 test_loss: 0.12787 \n",
      "Validation loss decreased (0.188377 --> 0.176935).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16079 valid_loss: 0.17741 test_loss: 0.12618 \n",
      "[ 12/100] train_loss: 0.15834 valid_loss: 0.16905 test_loss: 0.12100 \n",
      "Validation loss decreased (0.176935 --> 0.169048).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15516 valid_loss: 0.17046 test_loss: 0.12139 \n",
      "[ 14/100] train_loss: 0.15040 valid_loss: 0.16341 test_loss: 0.11648 \n",
      "Validation loss decreased (0.169048 --> 0.163410).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15059 valid_loss: 0.16343 test_loss: 0.11705 \n",
      "[ 16/100] train_loss: 0.14282 valid_loss: 0.15978 test_loss: 0.11390 \n",
      "Validation loss decreased (0.163410 --> 0.159779).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14447 valid_loss: 0.16229 test_loss: 0.11537 \n",
      "[ 18/100] train_loss: 0.13817 valid_loss: 0.15334 test_loss: 0.11009 \n",
      "Validation loss decreased (0.159779 --> 0.153340).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13804 valid_loss: 0.15341 test_loss: 0.11144 \n",
      "[ 20/100] train_loss: 0.13195 valid_loss: 0.15308 test_loss: 0.11029 \n",
      "Validation loss decreased (0.153340 --> 0.153085).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13518 valid_loss: 0.15028 test_loss: 0.10973 \n",
      "Validation loss decreased (0.153085 --> 0.150277).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13428 valid_loss: 0.14979 test_loss: 0.10938 \n",
      "Validation loss decreased (0.150277 --> 0.149786).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13266 valid_loss: 0.14640 test_loss: 0.10579 \n",
      "Validation loss decreased (0.149786 --> 0.146395).  Saving model ...\n",
      "[ 24/100] train_loss: 0.12695 valid_loss: 0.14369 test_loss: 0.10660 \n",
      "Validation loss decreased (0.146395 --> 0.143688).  Saving model ...\n",
      "[ 25/100] train_loss: 0.13047 valid_loss: 0.13994 test_loss: 0.10427 \n",
      "Validation loss decreased (0.143688 --> 0.139941).  Saving model ...\n",
      "[ 26/100] train_loss: 0.12942 valid_loss: 0.14236 test_loss: 0.10600 \n",
      "[ 27/100] train_loss: 0.12727 valid_loss: 0.14349 test_loss: 0.10893 \n",
      "[ 28/100] train_loss: 0.12396 valid_loss: 0.13866 test_loss: 0.10325 \n",
      "Validation loss decreased (0.139941 --> 0.138660).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12263 valid_loss: 0.13524 test_loss: 0.10143 \n",
      "Validation loss decreased (0.138660 --> 0.135242).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12005 valid_loss: 0.13575 test_loss: 0.10274 \n",
      "[ 31/100] train_loss: 0.12154 valid_loss: 0.13024 test_loss: 0.09965 \n",
      "Validation loss decreased (0.135242 --> 0.130241).  Saving model ...\n",
      "[ 32/100] train_loss: 0.11928 valid_loss: 0.13508 test_loss: 0.10178 \n",
      "[ 33/100] train_loss: 0.12010 valid_loss: 0.14096 test_loss: 0.10840 \n",
      "[ 34/100] train_loss: 0.11809 valid_loss: 0.13463 test_loss: 0.10355 \n",
      "[ 35/100] train_loss: 0.11935 valid_loss: 0.13112 test_loss: 0.10056 \n",
      "[ 36/100] train_loss: 0.11677 valid_loss: 0.13175 test_loss: 0.10173 \n",
      "[ 37/100] train_loss: 0.11181 valid_loss: 0.13053 test_loss: 0.10024 \n",
      "[ 38/100] train_loss: 0.11592 valid_loss: 0.12802 test_loss: 0.09724 \n",
      "Validation loss decreased (0.130241 --> 0.128025).  Saving model ...\n",
      "[ 39/100] train_loss: 0.11684 valid_loss: 0.12786 test_loss: 0.09894 \n",
      "Validation loss decreased (0.128025 --> 0.127859).  Saving model ...\n",
      "[ 40/100] train_loss: 0.11125 valid_loss: 0.12847 test_loss: 0.09982 \n",
      "[ 41/100] train_loss: 0.11071 valid_loss: 0.12689 test_loss: 0.09589 \n",
      "Validation loss decreased (0.127859 --> 0.126890).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11361 valid_loss: 0.12479 test_loss: 0.09488 \n",
      "Validation loss decreased (0.126890 --> 0.124789).  Saving model ...\n",
      "[ 43/100] train_loss: 0.11599 valid_loss: 0.12430 test_loss: 0.09586 \n",
      "Validation loss decreased (0.124789 --> 0.124305).  Saving model ...\n",
      "[ 44/100] train_loss: 0.11231 valid_loss: 0.12555 test_loss: 0.09663 \n",
      "[ 45/100] train_loss: 0.11465 valid_loss: 0.12494 test_loss: 0.09664 \n",
      "[ 46/100] train_loss: 0.11064 valid_loss: 0.12466 test_loss: 0.09862 \n",
      "[ 47/100] train_loss: 0.10964 valid_loss: 0.12326 test_loss: 0.09573 \n",
      "Validation loss decreased (0.124305 --> 0.123264).  Saving model ...\n",
      "[ 48/100] train_loss: 0.10807 valid_loss: 0.12352 test_loss: 0.09466 \n",
      "[ 49/100] train_loss: 0.11032 valid_loss: 0.12459 test_loss: 0.09422 \n",
      "[ 50/100] train_loss: 0.11346 valid_loss: 0.12321 test_loss: 0.09315 \n",
      "Validation loss decreased (0.123264 --> 0.123206).  Saving model ...\n",
      "[ 51/100] train_loss: 0.10648 valid_loss: 0.12027 test_loss: 0.09145 \n",
      "Validation loss decreased (0.123206 --> 0.120268).  Saving model ...\n",
      "[ 52/100] train_loss: 0.10694 valid_loss: 0.12442 test_loss: 0.09457 \n",
      "[ 53/100] train_loss: 0.10459 valid_loss: 0.11570 test_loss: 0.08861 \n",
      "Validation loss decreased (0.120268 --> 0.115699).  Saving model ...\n",
      "[ 54/100] train_loss: 0.10745 valid_loss: 0.12216 test_loss: 0.09506 \n",
      "[ 55/100] train_loss: 0.10595 valid_loss: 0.12097 test_loss: 0.09464 \n",
      "[ 56/100] train_loss: 0.10415 valid_loss: 0.12001 test_loss: 0.09316 \n",
      "[ 57/100] train_loss: 0.10570 valid_loss: 0.11913 test_loss: 0.09082 \n",
      "[ 58/100] train_loss: 0.09959 valid_loss: 0.12042 test_loss: 0.09392 \n",
      "[ 59/100] train_loss: 0.10836 valid_loss: 0.11645 test_loss: 0.09245 \n",
      "[ 60/100] train_loss: 0.10612 valid_loss: 0.11330 test_loss: 0.08849 \n",
      "Validation loss decreased (0.115699 --> 0.113299).  Saving model ...\n",
      "[ 61/100] train_loss: 0.10161 valid_loss: 0.11753 test_loss: 0.09121 \n",
      "[ 62/100] train_loss: 0.10289 valid_loss: 0.11521 test_loss: 0.09002 \n",
      "[ 63/100] train_loss: 0.10102 valid_loss: 0.11764 test_loss: 0.09382 \n",
      "[ 64/100] train_loss: 0.10278 valid_loss: 0.11573 test_loss: 0.09041 \n",
      "[ 65/100] train_loss: 0.10409 valid_loss: 0.11460 test_loss: 0.08924 \n",
      "[ 66/100] train_loss: 0.10024 valid_loss: 0.11212 test_loss: 0.08710 \n",
      "Validation loss decreased (0.113299 --> 0.112120).  Saving model ...\n",
      "[ 67/100] train_loss: 0.10056 valid_loss: 0.11601 test_loss: 0.08953 \n",
      "[ 68/100] train_loss: 0.10020 valid_loss: 0.11283 test_loss: 0.08827 \n",
      "[ 69/100] train_loss: 0.10060 valid_loss: 0.11247 test_loss: 0.08819 \n",
      "[ 70/100] train_loss: 0.10029 valid_loss: 0.11048 test_loss: 0.08737 \n",
      "Validation loss decreased (0.112120 --> 0.110483).  Saving model ...\n",
      "[ 71/100] train_loss: 0.09865 valid_loss: 0.11200 test_loss: 0.08792 \n",
      "[ 72/100] train_loss: 0.09877 valid_loss: 0.11450 test_loss: 0.08863 \n",
      "[ 73/100] train_loss: 0.10019 valid_loss: 0.11299 test_loss: 0.08951 \n",
      "[ 74/100] train_loss: 0.10060 valid_loss: 0.11367 test_loss: 0.08950 \n",
      "[ 75/100] train_loss: 0.09803 valid_loss: 0.11306 test_loss: 0.09102 \n",
      "[ 76/100] train_loss: 0.10004 valid_loss: 0.11296 test_loss: 0.08873 \n",
      "[ 77/100] train_loss: 0.09835 valid_loss: 0.11344 test_loss: 0.08755 \n",
      "[ 78/100] train_loss: 0.09640 valid_loss: 0.11256 test_loss: 0.08724 \n",
      "[ 79/100] train_loss: 0.09880 valid_loss: 0.11211 test_loss: 0.09107 \n",
      "[ 80/100] train_loss: 0.09678 valid_loss: 0.11093 test_loss: 0.08704 \n",
      "[ 81/100] train_loss: 0.09866 valid_loss: 0.11258 test_loss: 0.08755 \n",
      "[ 82/100] train_loss: 0.09949 valid_loss: 0.11217 test_loss: 0.08911 \n",
      "[ 83/100] train_loss: 0.09787 valid_loss: 0.11069 test_loss: 0.08777 \n",
      "[ 84/100] train_loss: 0.09810 valid_loss: 0.10916 test_loss: 0.08554 \n",
      "Validation loss decreased (0.110483 --> 0.109159).  Saving model ...\n",
      "[ 85/100] train_loss: 0.09391 valid_loss: 0.11017 test_loss: 0.08712 \n",
      "[ 86/100] train_loss: 0.09662 valid_loss: 0.11025 test_loss: 0.08943 \n",
      "[ 87/100] train_loss: 0.09906 valid_loss: 0.10639 test_loss: 0.08292 \n",
      "Validation loss decreased (0.109159 --> 0.106387).  Saving model ...\n",
      "[ 88/100] train_loss: 0.09472 valid_loss: 0.10757 test_loss: 0.08385 \n",
      "[ 89/100] train_loss: 0.09550 valid_loss: 0.10743 test_loss: 0.08583 \n",
      "[ 90/100] train_loss: 0.09608 valid_loss: 0.11242 test_loss: 0.08662 \n",
      "[ 91/100] train_loss: 0.09410 valid_loss: 0.10987 test_loss: 0.08712 \n",
      "[ 92/100] train_loss: 0.09452 valid_loss: 0.10882 test_loss: 0.08498 \n",
      "[ 93/100] train_loss: 0.09226 valid_loss: 0.11118 test_loss: 0.08786 \n",
      "[ 94/100] train_loss: 0.09644 valid_loss: 0.10479 test_loss: 0.08502 \n",
      "Validation loss decreased (0.106387 --> 0.104785).  Saving model ...\n",
      "[ 95/100] train_loss: 0.09510 valid_loss: 0.10445 test_loss: 0.08419 \n",
      "Validation loss decreased (0.104785 --> 0.104453).  Saving model ...\n",
      "[ 96/100] train_loss: 0.09385 valid_loss: 0.10795 test_loss: 0.08501 \n",
      "[ 97/100] train_loss: 0.09290 valid_loss: 0.10873 test_loss: 0.08624 \n",
      "[ 98/100] train_loss: 0.09245 valid_loss: 0.10665 test_loss: 0.08499 \n",
      "[ 99/100] train_loss: 0.09388 valid_loss: 0.10727 test_loss: 0.08550 \n",
      "[100/100] train_loss: 0.09487 valid_loss: 0.10812 test_loss: 0.08564 \n",
      "Model 7 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 8\n",
      "[  1/100] train_loss: 0.61604 valid_loss: 0.51358 test_loss: 0.49600 \n",
      "Validation loss decreased (inf --> 0.513580).  Saving model ...\n",
      "[  2/100] train_loss: 0.42918 valid_loss: 0.37171 test_loss: 0.32748 \n",
      "Validation loss decreased (0.513580 --> 0.371711).  Saving model ...\n",
      "[  3/100] train_loss: 0.32278 valid_loss: 0.30651 test_loss: 0.25890 \n",
      "Validation loss decreased (0.371711 --> 0.306509).  Saving model ...\n",
      "[  4/100] train_loss: 0.26781 valid_loss: 0.26731 test_loss: 0.21708 \n",
      "Validation loss decreased (0.306509 --> 0.267312).  Saving model ...\n",
      "[  5/100] train_loss: 0.23280 valid_loss: 0.23885 test_loss: 0.18697 \n",
      "Validation loss decreased (0.267312 --> 0.238847).  Saving model ...\n",
      "[  6/100] train_loss: 0.20823 valid_loss: 0.21612 test_loss: 0.16962 \n",
      "Validation loss decreased (0.238847 --> 0.216123).  Saving model ...\n",
      "[  7/100] train_loss: 0.19661 valid_loss: 0.20574 test_loss: 0.15773 \n",
      "Validation loss decreased (0.216123 --> 0.205735).  Saving model ...\n",
      "[  8/100] train_loss: 0.18395 valid_loss: 0.19350 test_loss: 0.14740 \n",
      "Validation loss decreased (0.205735 --> 0.193500).  Saving model ...\n",
      "[  9/100] train_loss: 0.17186 valid_loss: 0.18406 test_loss: 0.13532 \n",
      "Validation loss decreased (0.193500 --> 0.184058).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16755 valid_loss: 0.18012 test_loss: 0.13515 \n",
      "Validation loss decreased (0.184058 --> 0.180117).  Saving model ...\n",
      "[ 11/100] train_loss: 0.15926 valid_loss: 0.17580 test_loss: 0.12963 \n",
      "Validation loss decreased (0.180117 --> 0.175798).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15696 valid_loss: 0.16868 test_loss: 0.12322 \n",
      "Validation loss decreased (0.175798 --> 0.168680).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15085 valid_loss: 0.16780 test_loss: 0.12107 \n",
      "Validation loss decreased (0.168680 --> 0.167801).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15179 valid_loss: 0.16016 test_loss: 0.11758 \n",
      "Validation loss decreased (0.167801 --> 0.160163).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15057 valid_loss: 0.16557 test_loss: 0.11892 \n",
      "[ 16/100] train_loss: 0.14573 valid_loss: 0.15939 test_loss: 0.11537 \n",
      "Validation loss decreased (0.160163 --> 0.159388).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14393 valid_loss: 0.15914 test_loss: 0.11512 \n",
      "Validation loss decreased (0.159388 --> 0.159142).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13692 valid_loss: 0.16060 test_loss: 0.11600 \n",
      "[ 19/100] train_loss: 0.14119 valid_loss: 0.15537 test_loss: 0.11365 \n",
      "Validation loss decreased (0.159142 --> 0.155374).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13724 valid_loss: 0.15220 test_loss: 0.11122 \n",
      "Validation loss decreased (0.155374 --> 0.152198).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13629 valid_loss: 0.14754 test_loss: 0.11080 \n",
      "Validation loss decreased (0.152198 --> 0.147543).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13239 valid_loss: 0.14554 test_loss: 0.10687 \n",
      "Validation loss decreased (0.147543 --> 0.145540).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13073 valid_loss: 0.14593 test_loss: 0.10901 \n",
      "[ 24/100] train_loss: 0.12721 valid_loss: 0.14424 test_loss: 0.10561 \n",
      "Validation loss decreased (0.145540 --> 0.144240).  Saving model ...\n",
      "[ 25/100] train_loss: 0.12904 valid_loss: 0.14827 test_loss: 0.10875 \n",
      "[ 26/100] train_loss: 0.12329 valid_loss: 0.13761 test_loss: 0.10377 \n",
      "Validation loss decreased (0.144240 --> 0.137614).  Saving model ...\n",
      "[ 27/100] train_loss: 0.12511 valid_loss: 0.13804 test_loss: 0.10242 \n",
      "[ 28/100] train_loss: 0.12339 valid_loss: 0.13930 test_loss: 0.10485 \n",
      "[ 29/100] train_loss: 0.12573 valid_loss: 0.13734 test_loss: 0.10303 \n",
      "Validation loss decreased (0.137614 --> 0.137336).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12347 valid_loss: 0.13499 test_loss: 0.10401 \n",
      "Validation loss decreased (0.137336 --> 0.134994).  Saving model ...\n",
      "[ 31/100] train_loss: 0.11773 valid_loss: 0.13706 test_loss: 0.10572 \n",
      "[ 32/100] train_loss: 0.12051 valid_loss: 0.13331 test_loss: 0.10186 \n",
      "Validation loss decreased (0.134994 --> 0.133307).  Saving model ...\n",
      "[ 33/100] train_loss: 0.11790 valid_loss: 0.13499 test_loss: 0.10285 \n",
      "[ 34/100] train_loss: 0.11927 valid_loss: 0.13358 test_loss: 0.10258 \n",
      "[ 35/100] train_loss: 0.11606 valid_loss: 0.13139 test_loss: 0.09914 \n",
      "Validation loss decreased (0.133307 --> 0.131389).  Saving model ...\n",
      "[ 36/100] train_loss: 0.12243 valid_loss: 0.13469 test_loss: 0.09977 \n",
      "[ 37/100] train_loss: 0.11803 valid_loss: 0.12898 test_loss: 0.09913 \n",
      "Validation loss decreased (0.131389 --> 0.128981).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11590 valid_loss: 0.12442 test_loss: 0.09897 \n",
      "Validation loss decreased (0.128981 --> 0.124422).  Saving model ...\n",
      "[ 39/100] train_loss: 0.11285 valid_loss: 0.12970 test_loss: 0.09801 \n",
      "[ 40/100] train_loss: 0.11730 valid_loss: 0.12611 test_loss: 0.10021 \n",
      "[ 41/100] train_loss: 0.11094 valid_loss: 0.12680 test_loss: 0.09804 \n",
      "[ 42/100] train_loss: 0.11233 valid_loss: 0.12491 test_loss: 0.09890 \n",
      "[ 43/100] train_loss: 0.11263 valid_loss: 0.12474 test_loss: 0.10280 \n",
      "[ 44/100] train_loss: 0.11270 valid_loss: 0.12521 test_loss: 0.09611 \n",
      "[ 45/100] train_loss: 0.11103 valid_loss: 0.12537 test_loss: 0.09808 \n",
      "[ 46/100] train_loss: 0.11032 valid_loss: 0.12503 test_loss: 0.09722 \n",
      "[ 47/100] train_loss: 0.11286 valid_loss: 0.12146 test_loss: 0.09530 \n",
      "Validation loss decreased (0.124422 --> 0.121459).  Saving model ...\n",
      "[ 48/100] train_loss: 0.11049 valid_loss: 0.12158 test_loss: 0.09434 \n",
      "[ 49/100] train_loss: 0.10908 valid_loss: 0.11998 test_loss: 0.09776 \n",
      "Validation loss decreased (0.121459 --> 0.119985).  Saving model ...\n",
      "[ 50/100] train_loss: 0.10908 valid_loss: 0.12118 test_loss: 0.09414 \n",
      "[ 51/100] train_loss: 0.11177 valid_loss: 0.12129 test_loss: 0.09369 \n",
      "[ 52/100] train_loss: 0.10679 valid_loss: 0.12252 test_loss: 0.09377 \n",
      "[ 53/100] train_loss: 0.10973 valid_loss: 0.11855 test_loss: 0.09313 \n",
      "Validation loss decreased (0.119985 --> 0.118546).  Saving model ...\n",
      "[ 54/100] train_loss: 0.10485 valid_loss: 0.11739 test_loss: 0.09265 \n",
      "Validation loss decreased (0.118546 --> 0.117387).  Saving model ...\n",
      "[ 55/100] train_loss: 0.10327 valid_loss: 0.12063 test_loss: 0.09280 \n",
      "[ 56/100] train_loss: 0.10775 valid_loss: 0.12279 test_loss: 0.09610 \n",
      "[ 57/100] train_loss: 0.10400 valid_loss: 0.11730 test_loss: 0.09151 \n",
      "Validation loss decreased (0.117387 --> 0.117296).  Saving model ...\n",
      "[ 58/100] train_loss: 0.10404 valid_loss: 0.11817 test_loss: 0.09284 \n",
      "[ 59/100] train_loss: 0.10765 valid_loss: 0.12082 test_loss: 0.09564 \n",
      "[ 60/100] train_loss: 0.10560 valid_loss: 0.11549 test_loss: 0.09120 \n",
      "Validation loss decreased (0.117296 --> 0.115491).  Saving model ...\n",
      "[ 61/100] train_loss: 0.10292 valid_loss: 0.11617 test_loss: 0.09102 \n",
      "[ 62/100] train_loss: 0.10414 valid_loss: 0.11896 test_loss: 0.09302 \n",
      "[ 63/100] train_loss: 0.10109 valid_loss: 0.11933 test_loss: 0.09447 \n",
      "[ 64/100] train_loss: 0.10184 valid_loss: 0.12014 test_loss: 0.09393 \n",
      "[ 65/100] train_loss: 0.10144 valid_loss: 0.11419 test_loss: 0.08843 \n",
      "Validation loss decreased (0.115491 --> 0.114194).  Saving model ...\n",
      "[ 66/100] train_loss: 0.10162 valid_loss: 0.11500 test_loss: 0.09002 \n",
      "[ 67/100] train_loss: 0.10293 valid_loss: 0.11268 test_loss: 0.08983 \n",
      "Validation loss decreased (0.114194 --> 0.112683).  Saving model ...\n",
      "[ 68/100] train_loss: 0.10254 valid_loss: 0.11312 test_loss: 0.08990 \n",
      "[ 69/100] train_loss: 0.10217 valid_loss: 0.11587 test_loss: 0.09056 \n",
      "[ 70/100] train_loss: 0.10030 valid_loss: 0.11294 test_loss: 0.09162 \n",
      "[ 71/100] train_loss: 0.10179 valid_loss: 0.11548 test_loss: 0.09000 \n",
      "[ 72/100] train_loss: 0.09948 valid_loss: 0.11328 test_loss: 0.08943 \n",
      "[ 73/100] train_loss: 0.10010 valid_loss: 0.11501 test_loss: 0.09318 \n",
      "[ 74/100] train_loss: 0.09823 valid_loss: 0.11451 test_loss: 0.09145 \n",
      "[ 75/100] train_loss: 0.09985 valid_loss: 0.11505 test_loss: 0.09155 \n",
      "[ 76/100] train_loss: 0.09702 valid_loss: 0.11332 test_loss: 0.09162 \n",
      "[ 77/100] train_loss: 0.09807 valid_loss: 0.10974 test_loss: 0.08972 \n",
      "Validation loss decreased (0.112683 --> 0.109744).  Saving model ...\n",
      "[ 78/100] train_loss: 0.09989 valid_loss: 0.11334 test_loss: 0.09100 \n",
      "[ 79/100] train_loss: 0.09927 valid_loss: 0.11179 test_loss: 0.08852 \n",
      "[ 80/100] train_loss: 0.10088 valid_loss: 0.11020 test_loss: 0.08708 \n",
      "[ 81/100] train_loss: 0.09958 valid_loss: 0.11034 test_loss: 0.08699 \n",
      "[ 82/100] train_loss: 0.09898 valid_loss: 0.11090 test_loss: 0.08710 \n",
      "[ 83/100] train_loss: 0.09562 valid_loss: 0.11067 test_loss: 0.08820 \n",
      "[ 84/100] train_loss: 0.09814 valid_loss: 0.10823 test_loss: 0.08494 \n",
      "Validation loss decreased (0.109744 --> 0.108234).  Saving model ...\n",
      "[ 85/100] train_loss: 0.09831 valid_loss: 0.11076 test_loss: 0.08972 \n",
      "[ 86/100] train_loss: 0.09978 valid_loss: 0.10685 test_loss: 0.08572 \n",
      "Validation loss decreased (0.108234 --> 0.106851).  Saving model ...\n",
      "[ 87/100] train_loss: 0.09630 valid_loss: 0.10762 test_loss: 0.08748 \n",
      "[ 88/100] train_loss: 0.09745 valid_loss: 0.10808 test_loss: 0.08836 \n",
      "[ 89/100] train_loss: 0.09250 valid_loss: 0.11060 test_loss: 0.08797 \n",
      "[ 90/100] train_loss: 0.09445 valid_loss: 0.10859 test_loss: 0.08702 \n",
      "[ 91/100] train_loss: 0.09402 valid_loss: 0.11244 test_loss: 0.08918 \n",
      "[ 92/100] train_loss: 0.09421 valid_loss: 0.10878 test_loss: 0.08908 \n",
      "[ 93/100] train_loss: 0.09368 valid_loss: 0.10671 test_loss: 0.08629 \n",
      "Validation loss decreased (0.106851 --> 0.106706).  Saving model ...\n",
      "[ 94/100] train_loss: 0.09417 valid_loss: 0.11085 test_loss: 0.09094 \n",
      "[ 95/100] train_loss: 0.09398 valid_loss: 0.10563 test_loss: 0.08753 \n",
      "Validation loss decreased (0.106706 --> 0.105625).  Saving model ...\n",
      "[ 96/100] train_loss: 0.09324 valid_loss: 0.10747 test_loss: 0.08670 \n",
      "[ 97/100] train_loss: 0.09365 valid_loss: 0.10640 test_loss: 0.08584 \n",
      "[ 98/100] train_loss: 0.09401 valid_loss: 0.10555 test_loss: 0.08835 \n",
      "Validation loss decreased (0.105625 --> 0.105550).  Saving model ...\n",
      "[ 99/100] train_loss: 0.09450 valid_loss: 0.10455 test_loss: 0.08618 \n",
      "Validation loss decreased (0.105550 --> 0.104550).  Saving model ...\n",
      "[100/100] train_loss: 0.09255 valid_loss: 0.10390 test_loss: 0.08519 \n",
      "Validation loss decreased (0.104550 --> 0.103896).  Saving model ...\n",
      "Model 8 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 9\n",
      "[  1/100] train_loss: 0.59322 valid_loss: 0.48695 test_loss: 0.45881 \n",
      "Validation loss decreased (inf --> 0.486952).  Saving model ...\n",
      "[  2/100] train_loss: 0.39940 valid_loss: 0.34818 test_loss: 0.30252 \n",
      "Validation loss decreased (0.486952 --> 0.348181).  Saving model ...\n",
      "[  3/100] train_loss: 0.31053 valid_loss: 0.29721 test_loss: 0.25013 \n",
      "Validation loss decreased (0.348181 --> 0.297209).  Saving model ...\n",
      "[  4/100] train_loss: 0.26255 valid_loss: 0.26401 test_loss: 0.21474 \n",
      "Validation loss decreased (0.297209 --> 0.264014).  Saving model ...\n",
      "[  5/100] train_loss: 0.23230 valid_loss: 0.24002 test_loss: 0.18901 \n",
      "Validation loss decreased (0.264014 --> 0.240017).  Saving model ...\n",
      "[  6/100] train_loss: 0.20945 valid_loss: 0.21944 test_loss: 0.16966 \n",
      "Validation loss decreased (0.240017 --> 0.219440).  Saving model ...\n",
      "[  7/100] train_loss: 0.19225 valid_loss: 0.20553 test_loss: 0.15529 \n",
      "Validation loss decreased (0.219440 --> 0.205529).  Saving model ...\n",
      "[  8/100] train_loss: 0.18246 valid_loss: 0.19976 test_loss: 0.14822 \n",
      "Validation loss decreased (0.205529 --> 0.199756).  Saving model ...\n",
      "[  9/100] train_loss: 0.17161 valid_loss: 0.18657 test_loss: 0.13757 \n",
      "Validation loss decreased (0.199756 --> 0.186570).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16327 valid_loss: 0.17991 test_loss: 0.12965 \n",
      "Validation loss decreased (0.186570 --> 0.179914).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16342 valid_loss: 0.17808 test_loss: 0.12829 \n",
      "Validation loss decreased (0.179914 --> 0.178083).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15379 valid_loss: 0.16970 test_loss: 0.12153 \n",
      "Validation loss decreased (0.178083 --> 0.169699).  Saving model ...\n",
      "[ 13/100] train_loss: 0.14938 valid_loss: 0.16927 test_loss: 0.12150 \n",
      "Validation loss decreased (0.169699 --> 0.169267).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15078 valid_loss: 0.16779 test_loss: 0.11900 \n",
      "Validation loss decreased (0.169267 --> 0.167792).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14485 valid_loss: 0.16000 test_loss: 0.11390 \n",
      "Validation loss decreased (0.167792 --> 0.159995).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14202 valid_loss: 0.16497 test_loss: 0.11513 \n",
      "[ 17/100] train_loss: 0.14241 valid_loss: 0.16148 test_loss: 0.11511 \n",
      "[ 18/100] train_loss: 0.13757 valid_loss: 0.15510 test_loss: 0.11104 \n",
      "Validation loss decreased (0.159995 --> 0.155100).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13723 valid_loss: 0.15634 test_loss: 0.11278 \n",
      "[ 20/100] train_loss: 0.13844 valid_loss: 0.15419 test_loss: 0.11116 \n",
      "Validation loss decreased (0.155100 --> 0.154186).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13222 valid_loss: 0.15478 test_loss: 0.11064 \n",
      "[ 22/100] train_loss: 0.13097 valid_loss: 0.14579 test_loss: 0.10480 \n",
      "Validation loss decreased (0.154186 --> 0.145793).  Saving model ...\n",
      "[ 23/100] train_loss: 0.12588 valid_loss: 0.15012 test_loss: 0.10863 \n",
      "[ 24/100] train_loss: 0.12765 valid_loss: 0.14176 test_loss: 0.10600 \n",
      "Validation loss decreased (0.145793 --> 0.141758).  Saving model ...\n",
      "[ 25/100] train_loss: 0.12799 valid_loss: 0.14310 test_loss: 0.10468 \n",
      "[ 26/100] train_loss: 0.12680 valid_loss: 0.14613 test_loss: 0.10575 \n",
      "[ 27/100] train_loss: 0.12727 valid_loss: 0.13930 test_loss: 0.10391 \n",
      "Validation loss decreased (0.141758 --> 0.139298).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12197 valid_loss: 0.13632 test_loss: 0.10052 \n",
      "Validation loss decreased (0.139298 --> 0.136325).  Saving model ...\n",
      "[ 29/100] train_loss: 0.11834 valid_loss: 0.13666 test_loss: 0.10325 \n",
      "[ 30/100] train_loss: 0.12070 valid_loss: 0.13813 test_loss: 0.10188 \n",
      "[ 31/100] train_loss: 0.12189 valid_loss: 0.13444 test_loss: 0.10044 \n",
      "Validation loss decreased (0.136325 --> 0.134439).  Saving model ...\n",
      "[ 32/100] train_loss: 0.11810 valid_loss: 0.13425 test_loss: 0.09964 \n",
      "Validation loss decreased (0.134439 --> 0.134246).  Saving model ...\n",
      "[ 33/100] train_loss: 0.11691 valid_loss: 0.13341 test_loss: 0.09922 \n",
      "Validation loss decreased (0.134246 --> 0.133414).  Saving model ...\n",
      "[ 34/100] train_loss: 0.12100 valid_loss: 0.13401 test_loss: 0.10096 \n",
      "[ 35/100] train_loss: 0.11929 valid_loss: 0.13212 test_loss: 0.09912 \n",
      "Validation loss decreased (0.133414 --> 0.132125).  Saving model ...\n",
      "[ 36/100] train_loss: 0.11523 valid_loss: 0.13216 test_loss: 0.09956 \n",
      "[ 37/100] train_loss: 0.11427 valid_loss: 0.12997 test_loss: 0.09805 \n",
      "Validation loss decreased (0.132125 --> 0.129967).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11462 valid_loss: 0.12913 test_loss: 0.09808 \n",
      "Validation loss decreased (0.129967 --> 0.129133).  Saving model ...\n",
      "[ 39/100] train_loss: 0.11459 valid_loss: 0.12814 test_loss: 0.09883 \n",
      "Validation loss decreased (0.129133 --> 0.128137).  Saving model ...\n",
      "[ 40/100] train_loss: 0.11570 valid_loss: 0.12407 test_loss: 0.09526 \n",
      "Validation loss decreased (0.128137 --> 0.124068).  Saving model ...\n",
      "[ 41/100] train_loss: 0.11248 valid_loss: 0.12500 test_loss: 0.09710 \n",
      "[ 42/100] train_loss: 0.11515 valid_loss: 0.12687 test_loss: 0.09772 \n",
      "[ 43/100] train_loss: 0.11331 valid_loss: 0.12782 test_loss: 0.09705 \n",
      "[ 44/100] train_loss: 0.11083 valid_loss: 0.12668 test_loss: 0.09682 \n",
      "[ 45/100] train_loss: 0.11029 valid_loss: 0.12357 test_loss: 0.09572 \n",
      "Validation loss decreased (0.124068 --> 0.123567).  Saving model ...\n",
      "[ 46/100] train_loss: 0.11065 valid_loss: 0.12380 test_loss: 0.09695 \n",
      "[ 47/100] train_loss: 0.10638 valid_loss: 0.12350 test_loss: 0.09452 \n",
      "Validation loss decreased (0.123567 --> 0.123504).  Saving model ...\n",
      "[ 48/100] train_loss: 0.10946 valid_loss: 0.12445 test_loss: 0.09632 \n",
      "[ 49/100] train_loss: 0.10866 valid_loss: 0.12206 test_loss: 0.09659 \n",
      "Validation loss decreased (0.123504 --> 0.122060).  Saving model ...\n",
      "[ 50/100] train_loss: 0.10686 valid_loss: 0.12462 test_loss: 0.09528 \n",
      "[ 51/100] train_loss: 0.10915 valid_loss: 0.12247 test_loss: 0.09530 \n",
      "[ 52/100] train_loss: 0.10307 valid_loss: 0.11868 test_loss: 0.09191 \n",
      "Validation loss decreased (0.122060 --> 0.118683).  Saving model ...\n",
      "[ 53/100] train_loss: 0.10740 valid_loss: 0.12134 test_loss: 0.09416 \n",
      "[ 54/100] train_loss: 0.10324 valid_loss: 0.11937 test_loss: 0.09385 \n",
      "[ 55/100] train_loss: 0.10768 valid_loss: 0.12146 test_loss: 0.09416 \n",
      "[ 56/100] train_loss: 0.10585 valid_loss: 0.12033 test_loss: 0.09511 \n",
      "[ 57/100] train_loss: 0.10381 valid_loss: 0.12240 test_loss: 0.09343 \n",
      "[ 58/100] train_loss: 0.10515 valid_loss: 0.11690 test_loss: 0.09083 \n",
      "Validation loss decreased (0.118683 --> 0.116900).  Saving model ...\n",
      "[ 59/100] train_loss: 0.10661 valid_loss: 0.11765 test_loss: 0.09249 \n",
      "[ 60/100] train_loss: 0.10233 valid_loss: 0.12142 test_loss: 0.09436 \n",
      "[ 61/100] train_loss: 0.10174 valid_loss: 0.11874 test_loss: 0.09000 \n",
      "[ 62/100] train_loss: 0.10120 valid_loss: 0.11691 test_loss: 0.09281 \n",
      "[ 63/100] train_loss: 0.10335 valid_loss: 0.12017 test_loss: 0.09525 \n",
      "[ 64/100] train_loss: 0.10126 valid_loss: 0.12006 test_loss: 0.09123 \n",
      "[ 65/100] train_loss: 0.10027 valid_loss: 0.11498 test_loss: 0.08906 \n",
      "Validation loss decreased (0.116900 --> 0.114980).  Saving model ...\n",
      "[ 66/100] train_loss: 0.09882 valid_loss: 0.11416 test_loss: 0.08947 \n",
      "Validation loss decreased (0.114980 --> 0.114155).  Saving model ...\n",
      "[ 67/100] train_loss: 0.10138 valid_loss: 0.11753 test_loss: 0.08893 \n",
      "[ 68/100] train_loss: 0.10047 valid_loss: 0.11903 test_loss: 0.09188 \n",
      "[ 69/100] train_loss: 0.10098 valid_loss: 0.11701 test_loss: 0.09066 \n",
      "[ 70/100] train_loss: 0.09922 valid_loss: 0.11707 test_loss: 0.09068 \n",
      "[ 71/100] train_loss: 0.09616 valid_loss: 0.11574 test_loss: 0.09022 \n",
      "[ 72/100] train_loss: 0.09975 valid_loss: 0.11887 test_loss: 0.09255 \n",
      "[ 73/100] train_loss: 0.09754 valid_loss: 0.11405 test_loss: 0.08887 \n",
      "Validation loss decreased (0.114155 --> 0.114048).  Saving model ...\n",
      "[ 74/100] train_loss: 0.09990 valid_loss: 0.11451 test_loss: 0.08857 \n",
      "[ 75/100] train_loss: 0.09679 valid_loss: 0.11351 test_loss: 0.08908 \n",
      "Validation loss decreased (0.114048 --> 0.113511).  Saving model ...\n",
      "[ 76/100] train_loss: 0.09635 valid_loss: 0.11159 test_loss: 0.08823 \n",
      "Validation loss decreased (0.113511 --> 0.111588).  Saving model ...\n",
      "[ 77/100] train_loss: 0.09861 valid_loss: 0.11195 test_loss: 0.09565 \n",
      "[ 78/100] train_loss: 0.09792 valid_loss: 0.11322 test_loss: 0.08759 \n",
      "[ 79/100] train_loss: 0.10031 valid_loss: 0.11041 test_loss: 0.08680 \n",
      "Validation loss decreased (0.111588 --> 0.110409).  Saving model ...\n",
      "[ 80/100] train_loss: 0.09898 valid_loss: 0.11405 test_loss: 0.08987 \n",
      "[ 81/100] train_loss: 0.09817 valid_loss: 0.11171 test_loss: 0.08637 \n",
      "[ 82/100] train_loss: 0.09408 valid_loss: 0.11290 test_loss: 0.08822 \n",
      "[ 83/100] train_loss: 0.09828 valid_loss: 0.11015 test_loss: 0.09104 \n",
      "Validation loss decreased (0.110409 --> 0.110146).  Saving model ...\n",
      "[ 84/100] train_loss: 0.09723 valid_loss: 0.10851 test_loss: 0.08492 \n",
      "Validation loss decreased (0.110146 --> 0.108512).  Saving model ...\n",
      "[ 85/100] train_loss: 0.09748 valid_loss: 0.10828 test_loss: 0.08699 \n",
      "Validation loss decreased (0.108512 --> 0.108279).  Saving model ...\n",
      "[ 86/100] train_loss: 0.09451 valid_loss: 0.10786 test_loss: 0.08534 \n",
      "Validation loss decreased (0.108279 --> 0.107864).  Saving model ...\n",
      "[ 87/100] train_loss: 0.09465 valid_loss: 0.10961 test_loss: 0.08556 \n",
      "[ 88/100] train_loss: 0.09380 valid_loss: 0.11168 test_loss: 0.08704 \n",
      "[ 89/100] train_loss: 0.09503 valid_loss: 0.10615 test_loss: 0.08443 \n",
      "Validation loss decreased (0.107864 --> 0.106149).  Saving model ...\n",
      "[ 90/100] train_loss: 0.09123 valid_loss: 0.10819 test_loss: 0.08364 \n",
      "[ 91/100] train_loss: 0.09296 valid_loss: 0.10821 test_loss: 0.08608 \n",
      "[ 92/100] train_loss: 0.09355 valid_loss: 0.10786 test_loss: 0.08666 \n",
      "[ 93/100] train_loss: 0.09395 valid_loss: 0.10836 test_loss: 0.08535 \n",
      "[ 94/100] train_loss: 0.09146 valid_loss: 0.10714 test_loss: 0.08554 \n",
      "[ 95/100] train_loss: 0.09462 valid_loss: 0.10451 test_loss: 0.08480 \n",
      "Validation loss decreased (0.106149 --> 0.104511).  Saving model ...\n",
      "[ 96/100] train_loss: 0.09230 valid_loss: 0.10461 test_loss: 0.08265 \n",
      "[ 97/100] train_loss: 0.09115 valid_loss: 0.10633 test_loss: 0.08447 \n",
      "[ 98/100] train_loss: 0.09226 valid_loss: 0.10528 test_loss: 0.08447 \n",
      "[ 99/100] train_loss: 0.09101 valid_loss: 0.10613 test_loss: 0.08401 \n",
      "[100/100] train_loss: 0.09052 valid_loss: 0.10510 test_loss: 0.08483 \n",
      "Model 9 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 10\n",
      "[  1/100] train_loss: 0.53141 valid_loss: 0.45603 test_loss: 0.41306 \n",
      "Validation loss decreased (inf --> 0.456033).  Saving model ...\n",
      "[  2/100] train_loss: 0.36991 valid_loss: 0.33415 test_loss: 0.28733 \n",
      "Validation loss decreased (0.456033 --> 0.334154).  Saving model ...\n",
      "[  3/100] train_loss: 0.29141 valid_loss: 0.28037 test_loss: 0.22614 \n",
      "Validation loss decreased (0.334154 --> 0.280374).  Saving model ...\n",
      "[  4/100] train_loss: 0.25224 valid_loss: 0.24381 test_loss: 0.19529 \n",
      "Validation loss decreased (0.280374 --> 0.243815).  Saving model ...\n",
      "[  5/100] train_loss: 0.21814 valid_loss: 0.22392 test_loss: 0.17037 \n",
      "Validation loss decreased (0.243815 --> 0.223921).  Saving model ...\n",
      "[  6/100] train_loss: 0.19688 valid_loss: 0.20733 test_loss: 0.15478 \n",
      "Validation loss decreased (0.223921 --> 0.207325).  Saving model ...\n",
      "[  7/100] train_loss: 0.18576 valid_loss: 0.19233 test_loss: 0.14328 \n",
      "Validation loss decreased (0.207325 --> 0.192330).  Saving model ...\n",
      "[  8/100] train_loss: 0.17657 valid_loss: 0.18771 test_loss: 0.13726 \n",
      "Validation loss decreased (0.192330 --> 0.187707).  Saving model ...\n",
      "[  9/100] train_loss: 0.16873 valid_loss: 0.17946 test_loss: 0.13312 \n",
      "Validation loss decreased (0.187707 --> 0.179462).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16549 valid_loss: 0.17756 test_loss: 0.13027 \n",
      "Validation loss decreased (0.179462 --> 0.177565).  Saving model ...\n",
      "[ 11/100] train_loss: 0.15418 valid_loss: 0.16839 test_loss: 0.12244 \n",
      "Validation loss decreased (0.177565 --> 0.168387).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15005 valid_loss: 0.16953 test_loss: 0.12040 \n",
      "[ 13/100] train_loss: 0.14812 valid_loss: 0.16792 test_loss: 0.12179 \n",
      "Validation loss decreased (0.168387 --> 0.167919).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14384 valid_loss: 0.16389 test_loss: 0.12044 \n",
      "Validation loss decreased (0.167919 --> 0.163888).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14181 valid_loss: 0.15745 test_loss: 0.11498 \n",
      "Validation loss decreased (0.163888 --> 0.157446).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14200 valid_loss: 0.15584 test_loss: 0.11539 \n",
      "Validation loss decreased (0.157446 --> 0.155837).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13520 valid_loss: 0.15287 test_loss: 0.11254 \n",
      "Validation loss decreased (0.155837 --> 0.152870).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13298 valid_loss: 0.15022 test_loss: 0.11225 \n",
      "Validation loss decreased (0.152870 --> 0.150215).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13262 valid_loss: 0.14762 test_loss: 0.11043 \n",
      "Validation loss decreased (0.150215 --> 0.147617).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13266 valid_loss: 0.14354 test_loss: 0.10721 \n",
      "Validation loss decreased (0.147617 --> 0.143536).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12808 valid_loss: 0.14075 test_loss: 0.10510 \n",
      "Validation loss decreased (0.143536 --> 0.140752).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13101 valid_loss: 0.13874 test_loss: 0.10548 \n",
      "Validation loss decreased (0.140752 --> 0.138737).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13030 valid_loss: 0.13631 test_loss: 0.10485 \n",
      "Validation loss decreased (0.138737 --> 0.136309).  Saving model ...\n",
      "[ 24/100] train_loss: 0.12590 valid_loss: 0.13872 test_loss: 0.10643 \n",
      "[ 25/100] train_loss: 0.12625 valid_loss: 0.13388 test_loss: 0.10425 \n",
      "Validation loss decreased (0.136309 --> 0.133884).  Saving model ...\n",
      "[ 26/100] train_loss: 0.12322 valid_loss: 0.13044 test_loss: 0.10113 \n",
      "Validation loss decreased (0.133884 --> 0.130438).  Saving model ...\n",
      "[ 27/100] train_loss: 0.12380 valid_loss: 0.13333 test_loss: 0.10287 \n",
      "[ 28/100] train_loss: 0.12123 valid_loss: 0.13066 test_loss: 0.10053 \n",
      "[ 29/100] train_loss: 0.12037 valid_loss: 0.12916 test_loss: 0.09969 \n",
      "Validation loss decreased (0.130438 --> 0.129162).  Saving model ...\n",
      "[ 30/100] train_loss: 0.11643 valid_loss: 0.12921 test_loss: 0.09869 \n",
      "[ 31/100] train_loss: 0.12060 valid_loss: 0.13317 test_loss: 0.10235 \n",
      "[ 32/100] train_loss: 0.11845 valid_loss: 0.12800 test_loss: 0.10052 \n",
      "Validation loss decreased (0.129162 --> 0.128001).  Saving model ...\n",
      "[ 33/100] train_loss: 0.11828 valid_loss: 0.12450 test_loss: 0.09695 \n",
      "Validation loss decreased (0.128001 --> 0.124498).  Saving model ...\n",
      "[ 34/100] train_loss: 0.11434 valid_loss: 0.12691 test_loss: 0.09889 \n",
      "[ 35/100] train_loss: 0.11651 valid_loss: 0.12419 test_loss: 0.09514 \n",
      "Validation loss decreased (0.124498 --> 0.124191).  Saving model ...\n",
      "[ 36/100] train_loss: 0.11384 valid_loss: 0.12658 test_loss: 0.09559 \n",
      "[ 37/100] train_loss: 0.11364 valid_loss: 0.12474 test_loss: 0.09473 \n",
      "[ 38/100] train_loss: 0.11302 valid_loss: 0.12928 test_loss: 0.09861 \n",
      "[ 39/100] train_loss: 0.11431 valid_loss: 0.12434 test_loss: 0.09509 \n",
      "[ 40/100] train_loss: 0.11056 valid_loss: 0.12460 test_loss: 0.09465 \n",
      "[ 41/100] train_loss: 0.10626 valid_loss: 0.12469 test_loss: 0.09457 \n",
      "[ 42/100] train_loss: 0.11477 valid_loss: 0.12085 test_loss: 0.09395 \n",
      "Validation loss decreased (0.124191 --> 0.120853).  Saving model ...\n",
      "[ 43/100] train_loss: 0.11052 valid_loss: 0.12201 test_loss: 0.09557 \n",
      "[ 44/100] train_loss: 0.10933 valid_loss: 0.11888 test_loss: 0.09228 \n",
      "Validation loss decreased (0.120853 --> 0.118881).  Saving model ...\n",
      "[ 45/100] train_loss: 0.10787 valid_loss: 0.12127 test_loss: 0.09355 \n",
      "[ 46/100] train_loss: 0.10638 valid_loss: 0.11861 test_loss: 0.09204 \n",
      "Validation loss decreased (0.118881 --> 0.118611).  Saving model ...\n",
      "[ 47/100] train_loss: 0.10957 valid_loss: 0.12056 test_loss: 0.09284 \n",
      "[ 48/100] train_loss: 0.10525 valid_loss: 0.11736 test_loss: 0.08996 \n",
      "Validation loss decreased (0.118611 --> 0.117361).  Saving model ...\n",
      "[ 49/100] train_loss: 0.10762 valid_loss: 0.12225 test_loss: 0.09560 \n",
      "[ 50/100] train_loss: 0.10613 valid_loss: 0.11951 test_loss: 0.09335 \n",
      "[ 51/100] train_loss: 0.10647 valid_loss: 0.11761 test_loss: 0.09256 \n",
      "[ 52/100] train_loss: 0.10377 valid_loss: 0.12105 test_loss: 0.09239 \n",
      "[ 53/100] train_loss: 0.10497 valid_loss: 0.11652 test_loss: 0.09223 \n",
      "Validation loss decreased (0.117361 --> 0.116515).  Saving model ...\n",
      "[ 54/100] train_loss: 0.10396 valid_loss: 0.11573 test_loss: 0.09074 \n",
      "Validation loss decreased (0.116515 --> 0.115733).  Saving model ...\n",
      "[ 55/100] train_loss: 0.10409 valid_loss: 0.11705 test_loss: 0.09310 \n",
      "[ 56/100] train_loss: 0.10375 valid_loss: 0.11361 test_loss: 0.09044 \n",
      "Validation loss decreased (0.115733 --> 0.113607).  Saving model ...\n",
      "[ 57/100] train_loss: 0.09979 valid_loss: 0.11731 test_loss: 0.09216 \n",
      "[ 58/100] train_loss: 0.10221 valid_loss: 0.11501 test_loss: 0.09092 \n",
      "[ 59/100] train_loss: 0.10356 valid_loss: 0.11152 test_loss: 0.09456 \n",
      "Validation loss decreased (0.113607 --> 0.111516).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10136 valid_loss: 0.11456 test_loss: 0.09110 \n",
      "[ 61/100] train_loss: 0.10265 valid_loss: 0.11170 test_loss: 0.08927 \n",
      "[ 62/100] train_loss: 0.10073 valid_loss: 0.11534 test_loss: 0.09072 \n",
      "[ 63/100] train_loss: 0.09914 valid_loss: 0.11347 test_loss: 0.08787 \n",
      "[ 64/100] train_loss: 0.10295 valid_loss: 0.11181 test_loss: 0.08809 \n",
      "[ 65/100] train_loss: 0.09873 valid_loss: 0.11077 test_loss: 0.08821 \n",
      "Validation loss decreased (0.111516 --> 0.110774).  Saving model ...\n",
      "[ 66/100] train_loss: 0.10315 valid_loss: 0.11681 test_loss: 0.09269 \n",
      "[ 67/100] train_loss: 0.10006 valid_loss: 0.11673 test_loss: 0.09101 \n",
      "[ 68/100] train_loss: 0.09870 valid_loss: 0.11535 test_loss: 0.08947 \n",
      "[ 69/100] train_loss: 0.10082 valid_loss: 0.11268 test_loss: 0.08889 \n",
      "[ 70/100] train_loss: 0.10104 valid_loss: 0.11174 test_loss: 0.08790 \n",
      "[ 71/100] train_loss: 0.09899 valid_loss: 0.11345 test_loss: 0.08987 \n",
      "[ 72/100] train_loss: 0.09705 valid_loss: 0.11270 test_loss: 0.09024 \n",
      "[ 73/100] train_loss: 0.09826 valid_loss: 0.11431 test_loss: 0.08876 \n",
      "[ 74/100] train_loss: 0.09871 valid_loss: 0.11516 test_loss: 0.08884 \n",
      "[ 75/100] train_loss: 0.09754 valid_loss: 0.11132 test_loss: 0.08664 \n",
      "[ 76/100] train_loss: 0.09674 valid_loss: 0.10810 test_loss: 0.08627 \n",
      "Validation loss decreased (0.110774 --> 0.108104).  Saving model ...\n",
      "[ 77/100] train_loss: 0.09499 valid_loss: 0.11019 test_loss: 0.08760 \n",
      "[ 78/100] train_loss: 0.09907 valid_loss: 0.11256 test_loss: 0.09601 \n",
      "[ 79/100] train_loss: 0.09508 valid_loss: 0.10987 test_loss: 0.08848 \n",
      "[ 80/100] train_loss: 0.09459 valid_loss: 0.11009 test_loss: 0.08567 \n",
      "[ 81/100] train_loss: 0.09598 valid_loss: 0.11170 test_loss: 0.08903 \n",
      "[ 82/100] train_loss: 0.09519 valid_loss: 0.10864 test_loss: 0.08551 \n",
      "[ 83/100] train_loss: 0.09333 valid_loss: 0.10893 test_loss: 0.08578 \n",
      "[ 84/100] train_loss: 0.09221 valid_loss: 0.10729 test_loss: 0.08341 \n",
      "Validation loss decreased (0.108104 --> 0.107291).  Saving model ...\n",
      "[ 85/100] train_loss: 0.09351 valid_loss: 0.10928 test_loss: 0.08636 \n",
      "[ 86/100] train_loss: 0.09672 valid_loss: 0.10734 test_loss: 0.08889 \n",
      "[ 87/100] train_loss: 0.09487 valid_loss: 0.10936 test_loss: 0.09044 \n",
      "[ 88/100] train_loss: 0.09349 valid_loss: 0.10669 test_loss: 0.11473 \n",
      "Validation loss decreased (0.107291 --> 0.106689).  Saving model ...\n",
      "[ 89/100] train_loss: 0.09355 valid_loss: 0.10826 test_loss: 0.08571 \n",
      "[ 90/100] train_loss: 0.09253 valid_loss: 0.10913 test_loss: 0.08970 \n",
      "[ 91/100] train_loss: 0.09241 valid_loss: 0.10611 test_loss: 0.09611 \n",
      "Validation loss decreased (0.106689 --> 0.106115).  Saving model ...\n",
      "[ 92/100] train_loss: 0.09434 valid_loss: 0.10705 test_loss: 0.08720 \n",
      "[ 93/100] train_loss: 0.09176 valid_loss: 0.10705 test_loss: 0.08719 \n",
      "[ 94/100] train_loss: 0.09247 valid_loss: 0.11118 test_loss: 0.08755 \n",
      "[ 95/100] train_loss: 0.09121 valid_loss: 0.10938 test_loss: 0.08741 \n",
      "[ 96/100] train_loss: 0.09201 valid_loss: 0.10484 test_loss: 0.08425 \n",
      "Validation loss decreased (0.106115 --> 0.104838).  Saving model ...\n",
      "[ 97/100] train_loss: 0.09250 valid_loss: 0.10498 test_loss: 0.08607 \n",
      "[ 98/100] train_loss: 0.09417 valid_loss: 0.10732 test_loss: 0.08708 \n",
      "[ 99/100] train_loss: 0.08693 valid_loss: 0.10711 test_loss: 0.08898 \n",
      "[100/100] train_loss: 0.09083 valid_loss: 0.10274 test_loss: 0.08625 \n",
      "Validation loss decreased (0.104838 --> 0.102740).  Saving model ...\n",
      "Model 10 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 11\n",
      "[  1/100] train_loss: 0.58793 valid_loss: 0.47750 test_loss: 0.43409 \n",
      "Validation loss decreased (inf --> 0.477499).  Saving model ...\n",
      "[  2/100] train_loss: 0.38976 valid_loss: 0.33872 test_loss: 0.29535 \n",
      "Validation loss decreased (0.477499 --> 0.338715).  Saving model ...\n",
      "[  3/100] train_loss: 0.30140 valid_loss: 0.29060 test_loss: 0.24308 \n",
      "Validation loss decreased (0.338715 --> 0.290595).  Saving model ...\n",
      "[  4/100] train_loss: 0.25507 valid_loss: 0.25472 test_loss: 0.20420 \n",
      "Validation loss decreased (0.290595 --> 0.254724).  Saving model ...\n",
      "[  5/100] train_loss: 0.22596 valid_loss: 0.23605 test_loss: 0.18360 \n",
      "Validation loss decreased (0.254724 --> 0.236049).  Saving model ...\n",
      "[  6/100] train_loss: 0.20302 valid_loss: 0.21737 test_loss: 0.16325 \n",
      "Validation loss decreased (0.236049 --> 0.217367).  Saving model ...\n",
      "[  7/100] train_loss: 0.18986 valid_loss: 0.20121 test_loss: 0.14988 \n",
      "Validation loss decreased (0.217367 --> 0.201208).  Saving model ...\n",
      "[  8/100] train_loss: 0.18143 valid_loss: 0.19311 test_loss: 0.14144 \n",
      "Validation loss decreased (0.201208 --> 0.193113).  Saving model ...\n",
      "[  9/100] train_loss: 0.17338 valid_loss: 0.18461 test_loss: 0.13238 \n",
      "Validation loss decreased (0.193113 --> 0.184611).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16487 valid_loss: 0.18437 test_loss: 0.13072 \n",
      "Validation loss decreased (0.184611 --> 0.184368).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16086 valid_loss: 0.17816 test_loss: 0.12675 \n",
      "Validation loss decreased (0.184368 --> 0.178161).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15354 valid_loss: 0.17648 test_loss: 0.12600 \n",
      "Validation loss decreased (0.178161 --> 0.176482).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15295 valid_loss: 0.16561 test_loss: 0.11936 \n",
      "Validation loss decreased (0.176482 --> 0.165615).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14460 valid_loss: 0.16607 test_loss: 0.11802 \n",
      "[ 15/100] train_loss: 0.14588 valid_loss: 0.16327 test_loss: 0.11738 \n",
      "Validation loss decreased (0.165615 --> 0.163269).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14302 valid_loss: 0.15674 test_loss: 0.11221 \n",
      "Validation loss decreased (0.163269 --> 0.156736).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13807 valid_loss: 0.16119 test_loss: 0.11744 \n",
      "[ 18/100] train_loss: 0.13876 valid_loss: 0.15300 test_loss: 0.11179 \n",
      "Validation loss decreased (0.156736 --> 0.153002).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13829 valid_loss: 0.14698 test_loss: 0.11239 \n",
      "Validation loss decreased (0.153002 --> 0.146980).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13391 valid_loss: 0.14819 test_loss: 0.11000 \n",
      "[ 21/100] train_loss: 0.13096 valid_loss: 0.14456 test_loss: 0.10983 \n",
      "Validation loss decreased (0.146980 --> 0.144562).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13082 valid_loss: 0.14361 test_loss: 0.10787 \n",
      "Validation loss decreased (0.144562 --> 0.143615).  Saving model ...\n",
      "[ 23/100] train_loss: 0.12838 valid_loss: 0.14359 test_loss: 0.10770 \n",
      "Validation loss decreased (0.143615 --> 0.143585).  Saving model ...\n",
      "[ 24/100] train_loss: 0.12723 valid_loss: 0.14187 test_loss: 0.10597 \n",
      "Validation loss decreased (0.143585 --> 0.141874).  Saving model ...\n",
      "[ 25/100] train_loss: 0.12491 valid_loss: 0.13849 test_loss: 0.10559 \n",
      "Validation loss decreased (0.141874 --> 0.138485).  Saving model ...\n",
      "[ 26/100] train_loss: 0.12217 valid_loss: 0.13805 test_loss: 0.10457 \n",
      "Validation loss decreased (0.138485 --> 0.138048).  Saving model ...\n",
      "[ 27/100] train_loss: 0.12481 valid_loss: 0.13565 test_loss: 0.10747 \n",
      "Validation loss decreased (0.138048 --> 0.135654).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12440 valid_loss: 0.13649 test_loss: 0.10468 \n",
      "[ 29/100] train_loss: 0.11948 valid_loss: 0.13293 test_loss: 0.10474 \n",
      "Validation loss decreased (0.135654 --> 0.132925).  Saving model ...\n",
      "[ 30/100] train_loss: 0.11859 valid_loss: 0.13041 test_loss: 0.10341 \n",
      "Validation loss decreased (0.132925 --> 0.130407).  Saving model ...\n",
      "[ 31/100] train_loss: 0.12160 valid_loss: 0.13186 test_loss: 0.10424 \n",
      "[ 32/100] train_loss: 0.12052 valid_loss: 0.13018 test_loss: 0.10194 \n",
      "Validation loss decreased (0.130407 --> 0.130179).  Saving model ...\n",
      "[ 33/100] train_loss: 0.11753 valid_loss: 0.13039 test_loss: 0.09937 \n",
      "[ 34/100] train_loss: 0.11990 valid_loss: 0.12549 test_loss: 0.10909 \n",
      "Validation loss decreased (0.130179 --> 0.125492).  Saving model ...\n",
      "[ 35/100] train_loss: 0.11638 valid_loss: 0.12733 test_loss: 0.10268 \n",
      "[ 36/100] train_loss: 0.11274 valid_loss: 0.13013 test_loss: 0.09984 \n",
      "[ 37/100] train_loss: 0.11612 valid_loss: 0.12363 test_loss: 0.12243 \n",
      "Validation loss decreased (0.125492 --> 0.123631).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11667 valid_loss: 0.13047 test_loss: 0.10320 \n",
      "[ 39/100] train_loss: 0.11513 valid_loss: 0.12370 test_loss: 0.10039 \n",
      "[ 40/100] train_loss: 0.11174 valid_loss: 0.12605 test_loss: 0.09670 \n",
      "[ 41/100] train_loss: 0.11152 valid_loss: 0.12695 test_loss: 0.09719 \n",
      "[ 42/100] train_loss: 0.10964 valid_loss: 0.12434 test_loss: 0.09800 \n",
      "[ 43/100] train_loss: 0.10977 valid_loss: 0.12558 test_loss: 0.09841 \n",
      "[ 44/100] train_loss: 0.11248 valid_loss: 0.12623 test_loss: 0.10194 \n",
      "[ 45/100] train_loss: 0.11183 valid_loss: 0.12680 test_loss: 0.09924 \n",
      "[ 46/100] train_loss: 0.10780 valid_loss: 0.12137 test_loss: 0.09667 \n",
      "Validation loss decreased (0.123631 --> 0.121366).  Saving model ...\n",
      "[ 47/100] train_loss: 0.11163 valid_loss: 0.12181 test_loss: 0.09988 \n",
      "[ 48/100] train_loss: 0.11065 valid_loss: 0.11863 test_loss: 0.09170 \n",
      "Validation loss decreased (0.121366 --> 0.118628).  Saving model ...\n",
      "[ 49/100] train_loss: 0.10855 valid_loss: 0.12125 test_loss: 0.09609 \n",
      "[ 50/100] train_loss: 0.11004 valid_loss: 0.11984 test_loss: 0.09352 \n",
      "[ 51/100] train_loss: 0.11031 valid_loss: 0.12094 test_loss: 0.09667 \n",
      "[ 52/100] train_loss: 0.10512 valid_loss: 0.11777 test_loss: 0.09432 \n",
      "Validation loss decreased (0.118628 --> 0.117775).  Saving model ...\n",
      "[ 53/100] train_loss: 0.10216 valid_loss: 0.12021 test_loss: 0.09744 \n",
      "[ 54/100] train_loss: 0.10523 valid_loss: 0.11518 test_loss: 0.09533 \n",
      "Validation loss decreased (0.117775 --> 0.115185).  Saving model ...\n",
      "[ 55/100] train_loss: 0.10456 valid_loss: 0.11812 test_loss: 0.09303 \n",
      "[ 56/100] train_loss: 0.10595 valid_loss: 0.12090 test_loss: 0.09522 \n",
      "[ 57/100] train_loss: 0.10682 valid_loss: 0.12093 test_loss: 0.09474 \n",
      "[ 58/100] train_loss: 0.10480 valid_loss: 0.11832 test_loss: 0.09139 \n",
      "[ 59/100] train_loss: 0.10314 valid_loss: 0.12050 test_loss: 0.09566 \n",
      "[ 60/100] train_loss: 0.10199 valid_loss: 0.11830 test_loss: 0.09303 \n",
      "[ 61/100] train_loss: 0.10181 valid_loss: 0.11366 test_loss: 0.09011 \n",
      "Validation loss decreased (0.115185 --> 0.113660).  Saving model ...\n",
      "[ 62/100] train_loss: 0.10115 valid_loss: 0.11641 test_loss: 0.09242 \n",
      "[ 63/100] train_loss: 0.09759 valid_loss: 0.11392 test_loss: 0.08972 \n",
      "[ 64/100] train_loss: 0.10044 valid_loss: 0.11665 test_loss: 0.08961 \n",
      "[ 65/100] train_loss: 0.10037 valid_loss: 0.11534 test_loss: 0.09161 \n",
      "[ 66/100] train_loss: 0.09880 valid_loss: 0.11290 test_loss: 0.09130 \n",
      "Validation loss decreased (0.113660 --> 0.112900).  Saving model ...\n",
      "[ 67/100] train_loss: 0.09897 valid_loss: 0.11400 test_loss: 0.09159 \n",
      "[ 68/100] train_loss: 0.10074 valid_loss: 0.11182 test_loss: 0.09057 \n",
      "Validation loss decreased (0.112900 --> 0.111822).  Saving model ...\n",
      "[ 69/100] train_loss: 0.09775 valid_loss: 0.11325 test_loss: 0.09000 \n",
      "[ 70/100] train_loss: 0.09751 valid_loss: 0.11165 test_loss: 0.08961 \n",
      "Validation loss decreased (0.111822 --> 0.111646).  Saving model ...\n",
      "[ 71/100] train_loss: 0.09834 valid_loss: 0.11056 test_loss: 0.09103 \n",
      "Validation loss decreased (0.111646 --> 0.110559).  Saving model ...\n",
      "[ 72/100] train_loss: 0.09899 valid_loss: 0.11218 test_loss: 0.08789 \n",
      "[ 73/100] train_loss: 0.09620 valid_loss: 0.11499 test_loss: 0.09085 \n",
      "[ 74/100] train_loss: 0.09850 valid_loss: 0.11116 test_loss: 0.08752 \n",
      "[ 75/100] train_loss: 0.10165 valid_loss: 0.11095 test_loss: 0.08823 \n",
      "[ 76/100] train_loss: 0.09660 valid_loss: 0.10906 test_loss: 0.08943 \n",
      "Validation loss decreased (0.110559 --> 0.109062).  Saving model ...\n",
      "[ 77/100] train_loss: 0.09644 valid_loss: 0.11151 test_loss: 0.08835 \n",
      "[ 78/100] train_loss: 0.09916 valid_loss: 0.11041 test_loss: 0.08837 \n",
      "[ 79/100] train_loss: 0.09460 valid_loss: 0.11333 test_loss: 0.08979 \n",
      "[ 80/100] train_loss: 0.09349 valid_loss: 0.11513 test_loss: 0.09289 \n",
      "[ 81/100] train_loss: 0.09543 valid_loss: 0.11129 test_loss: 0.08953 \n",
      "[ 82/100] train_loss: 0.09656 valid_loss: 0.10790 test_loss: 0.09172 \n",
      "Validation loss decreased (0.109062 --> 0.107898).  Saving model ...\n",
      "[ 83/100] train_loss: 0.09335 valid_loss: 0.11117 test_loss: 0.09002 \n",
      "[ 84/100] train_loss: 0.09301 valid_loss: 0.11085 test_loss: 0.08864 \n",
      "[ 85/100] train_loss: 0.09684 valid_loss: 0.11089 test_loss: 0.09317 \n",
      "[ 86/100] train_loss: 0.09513 valid_loss: 0.10771 test_loss: 0.08868 \n",
      "Validation loss decreased (0.107898 --> 0.107713).  Saving model ...\n",
      "[ 87/100] train_loss: 0.09780 valid_loss: 0.10780 test_loss: 0.09087 \n",
      "[ 88/100] train_loss: 0.09488 valid_loss: 0.11275 test_loss: 0.08956 \n",
      "[ 89/100] train_loss: 0.09241 valid_loss: 0.11115 test_loss: 0.09045 \n",
      "[ 90/100] train_loss: 0.09447 valid_loss: 0.11157 test_loss: 0.09207 \n",
      "[ 91/100] train_loss: 0.09468 valid_loss: 0.10977 test_loss: 0.08864 \n",
      "[ 92/100] train_loss: 0.09473 valid_loss: 0.10987 test_loss: 0.09096 \n",
      "[ 93/100] train_loss: 0.09573 valid_loss: 0.11057 test_loss: 0.09057 \n",
      "[ 94/100] train_loss: 0.09240 valid_loss: 0.10881 test_loss: 0.08657 \n",
      "[ 95/100] train_loss: 0.09322 valid_loss: 0.10850 test_loss: 0.08713 \n",
      "[ 96/100] train_loss: 0.09249 valid_loss: 0.10849 test_loss: 0.08848 \n",
      "[ 97/100] train_loss: 0.09243 valid_loss: 0.10601 test_loss: 0.08781 \n",
      "Validation loss decreased (0.107713 --> 0.106007).  Saving model ...\n",
      "[ 98/100] train_loss: 0.09290 valid_loss: 0.10752 test_loss: 0.08850 \n",
      "[ 99/100] train_loss: 0.09250 valid_loss: 0.10826 test_loss: 0.08913 \n",
      "[100/100] train_loss: 0.09299 valid_loss: 0.10477 test_loss: 0.08647 \n",
      "Validation loss decreased (0.106007 --> 0.104768).  Saving model ...\n",
      "Model 11 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 12\n",
      "[  1/100] train_loss: 0.59539 valid_loss: 0.49212 test_loss: 0.45683 \n",
      "Validation loss decreased (inf --> 0.492119).  Saving model ...\n",
      "[  2/100] train_loss: 0.41155 valid_loss: 0.36132 test_loss: 0.31749 \n",
      "Validation loss decreased (0.492119 --> 0.361319).  Saving model ...\n",
      "[  3/100] train_loss: 0.31864 valid_loss: 0.30339 test_loss: 0.26021 \n",
      "Validation loss decreased (0.361319 --> 0.303389).  Saving model ...\n",
      "[  4/100] train_loss: 0.26876 valid_loss: 0.26371 test_loss: 0.22041 \n",
      "Validation loss decreased (0.303389 --> 0.263715).  Saving model ...\n",
      "[  5/100] train_loss: 0.23464 valid_loss: 0.23897 test_loss: 0.19216 \n",
      "Validation loss decreased (0.263715 --> 0.238967).  Saving model ...\n",
      "[  6/100] train_loss: 0.21297 valid_loss: 0.22514 test_loss: 0.17115 \n",
      "Validation loss decreased (0.238967 --> 0.225144).  Saving model ...\n",
      "[  7/100] train_loss: 0.19915 valid_loss: 0.20852 test_loss: 0.16007 \n",
      "Validation loss decreased (0.225144 --> 0.208519).  Saving model ...\n",
      "[  8/100] train_loss: 0.18389 valid_loss: 0.19480 test_loss: 0.14569 \n",
      "Validation loss decreased (0.208519 --> 0.194801).  Saving model ...\n",
      "[  9/100] train_loss: 0.17938 valid_loss: 0.18776 test_loss: 0.14408 \n",
      "Validation loss decreased (0.194801 --> 0.187757).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16838 valid_loss: 0.17939 test_loss: 0.13319 \n",
      "Validation loss decreased (0.187757 --> 0.179387).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16254 valid_loss: 0.17760 test_loss: 0.13003 \n",
      "Validation loss decreased (0.179387 --> 0.177596).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15780 valid_loss: 0.17189 test_loss: 0.12632 \n",
      "Validation loss decreased (0.177596 --> 0.171888).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15652 valid_loss: 0.16959 test_loss: 0.12241 \n",
      "Validation loss decreased (0.171888 --> 0.169588).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15065 valid_loss: 0.16483 test_loss: 0.11928 \n",
      "Validation loss decreased (0.169588 --> 0.164835).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15417 valid_loss: 0.16505 test_loss: 0.11948 \n",
      "[ 16/100] train_loss: 0.14766 valid_loss: 0.16179 test_loss: 0.11727 \n",
      "Validation loss decreased (0.164835 --> 0.161787).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14501 valid_loss: 0.15575 test_loss: 0.11632 \n",
      "Validation loss decreased (0.161787 --> 0.155752).  Saving model ...\n",
      "[ 18/100] train_loss: 0.14071 valid_loss: 0.15342 test_loss: 0.11192 \n",
      "Validation loss decreased (0.155752 --> 0.153420).  Saving model ...\n",
      "[ 19/100] train_loss: 0.14019 valid_loss: 0.15633 test_loss: 0.11367 \n",
      "[ 20/100] train_loss: 0.13764 valid_loss: 0.15126 test_loss: 0.11066 \n",
      "Validation loss decreased (0.153420 --> 0.151263).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13467 valid_loss: 0.15330 test_loss: 0.11030 \n",
      "[ 22/100] train_loss: 0.13632 valid_loss: 0.15068 test_loss: 0.11335 \n",
      "Validation loss decreased (0.151263 --> 0.150678).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13179 valid_loss: 0.14679 test_loss: 0.10821 \n",
      "Validation loss decreased (0.150678 --> 0.146794).  Saving model ...\n",
      "[ 24/100] train_loss: 0.13083 valid_loss: 0.14418 test_loss: 0.10875 \n",
      "Validation loss decreased (0.146794 --> 0.144185).  Saving model ...\n",
      "[ 25/100] train_loss: 0.13027 valid_loss: 0.14856 test_loss: 0.10852 \n",
      "[ 26/100] train_loss: 0.12995 valid_loss: 0.14117 test_loss: 0.10497 \n",
      "Validation loss decreased (0.144185 --> 0.141175).  Saving model ...\n",
      "[ 27/100] train_loss: 0.13058 valid_loss: 0.14043 test_loss: 0.10488 \n",
      "Validation loss decreased (0.141175 --> 0.140430).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12423 valid_loss: 0.14031 test_loss: 0.10423 \n",
      "Validation loss decreased (0.140430 --> 0.140313).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12715 valid_loss: 0.13765 test_loss: 0.10683 \n",
      "Validation loss decreased (0.140313 --> 0.137655).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12639 valid_loss: 0.14060 test_loss: 0.10428 \n",
      "[ 31/100] train_loss: 0.12302 valid_loss: 0.13708 test_loss: 0.10320 \n",
      "Validation loss decreased (0.137655 --> 0.137075).  Saving model ...\n",
      "[ 32/100] train_loss: 0.12371 valid_loss: 0.14048 test_loss: 0.10521 \n",
      "[ 33/100] train_loss: 0.12181 valid_loss: 0.13898 test_loss: 0.10643 \n",
      "[ 34/100] train_loss: 0.12406 valid_loss: 0.13569 test_loss: 0.10484 \n",
      "Validation loss decreased (0.137075 --> 0.135695).  Saving model ...\n",
      "[ 35/100] train_loss: 0.12342 valid_loss: 0.13607 test_loss: 0.10230 \n",
      "[ 36/100] train_loss: 0.12157 valid_loss: 0.13128 test_loss: 0.09994 \n",
      "Validation loss decreased (0.135695 --> 0.131276).  Saving model ...\n",
      "[ 37/100] train_loss: 0.12353 valid_loss: 0.13347 test_loss: 0.10361 \n",
      "[ 38/100] train_loss: 0.11870 valid_loss: 0.13323 test_loss: 0.10114 \n",
      "[ 39/100] train_loss: 0.11940 valid_loss: 0.13151 test_loss: 0.10161 \n",
      "[ 40/100] train_loss: 0.11803 valid_loss: 0.13097 test_loss: 0.09990 \n",
      "Validation loss decreased (0.131276 --> 0.130974).  Saving model ...\n",
      "[ 41/100] train_loss: 0.11436 valid_loss: 0.12989 test_loss: 0.09918 \n",
      "Validation loss decreased (0.130974 --> 0.129886).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11700 valid_loss: 0.13045 test_loss: 0.10081 \n",
      "[ 43/100] train_loss: 0.11721 valid_loss: 0.13246 test_loss: 0.10026 \n",
      "[ 44/100] train_loss: 0.11286 valid_loss: 0.12680 test_loss: 0.09424 \n",
      "Validation loss decreased (0.129886 --> 0.126800).  Saving model ...\n",
      "[ 45/100] train_loss: 0.11416 valid_loss: 0.12694 test_loss: 0.09686 \n",
      "[ 46/100] train_loss: 0.11504 valid_loss: 0.13177 test_loss: 0.10218 \n",
      "[ 47/100] train_loss: 0.11303 valid_loss: 0.12808 test_loss: 0.09656 \n",
      "[ 48/100] train_loss: 0.11015 valid_loss: 0.12863 test_loss: 0.09872 \n",
      "[ 49/100] train_loss: 0.11430 valid_loss: 0.12652 test_loss: 0.09671 \n",
      "Validation loss decreased (0.126800 --> 0.126523).  Saving model ...\n",
      "[ 50/100] train_loss: 0.11142 valid_loss: 0.12610 test_loss: 0.09637 \n",
      "Validation loss decreased (0.126523 --> 0.126095).  Saving model ...\n",
      "[ 51/100] train_loss: 0.10865 valid_loss: 0.12424 test_loss: 0.09308 \n",
      "Validation loss decreased (0.126095 --> 0.124240).  Saving model ...\n",
      "[ 52/100] train_loss: 0.11174 valid_loss: 0.12418 test_loss: 0.09377 \n",
      "Validation loss decreased (0.124240 --> 0.124177).  Saving model ...\n",
      "[ 53/100] train_loss: 0.10994 valid_loss: 0.12321 test_loss: 0.09306 \n",
      "Validation loss decreased (0.124177 --> 0.123209).  Saving model ...\n",
      "[ 54/100] train_loss: 0.10889 valid_loss: 0.12214 test_loss: 0.09543 \n",
      "Validation loss decreased (0.123209 --> 0.122142).  Saving model ...\n",
      "[ 55/100] train_loss: 0.11195 valid_loss: 0.13032 test_loss: 0.10066 \n",
      "[ 56/100] train_loss: 0.10939 valid_loss: 0.11848 test_loss: 0.09250 \n",
      "Validation loss decreased (0.122142 --> 0.118478).  Saving model ...\n",
      "[ 57/100] train_loss: 0.10719 valid_loss: 0.11898 test_loss: 0.09366 \n",
      "[ 58/100] train_loss: 0.10610 valid_loss: 0.12243 test_loss: 0.09303 \n",
      "[ 59/100] train_loss: 0.10718 valid_loss: 0.12186 test_loss: 0.09286 \n",
      "[ 60/100] train_loss: 0.10622 valid_loss: 0.12149 test_loss: 0.09371 \n",
      "[ 61/100] train_loss: 0.10648 valid_loss: 0.12385 test_loss: 0.09622 \n",
      "[ 62/100] train_loss: 0.10894 valid_loss: 0.11725 test_loss: 0.08902 \n",
      "Validation loss decreased (0.118478 --> 0.117252).  Saving model ...\n",
      "[ 63/100] train_loss: 0.10762 valid_loss: 0.12096 test_loss: 0.09505 \n",
      "[ 64/100] train_loss: 0.10379 valid_loss: 0.11727 test_loss: 0.09120 \n",
      "[ 65/100] train_loss: 0.10441 valid_loss: 0.11822 test_loss: 0.09519 \n",
      "[ 66/100] train_loss: 0.10603 valid_loss: 0.11376 test_loss: 0.09010 \n",
      "Validation loss decreased (0.117252 --> 0.113763).  Saving model ...\n",
      "[ 67/100] train_loss: 0.10374 valid_loss: 0.11528 test_loss: 0.08807 \n",
      "[ 68/100] train_loss: 0.10182 valid_loss: 0.11740 test_loss: 0.09241 \n",
      "[ 69/100] train_loss: 0.10415 valid_loss: 0.11579 test_loss: 0.08933 \n",
      "[ 70/100] train_loss: 0.10067 valid_loss: 0.11730 test_loss: 0.09261 \n",
      "[ 71/100] train_loss: 0.10519 valid_loss: 0.11389 test_loss: 0.08853 \n",
      "[ 72/100] train_loss: 0.10004 valid_loss: 0.11425 test_loss: 0.09149 \n",
      "[ 73/100] train_loss: 0.10264 valid_loss: 0.11209 test_loss: 0.08994 \n",
      "Validation loss decreased (0.113763 --> 0.112089).  Saving model ...\n",
      "[ 74/100] train_loss: 0.10245 valid_loss: 0.11219 test_loss: 0.08744 \n",
      "[ 75/100] train_loss: 0.09926 valid_loss: 0.11575 test_loss: 0.09015 \n",
      "[ 76/100] train_loss: 0.10354 valid_loss: 0.11474 test_loss: 0.08858 \n",
      "[ 77/100] train_loss: 0.09807 valid_loss: 0.11151 test_loss: 0.08597 \n",
      "Validation loss decreased (0.112089 --> 0.111515).  Saving model ...\n",
      "[ 78/100] train_loss: 0.10153 valid_loss: 0.11091 test_loss: 0.08778 \n",
      "Validation loss decreased (0.111515 --> 0.110915).  Saving model ...\n",
      "[ 79/100] train_loss: 0.10092 valid_loss: 0.11190 test_loss: 0.08825 \n",
      "[ 80/100] train_loss: 0.10186 valid_loss: 0.10906 test_loss: 0.08668 \n",
      "Validation loss decreased (0.110915 --> 0.109058).  Saving model ...\n",
      "[ 81/100] train_loss: 0.10028 valid_loss: 0.11449 test_loss: 0.09093 \n",
      "[ 82/100] train_loss: 0.09984 valid_loss: 0.11451 test_loss: 0.09073 \n",
      "[ 83/100] train_loss: 0.09562 valid_loss: 0.11065 test_loss: 0.08685 \n",
      "[ 84/100] train_loss: 0.09742 valid_loss: 0.11195 test_loss: 0.08846 \n",
      "[ 85/100] train_loss: 0.09865 valid_loss: 0.11252 test_loss: 0.08778 \n",
      "[ 86/100] train_loss: 0.09864 valid_loss: 0.11273 test_loss: 0.08930 \n",
      "[ 87/100] train_loss: 0.09821 valid_loss: 0.11045 test_loss: 0.08573 \n",
      "[ 88/100] train_loss: 0.09768 valid_loss: 0.11185 test_loss: 0.08722 \n",
      "[ 89/100] train_loss: 0.09697 valid_loss: 0.11318 test_loss: 0.08917 \n",
      "[ 90/100] train_loss: 0.09581 valid_loss: 0.11264 test_loss: 0.09162 \n",
      "[ 91/100] train_loss: 0.09554 valid_loss: 0.11149 test_loss: 0.08774 \n",
      "[ 92/100] train_loss: 0.09823 valid_loss: 0.10694 test_loss: 0.08417 \n",
      "Validation loss decreased (0.109058 --> 0.106945).  Saving model ...\n",
      "[ 93/100] train_loss: 0.09674 valid_loss: 0.10732 test_loss: 0.08363 \n",
      "[ 94/100] train_loss: 0.09750 valid_loss: 0.10896 test_loss: 0.08645 \n",
      "[ 95/100] train_loss: 0.09654 valid_loss: 0.10938 test_loss: 0.08393 \n",
      "[ 96/100] train_loss: 0.09375 valid_loss: 0.10851 test_loss: 0.08431 \n",
      "[ 97/100] train_loss: 0.09546 valid_loss: 0.11055 test_loss: 0.08687 \n",
      "[ 98/100] train_loss: 0.09496 valid_loss: 0.10874 test_loss: 0.08479 \n",
      "[ 99/100] train_loss: 0.09284 valid_loss: 0.10932 test_loss: 0.08690 \n",
      "[100/100] train_loss: 0.09440 valid_loss: 0.11075 test_loss: 0.08553 \n",
      "Model 12 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 13\n",
      "[  1/100] train_loss: 0.58100 valid_loss: 0.48814 test_loss: 0.43454 \n",
      "Validation loss decreased (inf --> 0.488145).  Saving model ...\n",
      "[  2/100] train_loss: 0.40248 valid_loss: 0.35800 test_loss: 0.31083 \n",
      "Validation loss decreased (0.488145 --> 0.357995).  Saving model ...\n",
      "[  3/100] train_loss: 0.31957 valid_loss: 0.30253 test_loss: 0.26114 \n",
      "Validation loss decreased (0.357995 --> 0.302533).  Saving model ...\n",
      "[  4/100] train_loss: 0.27230 valid_loss: 0.26476 test_loss: 0.22339 \n",
      "Validation loss decreased (0.302533 --> 0.264755).  Saving model ...\n",
      "[  5/100] train_loss: 0.23635 valid_loss: 0.24221 test_loss: 0.19840 \n",
      "Validation loss decreased (0.264755 --> 0.242214).  Saving model ...\n",
      "[  6/100] train_loss: 0.21320 valid_loss: 0.22106 test_loss: 0.17507 \n",
      "Validation loss decreased (0.242214 --> 0.221059).  Saving model ...\n",
      "[  7/100] train_loss: 0.20072 valid_loss: 0.20418 test_loss: 0.15730 \n",
      "Validation loss decreased (0.221059 --> 0.204184).  Saving model ...\n",
      "[  8/100] train_loss: 0.18553 valid_loss: 0.19618 test_loss: 0.14882 \n",
      "Validation loss decreased (0.204184 --> 0.196182).  Saving model ...\n",
      "[  9/100] train_loss: 0.18008 valid_loss: 0.18577 test_loss: 0.13895 \n",
      "Validation loss decreased (0.196182 --> 0.185773).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16908 valid_loss: 0.17968 test_loss: 0.13118 \n",
      "Validation loss decreased (0.185773 --> 0.179680).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16282 valid_loss: 0.17643 test_loss: 0.12873 \n",
      "Validation loss decreased (0.179680 --> 0.176425).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15610 valid_loss: 0.17010 test_loss: 0.12273 \n",
      "Validation loss decreased (0.176425 --> 0.170095).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15429 valid_loss: 0.16719 test_loss: 0.12263 \n",
      "Validation loss decreased (0.170095 --> 0.167187).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14882 valid_loss: 0.16606 test_loss: 0.11889 \n",
      "Validation loss decreased (0.167187 --> 0.166063).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14604 valid_loss: 0.16582 test_loss: 0.11841 \n",
      "Validation loss decreased (0.166063 --> 0.165820).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14630 valid_loss: 0.15646 test_loss: 0.11512 \n",
      "Validation loss decreased (0.165820 --> 0.156460).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14056 valid_loss: 0.15285 test_loss: 0.11198 \n",
      "Validation loss decreased (0.156460 --> 0.152846).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13954 valid_loss: 0.15448 test_loss: 0.10911 \n",
      "[ 19/100] train_loss: 0.13861 valid_loss: 0.15020 test_loss: 0.10962 \n",
      "Validation loss decreased (0.152846 --> 0.150203).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13731 valid_loss: 0.14444 test_loss: 0.10581 \n",
      "Validation loss decreased (0.150203 --> 0.144442).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13330 valid_loss: 0.14509 test_loss: 0.10645 \n",
      "[ 22/100] train_loss: 0.13110 valid_loss: 0.14190 test_loss: 0.10663 \n",
      "Validation loss decreased (0.144442 --> 0.141899).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13106 valid_loss: 0.14277 test_loss: 0.10382 \n",
      "[ 24/100] train_loss: 0.12591 valid_loss: 0.13842 test_loss: 0.10373 \n",
      "Validation loss decreased (0.141899 --> 0.138420).  Saving model ...\n",
      "[ 25/100] train_loss: 0.12803 valid_loss: 0.13717 test_loss: 0.10111 \n",
      "Validation loss decreased (0.138420 --> 0.137168).  Saving model ...\n",
      "[ 26/100] train_loss: 0.13361 valid_loss: 0.13972 test_loss: 0.10376 \n",
      "[ 27/100] train_loss: 0.12123 valid_loss: 0.13609 test_loss: 0.10337 \n",
      "Validation loss decreased (0.137168 --> 0.136089).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12459 valid_loss: 0.13353 test_loss: 0.10116 \n",
      "Validation loss decreased (0.136089 --> 0.133529).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12114 valid_loss: 0.13133 test_loss: 0.09925 \n",
      "Validation loss decreased (0.133529 --> 0.131330).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12131 valid_loss: 0.13410 test_loss: 0.10237 \n",
      "[ 31/100] train_loss: 0.12398 valid_loss: 0.13469 test_loss: 0.10189 \n",
      "[ 32/100] train_loss: 0.11763 valid_loss: 0.13141 test_loss: 0.10025 \n",
      "[ 33/100] train_loss: 0.11973 valid_loss: 0.13062 test_loss: 0.09979 \n",
      "Validation loss decreased (0.131330 --> 0.130623).  Saving model ...\n",
      "[ 34/100] train_loss: 0.11467 valid_loss: 0.12862 test_loss: 0.09774 \n",
      "Validation loss decreased (0.130623 --> 0.128616).  Saving model ...\n",
      "[ 35/100] train_loss: 0.11820 valid_loss: 0.12867 test_loss: 0.09917 \n",
      "[ 36/100] train_loss: 0.11770 valid_loss: 0.12769 test_loss: 0.09856 \n",
      "Validation loss decreased (0.128616 --> 0.127693).  Saving model ...\n",
      "[ 37/100] train_loss: 0.11523 valid_loss: 0.12345 test_loss: 0.09600 \n",
      "Validation loss decreased (0.127693 --> 0.123450).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11376 valid_loss: 0.12363 test_loss: 0.09563 \n",
      "[ 39/100] train_loss: 0.11206 valid_loss: 0.12359 test_loss: 0.09749 \n",
      "[ 40/100] train_loss: 0.11378 valid_loss: 0.12858 test_loss: 0.10144 \n",
      "[ 41/100] train_loss: 0.11341 valid_loss: 0.12458 test_loss: 0.09637 \n",
      "[ 42/100] train_loss: 0.11394 valid_loss: 0.12210 test_loss: 0.09553 \n",
      "Validation loss decreased (0.123450 --> 0.122095).  Saving model ...\n",
      "[ 43/100] train_loss: 0.11025 valid_loss: 0.12529 test_loss: 0.09851 \n",
      "[ 44/100] train_loss: 0.11063 valid_loss: 0.12234 test_loss: 0.09388 \n",
      "[ 45/100] train_loss: 0.10845 valid_loss: 0.11889 test_loss: 0.09159 \n",
      "Validation loss decreased (0.122095 --> 0.118894).  Saving model ...\n",
      "[ 46/100] train_loss: 0.11240 valid_loss: 0.12278 test_loss: 0.09577 \n",
      "[ 47/100] train_loss: 0.11247 valid_loss: 0.12034 test_loss: 0.09460 \n",
      "[ 48/100] train_loss: 0.11043 valid_loss: 0.11739 test_loss: 0.09165 \n",
      "Validation loss decreased (0.118894 --> 0.117386).  Saving model ...\n",
      "[ 49/100] train_loss: 0.10739 valid_loss: 0.11965 test_loss: 0.09506 \n",
      "[ 50/100] train_loss: 0.10866 valid_loss: 0.12400 test_loss: 0.09905 \n",
      "[ 51/100] train_loss: 0.10819 valid_loss: 0.12087 test_loss: 0.09426 \n",
      "[ 52/100] train_loss: 0.10843 valid_loss: 0.11965 test_loss: 0.09604 \n",
      "[ 53/100] train_loss: 0.10915 valid_loss: 0.11986 test_loss: 0.09457 \n",
      "[ 54/100] train_loss: 0.10577 valid_loss: 0.11903 test_loss: 0.09508 \n",
      "[ 55/100] train_loss: 0.10504 valid_loss: 0.11613 test_loss: 0.09221 \n",
      "Validation loss decreased (0.117386 --> 0.116129).  Saving model ...\n",
      "[ 56/100] train_loss: 0.10813 valid_loss: 0.11696 test_loss: 0.09422 \n",
      "[ 57/100] train_loss: 0.10765 valid_loss: 0.11815 test_loss: 0.09479 \n",
      "[ 58/100] train_loss: 0.10392 valid_loss: 0.11926 test_loss: 0.09366 \n",
      "[ 59/100] train_loss: 0.10517 valid_loss: 0.11590 test_loss: 0.09029 \n",
      "Validation loss decreased (0.116129 --> 0.115902).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10417 valid_loss: 0.11237 test_loss: 0.08841 \n",
      "Validation loss decreased (0.115902 --> 0.112366).  Saving model ...\n",
      "[ 61/100] train_loss: 0.10241 valid_loss: 0.11039 test_loss: 0.09024 \n",
      "Validation loss decreased (0.112366 --> 0.110395).  Saving model ...\n",
      "[ 62/100] train_loss: 0.10055 valid_loss: 0.11530 test_loss: 0.09213 \n",
      "[ 63/100] train_loss: 0.10176 valid_loss: 0.11297 test_loss: 0.08909 \n",
      "[ 64/100] train_loss: 0.10315 valid_loss: 0.10933 test_loss: 0.08752 \n",
      "Validation loss decreased (0.110395 --> 0.109333).  Saving model ...\n",
      "[ 65/100] train_loss: 0.10038 valid_loss: 0.11557 test_loss: 0.09226 \n",
      "[ 66/100] train_loss: 0.10125 valid_loss: 0.11203 test_loss: 0.08917 \n",
      "[ 67/100] train_loss: 0.09971 valid_loss: 0.11392 test_loss: 0.08902 \n",
      "[ 68/100] train_loss: 0.10233 valid_loss: 0.11259 test_loss: 0.08994 \n",
      "[ 69/100] train_loss: 0.09829 valid_loss: 0.11380 test_loss: 0.09139 \n",
      "[ 70/100] train_loss: 0.09801 valid_loss: 0.11076 test_loss: 0.08901 \n",
      "[ 71/100] train_loss: 0.09905 valid_loss: 0.11265 test_loss: 0.09083 \n",
      "[ 72/100] train_loss: 0.09973 valid_loss: 0.11135 test_loss: 0.08687 \n",
      "[ 73/100] train_loss: 0.10298 valid_loss: 0.11212 test_loss: 0.09137 \n",
      "[ 74/100] train_loss: 0.10193 valid_loss: 0.11214 test_loss: 0.09233 \n",
      "[ 75/100] train_loss: 0.09737 valid_loss: 0.10876 test_loss: 0.08852 \n",
      "Validation loss decreased (0.109333 --> 0.108762).  Saving model ...\n",
      "[ 76/100] train_loss: 0.09786 valid_loss: 0.10887 test_loss: 0.08615 \n",
      "[ 77/100] train_loss: 0.09696 valid_loss: 0.11502 test_loss: 0.08959 \n",
      "[ 78/100] train_loss: 0.09540 valid_loss: 0.10929 test_loss: 0.08654 \n",
      "[ 79/100] train_loss: 0.10116 valid_loss: 0.11116 test_loss: 0.08893 \n",
      "[ 80/100] train_loss: 0.09681 valid_loss: 0.11221 test_loss: 0.08948 \n",
      "[ 81/100] train_loss: 0.09331 valid_loss: 0.11137 test_loss: 0.08830 \n",
      "[ 82/100] train_loss: 0.09775 valid_loss: 0.10863 test_loss: 0.08652 \n",
      "Validation loss decreased (0.108762 --> 0.108629).  Saving model ...\n",
      "[ 83/100] train_loss: 0.09594 valid_loss: 0.11109 test_loss: 0.08986 \n",
      "[ 84/100] train_loss: 0.10073 valid_loss: 0.10934 test_loss: 0.08810 \n",
      "[ 85/100] train_loss: 0.09354 valid_loss: 0.10932 test_loss: 0.08531 \n",
      "[ 86/100] train_loss: 0.09533 valid_loss: 0.10998 test_loss: 0.08612 \n",
      "[ 87/100] train_loss: 0.09240 valid_loss: 0.10953 test_loss: 0.08605 \n",
      "[ 88/100] train_loss: 0.09360 valid_loss: 0.10749 test_loss: 0.08493 \n",
      "Validation loss decreased (0.108629 --> 0.107489).  Saving model ...\n",
      "[ 89/100] train_loss: 0.09387 valid_loss: 0.10748 test_loss: 0.08627 \n",
      "Validation loss decreased (0.107489 --> 0.107478).  Saving model ...\n",
      "[ 90/100] train_loss: 0.09888 valid_loss: 0.10454 test_loss: 0.08447 \n",
      "Validation loss decreased (0.107478 --> 0.104542).  Saving model ...\n",
      "[ 91/100] train_loss: 0.09521 valid_loss: 0.10878 test_loss: 0.08665 \n",
      "[ 92/100] train_loss: 0.09315 valid_loss: 0.10902 test_loss: 0.08877 \n",
      "[ 93/100] train_loss: 0.09206 valid_loss: 0.10697 test_loss: 0.08505 \n",
      "[ 94/100] train_loss: 0.09085 valid_loss: 0.10715 test_loss: 0.08634 \n",
      "[ 95/100] train_loss: 0.09279 valid_loss: 0.10610 test_loss: 0.08601 \n",
      "[ 96/100] train_loss: 0.09488 valid_loss: 0.10860 test_loss: 0.08869 \n",
      "[ 97/100] train_loss: 0.09415 valid_loss: 0.10561 test_loss: 0.08492 \n",
      "[ 98/100] train_loss: 0.09104 valid_loss: 0.10526 test_loss: 0.08565 \n",
      "[ 99/100] train_loss: 0.08916 valid_loss: 0.10661 test_loss: 0.08606 \n",
      "[100/100] train_loss: 0.09104 valid_loss: 0.10443 test_loss: 0.08362 \n",
      "Validation loss decreased (0.104542 --> 0.104432).  Saving model ...\n",
      "Model 13 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 14\n",
      "[  1/100] train_loss: 0.53877 valid_loss: 0.42842 test_loss: 0.39288 \n",
      "Validation loss decreased (inf --> 0.428416).  Saving model ...\n",
      "[  2/100] train_loss: 0.35708 valid_loss: 0.32595 test_loss: 0.27904 \n",
      "Validation loss decreased (0.428416 --> 0.325954).  Saving model ...\n",
      "[  3/100] train_loss: 0.28832 valid_loss: 0.27936 test_loss: 0.23393 \n",
      "Validation loss decreased (0.325954 --> 0.279355).  Saving model ...\n",
      "[  4/100] train_loss: 0.24836 valid_loss: 0.24782 test_loss: 0.20103 \n",
      "Validation loss decreased (0.279355 --> 0.247821).  Saving model ...\n",
      "[  5/100] train_loss: 0.22118 valid_loss: 0.22744 test_loss: 0.18122 \n",
      "Validation loss decreased (0.247821 --> 0.227440).  Saving model ...\n",
      "[  6/100] train_loss: 0.20720 valid_loss: 0.21148 test_loss: 0.16686 \n",
      "Validation loss decreased (0.227440 --> 0.211484).  Saving model ...\n",
      "[  7/100] train_loss: 0.18930 valid_loss: 0.20336 test_loss: 0.15661 \n",
      "Validation loss decreased (0.211484 --> 0.203356).  Saving model ...\n",
      "[  8/100] train_loss: 0.18422 valid_loss: 0.19412 test_loss: 0.14593 \n",
      "Validation loss decreased (0.203356 --> 0.194117).  Saving model ...\n",
      "[  9/100] train_loss: 0.17688 valid_loss: 0.18552 test_loss: 0.13976 \n",
      "Validation loss decreased (0.194117 --> 0.185517).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16808 valid_loss: 0.18734 test_loss: 0.13788 \n",
      "[ 11/100] train_loss: 0.16543 valid_loss: 0.17468 test_loss: 0.12920 \n",
      "Validation loss decreased (0.185517 --> 0.174684).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15611 valid_loss: 0.17497 test_loss: 0.12656 \n",
      "[ 13/100] train_loss: 0.15416 valid_loss: 0.17665 test_loss: 0.12687 \n",
      "[ 14/100] train_loss: 0.15094 valid_loss: 0.16569 test_loss: 0.12094 \n",
      "Validation loss decreased (0.174684 --> 0.165686).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15129 valid_loss: 0.16396 test_loss: 0.12062 \n",
      "Validation loss decreased (0.165686 --> 0.163963).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14770 valid_loss: 0.16316 test_loss: 0.11901 \n",
      "Validation loss decreased (0.163963 --> 0.163156).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13991 valid_loss: 0.16386 test_loss: 0.11738 \n",
      "[ 18/100] train_loss: 0.14493 valid_loss: 0.15982 test_loss: 0.11700 \n",
      "Validation loss decreased (0.163156 --> 0.159822).  Saving model ...\n",
      "[ 19/100] train_loss: 0.14078 valid_loss: 0.15111 test_loss: 0.11129 \n",
      "Validation loss decreased (0.159822 --> 0.151114).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13528 valid_loss: 0.15392 test_loss: 0.11583 \n",
      "[ 21/100] train_loss: 0.13624 valid_loss: 0.14941 test_loss: 0.10940 \n",
      "Validation loss decreased (0.151114 --> 0.149411).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13245 valid_loss: 0.14972 test_loss: 0.11131 \n",
      "[ 23/100] train_loss: 0.13338 valid_loss: 0.15097 test_loss: 0.11339 \n",
      "[ 24/100] train_loss: 0.12916 valid_loss: 0.14600 test_loss: 0.10970 \n",
      "Validation loss decreased (0.149411 --> 0.146004).  Saving model ...\n",
      "[ 25/100] train_loss: 0.12798 valid_loss: 0.14779 test_loss: 0.10945 \n",
      "[ 26/100] train_loss: 0.13009 valid_loss: 0.14440 test_loss: 0.10907 \n",
      "Validation loss decreased (0.146004 --> 0.144398).  Saving model ...\n",
      "[ 27/100] train_loss: 0.12505 valid_loss: 0.14250 test_loss: 0.10622 \n",
      "Validation loss decreased (0.144398 --> 0.142496).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12638 valid_loss: 0.14155 test_loss: 0.10691 \n",
      "Validation loss decreased (0.142496 --> 0.141554).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12270 valid_loss: 0.13935 test_loss: 0.10577 \n",
      "Validation loss decreased (0.141554 --> 0.139349).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12544 valid_loss: 0.13781 test_loss: 0.10424 \n",
      "Validation loss decreased (0.139349 --> 0.137809).  Saving model ...\n",
      "[ 31/100] train_loss: 0.12396 valid_loss: 0.13360 test_loss: 0.10272 \n",
      "Validation loss decreased (0.137809 --> 0.133601).  Saving model ...\n",
      "[ 32/100] train_loss: 0.12226 valid_loss: 0.14035 test_loss: 0.10670 \n",
      "[ 33/100] train_loss: 0.12221 valid_loss: 0.13751 test_loss: 0.10447 \n",
      "[ 34/100] train_loss: 0.12115 valid_loss: 0.13767 test_loss: 0.10530 \n",
      "[ 35/100] train_loss: 0.11944 valid_loss: 0.13665 test_loss: 0.10304 \n",
      "[ 36/100] train_loss: 0.11711 valid_loss: 0.13086 test_loss: 0.10011 \n",
      "Validation loss decreased (0.133601 --> 0.130857).  Saving model ...\n",
      "[ 37/100] train_loss: 0.11971 valid_loss: 0.13003 test_loss: 0.10163 \n",
      "Validation loss decreased (0.130857 --> 0.130030).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11981 valid_loss: 0.13101 test_loss: 0.09933 \n",
      "[ 39/100] train_loss: 0.11664 valid_loss: 0.13098 test_loss: 0.09873 \n",
      "[ 40/100] train_loss: 0.11522 valid_loss: 0.13325 test_loss: 0.10187 \n",
      "[ 41/100] train_loss: 0.11545 valid_loss: 0.12999 test_loss: 0.10048 \n",
      "Validation loss decreased (0.130030 --> 0.129990).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11666 valid_loss: 0.12770 test_loss: 0.09898 \n",
      "Validation loss decreased (0.129990 --> 0.127704).  Saving model ...\n",
      "[ 43/100] train_loss: 0.11607 valid_loss: 0.13317 test_loss: 0.10370 \n",
      "[ 44/100] train_loss: 0.11367 valid_loss: 0.13387 test_loss: 0.10470 \n",
      "[ 45/100] train_loss: 0.10954 valid_loss: 0.12963 test_loss: 0.10032 \n",
      "[ 46/100] train_loss: 0.11325 valid_loss: 0.12858 test_loss: 0.09993 \n",
      "[ 47/100] train_loss: 0.11019 valid_loss: 0.12747 test_loss: 0.09939 \n",
      "Validation loss decreased (0.127704 --> 0.127473).  Saving model ...\n",
      "[ 48/100] train_loss: 0.10916 valid_loss: 0.12707 test_loss: 0.09924 \n",
      "Validation loss decreased (0.127473 --> 0.127066).  Saving model ...\n",
      "[ 49/100] train_loss: 0.10767 valid_loss: 0.12878 test_loss: 0.10039 \n",
      "[ 50/100] train_loss: 0.10939 valid_loss: 0.12619 test_loss: 0.09631 \n",
      "Validation loss decreased (0.127066 --> 0.126188).  Saving model ...\n",
      "[ 51/100] train_loss: 0.11184 valid_loss: 0.12375 test_loss: 0.09665 \n",
      "Validation loss decreased (0.126188 --> 0.123750).  Saving model ...\n",
      "[ 52/100] train_loss: 0.10860 valid_loss: 0.12385 test_loss: 0.09643 \n",
      "[ 53/100] train_loss: 0.11053 valid_loss: 0.12945 test_loss: 0.10146 \n",
      "[ 54/100] train_loss: 0.10796 valid_loss: 0.12433 test_loss: 0.09822 \n",
      "[ 55/100] train_loss: 0.10735 valid_loss: 0.12137 test_loss: 0.09487 \n",
      "Validation loss decreased (0.123750 --> 0.121369).  Saving model ...\n",
      "[ 56/100] train_loss: 0.10524 valid_loss: 0.12571 test_loss: 0.09769 \n",
      "[ 57/100] train_loss: 0.10909 valid_loss: 0.12937 test_loss: 0.10226 \n",
      "[ 58/100] train_loss: 0.10766 valid_loss: 0.12437 test_loss: 0.09949 \n",
      "[ 59/100] train_loss: 0.10735 valid_loss: 0.12491 test_loss: 0.09781 \n",
      "[ 60/100] train_loss: 0.10648 valid_loss: 0.12210 test_loss: 0.09677 \n",
      "[ 61/100] train_loss: 0.10280 valid_loss: 0.12394 test_loss: 0.09698 \n",
      "[ 62/100] train_loss: 0.10496 valid_loss: 0.12441 test_loss: 0.09825 \n",
      "[ 63/100] train_loss: 0.10384 valid_loss: 0.12485 test_loss: 0.09932 \n",
      "[ 64/100] train_loss: 0.10221 valid_loss: 0.11707 test_loss: 0.09170 \n",
      "Validation loss decreased (0.121369 --> 0.117074).  Saving model ...\n",
      "[ 65/100] train_loss: 0.10439 valid_loss: 0.12188 test_loss: 0.09704 \n",
      "[ 66/100] train_loss: 0.10524 valid_loss: 0.11973 test_loss: 0.09727 \n",
      "[ 67/100] train_loss: 0.10300 valid_loss: 0.12565 test_loss: 0.10376 \n",
      "[ 68/100] train_loss: 0.10220 valid_loss: 0.12044 test_loss: 0.09555 \n",
      "[ 69/100] train_loss: 0.10015 valid_loss: 0.11675 test_loss: 0.09172 \n",
      "Validation loss decreased (0.117074 --> 0.116752).  Saving model ...\n",
      "[ 70/100] train_loss: 0.10132 valid_loss: 0.12541 test_loss: 0.10170 \n",
      "[ 71/100] train_loss: 0.10253 valid_loss: 0.11925 test_loss: 0.09494 \n",
      "[ 72/100] train_loss: 0.09685 valid_loss: 0.11737 test_loss: 0.09201 \n",
      "[ 73/100] train_loss: 0.10158 valid_loss: 0.11803 test_loss: 0.09279 \n",
      "[ 74/100] train_loss: 0.09962 valid_loss: 0.12082 test_loss: 0.09283 \n",
      "[ 75/100] train_loss: 0.10296 valid_loss: 0.11760 test_loss: 0.09222 \n",
      "[ 76/100] train_loss: 0.10360 valid_loss: 0.11983 test_loss: 0.09554 \n",
      "[ 77/100] train_loss: 0.09849 valid_loss: 0.11886 test_loss: 0.09547 \n",
      "[ 78/100] train_loss: 0.09923 valid_loss: 0.11761 test_loss: 0.09190 \n",
      "[ 79/100] train_loss: 0.10056 valid_loss: 0.11317 test_loss: 0.08942 \n",
      "Validation loss decreased (0.116752 --> 0.113168).  Saving model ...\n",
      "[ 80/100] train_loss: 0.09667 valid_loss: 0.11298 test_loss: 0.08898 \n",
      "Validation loss decreased (0.113168 --> 0.112977).  Saving model ...\n",
      "[ 81/100] train_loss: 0.09512 valid_loss: 0.11221 test_loss: 0.08752 \n",
      "Validation loss decreased (0.112977 --> 0.112210).  Saving model ...\n",
      "[ 82/100] train_loss: 0.09903 valid_loss: 0.11392 test_loss: 0.09082 \n",
      "[ 83/100] train_loss: 0.09782 valid_loss: 0.11049 test_loss: 0.08691 \n",
      "Validation loss decreased (0.112210 --> 0.110489).  Saving model ...\n",
      "[ 84/100] train_loss: 0.09792 valid_loss: 0.11349 test_loss: 0.09052 \n",
      "[ 85/100] train_loss: 0.09555 valid_loss: 0.11468 test_loss: 0.08979 \n",
      "[ 86/100] train_loss: 0.09589 valid_loss: 0.11411 test_loss: 0.09268 \n",
      "[ 87/100] train_loss: 0.09622 valid_loss: 0.11746 test_loss: 0.09376 \n",
      "[ 88/100] train_loss: 0.09646 valid_loss: 0.11493 test_loss: 0.08943 \n",
      "[ 89/100] train_loss: 0.09612 valid_loss: 0.11445 test_loss: 0.09359 \n",
      "[ 90/100] train_loss: 0.09573 valid_loss: 0.11319 test_loss: 0.08837 \n",
      "[ 91/100] train_loss: 0.09639 valid_loss: 0.11108 test_loss: 0.08459 \n",
      "[ 92/100] train_loss: 0.09475 valid_loss: 0.10875 test_loss: 0.08843 \n",
      "Validation loss decreased (0.110489 --> 0.108748).  Saving model ...\n",
      "[ 93/100] train_loss: 0.09534 valid_loss: 0.11336 test_loss: 0.08975 \n",
      "[ 94/100] train_loss: 0.09490 valid_loss: 0.11152 test_loss: 0.08866 \n",
      "[ 95/100] train_loss: 0.09460 valid_loss: 0.10961 test_loss: 0.08717 \n",
      "[ 96/100] train_loss: 0.09295 valid_loss: 0.11004 test_loss: 0.08727 \n",
      "[ 97/100] train_loss: 0.09403 valid_loss: 0.11029 test_loss: 0.08997 \n",
      "[ 98/100] train_loss: 0.09704 valid_loss: 0.10963 test_loss: 0.08705 \n",
      "[ 99/100] train_loss: 0.09416 valid_loss: 0.10930 test_loss: 0.08887 \n",
      "[100/100] train_loss: 0.09420 valid_loss: 0.10826 test_loss: 0.08797 \n",
      "Validation loss decreased (0.108748 --> 0.108262).  Saving model ...\n",
      "Model 14 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 15\n",
      "[  1/100] train_loss: 0.53555 valid_loss: 0.41701 test_loss: 0.36285 \n",
      "Validation loss decreased (inf --> 0.417007).  Saving model ...\n",
      "[  2/100] train_loss: 0.35846 valid_loss: 0.32338 test_loss: 0.27625 \n",
      "Validation loss decreased (0.417007 --> 0.323379).  Saving model ...\n",
      "[  3/100] train_loss: 0.28924 valid_loss: 0.27848 test_loss: 0.23321 \n",
      "Validation loss decreased (0.323379 --> 0.278480).  Saving model ...\n",
      "[  4/100] train_loss: 0.24937 valid_loss: 0.25015 test_loss: 0.20136 \n",
      "Validation loss decreased (0.278480 --> 0.250149).  Saving model ...\n",
      "[  5/100] train_loss: 0.21799 valid_loss: 0.22877 test_loss: 0.17921 \n",
      "Validation loss decreased (0.250149 --> 0.228766).  Saving model ...\n",
      "[  6/100] train_loss: 0.20133 valid_loss: 0.21188 test_loss: 0.16105 \n",
      "Validation loss decreased (0.228766 --> 0.211883).  Saving model ...\n",
      "[  7/100] train_loss: 0.18904 valid_loss: 0.19913 test_loss: 0.15048 \n",
      "Validation loss decreased (0.211883 --> 0.199132).  Saving model ...\n",
      "[  8/100] train_loss: 0.17851 valid_loss: 0.19194 test_loss: 0.14200 \n",
      "Validation loss decreased (0.199132 --> 0.191939).  Saving model ...\n",
      "[  9/100] train_loss: 0.17196 valid_loss: 0.18518 test_loss: 0.13584 \n",
      "Validation loss decreased (0.191939 --> 0.185179).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16440 valid_loss: 0.17586 test_loss: 0.12948 \n",
      "Validation loss decreased (0.185179 --> 0.175859).  Saving model ...\n",
      "[ 11/100] train_loss: 0.15790 valid_loss: 0.17934 test_loss: 0.12925 \n",
      "[ 12/100] train_loss: 0.15125 valid_loss: 0.17794 test_loss: 0.12628 \n",
      "[ 13/100] train_loss: 0.15165 valid_loss: 0.17305 test_loss: 0.12465 \n",
      "Validation loss decreased (0.175859 --> 0.173048).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15021 valid_loss: 0.17442 test_loss: 0.12604 \n",
      "[ 15/100] train_loss: 0.14958 valid_loss: 0.17487 test_loss: 0.12587 \n",
      "[ 16/100] train_loss: 0.14365 valid_loss: 0.16861 test_loss: 0.12215 \n",
      "Validation loss decreased (0.173048 --> 0.168605).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13981 valid_loss: 0.16461 test_loss: 0.11963 \n",
      "Validation loss decreased (0.168605 --> 0.164614).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13829 valid_loss: 0.16057 test_loss: 0.11785 \n",
      "Validation loss decreased (0.164614 --> 0.160570).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13764 valid_loss: 0.16202 test_loss: 0.11879 \n",
      "[ 20/100] train_loss: 0.13424 valid_loss: 0.16618 test_loss: 0.12198 \n",
      "[ 21/100] train_loss: 0.13308 valid_loss: 0.16220 test_loss: 0.11990 \n",
      "[ 22/100] train_loss: 0.13392 valid_loss: 0.15714 test_loss: 0.11467 \n",
      "Validation loss decreased (0.160570 --> 0.157139).  Saving model ...\n",
      "[ 23/100] train_loss: 0.12886 valid_loss: 0.15386 test_loss: 0.11501 \n",
      "Validation loss decreased (0.157139 --> 0.153860).  Saving model ...\n",
      "[ 24/100] train_loss: 0.12774 valid_loss: 0.15706 test_loss: 0.11572 \n",
      "[ 25/100] train_loss: 0.12998 valid_loss: 0.14672 test_loss: 0.10930 \n",
      "Validation loss decreased (0.153860 --> 0.146719).  Saving model ...\n",
      "[ 26/100] train_loss: 0.12540 valid_loss: 0.15300 test_loss: 0.11393 \n",
      "[ 27/100] train_loss: 0.12638 valid_loss: 0.14442 test_loss: 0.10591 \n",
      "Validation loss decreased (0.146719 --> 0.144424).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12461 valid_loss: 0.15062 test_loss: 0.11685 \n",
      "[ 29/100] train_loss: 0.12487 valid_loss: 0.14651 test_loss: 0.10994 \n",
      "[ 30/100] train_loss: 0.12338 valid_loss: 0.14957 test_loss: 0.11202 \n",
      "[ 31/100] train_loss: 0.12197 valid_loss: 0.14025 test_loss: 0.10559 \n",
      "Validation loss decreased (0.144424 --> 0.140255).  Saving model ...\n",
      "[ 32/100] train_loss: 0.11877 valid_loss: 0.14573 test_loss: 0.10945 \n",
      "[ 33/100] train_loss: 0.12243 valid_loss: 0.13837 test_loss: 0.10653 \n",
      "Validation loss decreased (0.140255 --> 0.138373).  Saving model ...\n",
      "[ 34/100] train_loss: 0.11764 valid_loss: 0.13869 test_loss: 0.10525 \n",
      "[ 35/100] train_loss: 0.11848 valid_loss: 0.13896 test_loss: 0.10595 \n",
      "[ 36/100] train_loss: 0.12057 valid_loss: 0.13652 test_loss: 0.10623 \n",
      "Validation loss decreased (0.138373 --> 0.136519).  Saving model ...\n",
      "[ 37/100] train_loss: 0.11914 valid_loss: 0.14062 test_loss: 0.10719 \n",
      "[ 38/100] train_loss: 0.11659 valid_loss: 0.13690 test_loss: 0.10489 \n",
      "[ 39/100] train_loss: 0.11640 valid_loss: 0.13684 test_loss: 0.10752 \n",
      "[ 40/100] train_loss: 0.11554 valid_loss: 0.13437 test_loss: 0.10621 \n",
      "Validation loss decreased (0.136519 --> 0.134368).  Saving model ...\n",
      "[ 41/100] train_loss: 0.11284 valid_loss: 0.13757 test_loss: 0.10804 \n",
      "[ 42/100] train_loss: 0.11794 valid_loss: 0.13740 test_loss: 0.10609 \n",
      "[ 43/100] train_loss: 0.11452 valid_loss: 0.13329 test_loss: 0.10603 \n",
      "Validation loss decreased (0.134368 --> 0.133290).  Saving model ...\n",
      "[ 44/100] train_loss: 0.11139 valid_loss: 0.13113 test_loss: 0.10287 \n",
      "Validation loss decreased (0.133290 --> 0.131125).  Saving model ...\n",
      "[ 45/100] train_loss: 0.11297 valid_loss: 0.13007 test_loss: 0.10414 \n",
      "Validation loss decreased (0.131125 --> 0.130069).  Saving model ...\n",
      "[ 46/100] train_loss: 0.11112 valid_loss: 0.13046 test_loss: 0.10129 \n",
      "[ 47/100] train_loss: 0.11221 valid_loss: 0.13740 test_loss: 0.10719 \n",
      "[ 48/100] train_loss: 0.10826 valid_loss: 0.13648 test_loss: 0.10500 \n",
      "[ 49/100] train_loss: 0.11254 valid_loss: 0.13460 test_loss: 0.10554 \n",
      "[ 50/100] train_loss: 0.11185 valid_loss: 0.12890 test_loss: 0.10113 \n",
      "Validation loss decreased (0.130069 --> 0.128903).  Saving model ...\n",
      "[ 51/100] train_loss: 0.10979 valid_loss: 0.12671 test_loss: 0.09952 \n",
      "Validation loss decreased (0.128903 --> 0.126713).  Saving model ...\n",
      "[ 52/100] train_loss: 0.11221 valid_loss: 0.12856 test_loss: 0.09975 \n",
      "[ 53/100] train_loss: 0.10621 valid_loss: 0.12833 test_loss: 0.10143 \n",
      "[ 54/100] train_loss: 0.10681 valid_loss: 0.12904 test_loss: 0.09908 \n",
      "[ 55/100] train_loss: 0.10605 valid_loss: 0.12714 test_loss: 0.09856 \n",
      "[ 56/100] train_loss: 0.11090 valid_loss: 0.12409 test_loss: 0.09655 \n",
      "Validation loss decreased (0.126713 --> 0.124087).  Saving model ...\n",
      "[ 57/100] train_loss: 0.10386 valid_loss: 0.12667 test_loss: 0.10127 \n",
      "[ 58/100] train_loss: 0.10832 valid_loss: 0.12539 test_loss: 0.09959 \n",
      "[ 59/100] train_loss: 0.10696 valid_loss: 0.12154 test_loss: 0.09472 \n",
      "Validation loss decreased (0.124087 --> 0.121543).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10483 valid_loss: 0.12418 test_loss: 0.09529 \n",
      "[ 61/100] train_loss: 0.10150 valid_loss: 0.12661 test_loss: 0.09995 \n",
      "[ 62/100] train_loss: 0.10818 valid_loss: 0.12486 test_loss: 0.09831 \n",
      "[ 63/100] train_loss: 0.10196 valid_loss: 0.12188 test_loss: 0.09581 \n",
      "[ 64/100] train_loss: 0.10396 valid_loss: 0.12408 test_loss: 0.10013 \n",
      "[ 65/100] train_loss: 0.10681 valid_loss: 0.12111 test_loss: 0.09614 \n",
      "Validation loss decreased (0.121543 --> 0.121109).  Saving model ...\n",
      "[ 66/100] train_loss: 0.10814 valid_loss: 0.11878 test_loss: 0.09381 \n",
      "Validation loss decreased (0.121109 --> 0.118777).  Saving model ...\n",
      "[ 67/100] train_loss: 0.10195 valid_loss: 0.12233 test_loss: 0.09500 \n",
      "[ 68/100] train_loss: 0.10280 valid_loss: 0.12175 test_loss: 0.09452 \n",
      "[ 69/100] train_loss: 0.09998 valid_loss: 0.12191 test_loss: 0.09591 \n",
      "[ 70/100] train_loss: 0.10043 valid_loss: 0.11816 test_loss: 0.09276 \n",
      "Validation loss decreased (0.118777 --> 0.118158).  Saving model ...\n",
      "[ 71/100] train_loss: 0.10082 valid_loss: 0.12117 test_loss: 0.09626 \n",
      "[ 72/100] train_loss: 0.09997 valid_loss: 0.11830 test_loss: 0.09381 \n",
      "[ 73/100] train_loss: 0.10307 valid_loss: 0.12704 test_loss: 0.10146 \n",
      "[ 74/100] train_loss: 0.09963 valid_loss: 0.11989 test_loss: 0.09519 \n",
      "[ 75/100] train_loss: 0.09783 valid_loss: 0.11906 test_loss: 0.09440 \n",
      "[ 76/100] train_loss: 0.10112 valid_loss: 0.12089 test_loss: 0.09681 \n",
      "[ 77/100] train_loss: 0.09933 valid_loss: 0.11625 test_loss: 0.09123 \n",
      "Validation loss decreased (0.118158 --> 0.116246).  Saving model ...\n",
      "[ 78/100] train_loss: 0.09842 valid_loss: 0.11636 test_loss: 0.09317 \n",
      "[ 79/100] train_loss: 0.09881 valid_loss: 0.11694 test_loss: 0.09296 \n",
      "[ 80/100] train_loss: 0.09792 valid_loss: 0.11521 test_loss: 0.08995 \n",
      "Validation loss decreased (0.116246 --> 0.115208).  Saving model ...\n",
      "[ 81/100] train_loss: 0.09834 valid_loss: 0.11617 test_loss: 0.09264 \n",
      "[ 82/100] train_loss: 0.09870 valid_loss: 0.11575 test_loss: 0.08989 \n",
      "[ 83/100] train_loss: 0.10240 valid_loss: 0.11573 test_loss: 0.09021 \n",
      "[ 84/100] train_loss: 0.09872 valid_loss: 0.11566 test_loss: 0.08932 \n",
      "[ 85/100] train_loss: 0.09610 valid_loss: 0.11802 test_loss: 0.09142 \n",
      "[ 86/100] train_loss: 0.09698 valid_loss: 0.11413 test_loss: 0.08880 \n",
      "Validation loss decreased (0.115208 --> 0.114132).  Saving model ...\n",
      "[ 87/100] train_loss: 0.09860 valid_loss: 0.11341 test_loss: 0.09045 \n",
      "Validation loss decreased (0.114132 --> 0.113406).  Saving model ...\n",
      "[ 88/100] train_loss: 0.09497 valid_loss: 0.11554 test_loss: 0.09096 \n",
      "[ 89/100] train_loss: 0.09446 valid_loss: 0.11515 test_loss: 0.09151 \n",
      "[ 90/100] train_loss: 0.09625 valid_loss: 0.11312 test_loss: 0.08700 \n",
      "Validation loss decreased (0.113406 --> 0.113124).  Saving model ...\n",
      "[ 91/100] train_loss: 0.09586 valid_loss: 0.11980 test_loss: 0.09732 \n",
      "[ 92/100] train_loss: 0.09666 valid_loss: 0.11658 test_loss: 0.09358 \n",
      "[ 93/100] train_loss: 0.09491 valid_loss: 0.11309 test_loss: 0.08846 \n",
      "Validation loss decreased (0.113124 --> 0.113087).  Saving model ...\n",
      "[ 94/100] train_loss: 0.09496 valid_loss: 0.11245 test_loss: 0.08892 \n",
      "Validation loss decreased (0.113087 --> 0.112449).  Saving model ...\n",
      "[ 95/100] train_loss: 0.09619 valid_loss: 0.11290 test_loss: 0.09037 \n",
      "[ 96/100] train_loss: 0.09463 valid_loss: 0.10949 test_loss: 0.08758 \n",
      "Validation loss decreased (0.112449 --> 0.109492).  Saving model ...\n",
      "[ 97/100] train_loss: 0.09640 valid_loss: 0.11361 test_loss: 0.08948 \n",
      "[ 98/100] train_loss: 0.09788 valid_loss: 0.11456 test_loss: 0.09436 \n",
      "[ 99/100] train_loss: 0.09340 valid_loss: 0.11554 test_loss: 0.08967 \n",
      "[100/100] train_loss: 0.09368 valid_loss: 0.11216 test_loss: 0.08728 \n",
      "Model 15 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 16\n",
      "[  1/100] train_loss: 0.64213 valid_loss: 0.56151 test_loss: 0.53053 \n",
      "Validation loss decreased (inf --> 0.561510).  Saving model ...\n",
      "[  2/100] train_loss: 0.47183 valid_loss: 0.40565 test_loss: 0.35937 \n",
      "Validation loss decreased (0.561510 --> 0.405654).  Saving model ...\n",
      "[  3/100] train_loss: 0.35454 valid_loss: 0.32356 test_loss: 0.28546 \n",
      "Validation loss decreased (0.405654 --> 0.323556).  Saving model ...\n",
      "[  4/100] train_loss: 0.28660 valid_loss: 0.27683 test_loss: 0.23618 \n",
      "Validation loss decreased (0.323556 --> 0.276830).  Saving model ...\n",
      "[  5/100] train_loss: 0.24863 valid_loss: 0.24487 test_loss: 0.20297 \n",
      "Validation loss decreased (0.276830 --> 0.244872).  Saving model ...\n",
      "[  6/100] train_loss: 0.21961 valid_loss: 0.22358 test_loss: 0.17718 \n",
      "Validation loss decreased (0.244872 --> 0.223575).  Saving model ...\n",
      "[  7/100] train_loss: 0.20211 valid_loss: 0.20689 test_loss: 0.16005 \n",
      "Validation loss decreased (0.223575 --> 0.206891).  Saving model ...\n",
      "[  8/100] train_loss: 0.19017 valid_loss: 0.19895 test_loss: 0.14900 \n",
      "Validation loss decreased (0.206891 --> 0.198950).  Saving model ...\n",
      "[  9/100] train_loss: 0.18110 valid_loss: 0.19236 test_loss: 0.14136 \n",
      "Validation loss decreased (0.198950 --> 0.192357).  Saving model ...\n",
      "[ 10/100] train_loss: 0.17588 valid_loss: 0.18822 test_loss: 0.13634 \n",
      "Validation loss decreased (0.192357 --> 0.188223).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16806 valid_loss: 0.17744 test_loss: 0.13105 \n",
      "Validation loss decreased (0.188223 --> 0.177444).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16163 valid_loss: 0.17678 test_loss: 0.12853 \n",
      "Validation loss decreased (0.177444 --> 0.176782).  Saving model ...\n",
      "[ 13/100] train_loss: 0.16052 valid_loss: 0.17230 test_loss: 0.12470 \n",
      "Validation loss decreased (0.176782 --> 0.172296).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15335 valid_loss: 0.16787 test_loss: 0.12244 \n",
      "Validation loss decreased (0.172296 --> 0.167870).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15191 valid_loss: 0.16898 test_loss: 0.12364 \n",
      "[ 16/100] train_loss: 0.14689 valid_loss: 0.17111 test_loss: 0.12211 \n",
      "[ 17/100] train_loss: 0.14934 valid_loss: 0.16749 test_loss: 0.11931 \n",
      "Validation loss decreased (0.167870 --> 0.167486).  Saving model ...\n",
      "[ 18/100] train_loss: 0.14393 valid_loss: 0.15757 test_loss: 0.11603 \n",
      "Validation loss decreased (0.167486 --> 0.157568).  Saving model ...\n",
      "[ 19/100] train_loss: 0.14214 valid_loss: 0.15809 test_loss: 0.11507 \n",
      "[ 20/100] train_loss: 0.14192 valid_loss: 0.15753 test_loss: 0.11449 \n",
      "Validation loss decreased (0.157568 --> 0.157533).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13944 valid_loss: 0.15561 test_loss: 0.11286 \n",
      "Validation loss decreased (0.157533 --> 0.155609).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13488 valid_loss: 0.15241 test_loss: 0.11016 \n",
      "Validation loss decreased (0.155609 --> 0.152411).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13374 valid_loss: 0.15085 test_loss: 0.10849 \n",
      "Validation loss decreased (0.152411 --> 0.150851).  Saving model ...\n",
      "[ 24/100] train_loss: 0.13179 valid_loss: 0.15126 test_loss: 0.11002 \n",
      "[ 25/100] train_loss: 0.13308 valid_loss: 0.14794 test_loss: 0.11058 \n",
      "Validation loss decreased (0.150851 --> 0.147940).  Saving model ...\n",
      "[ 26/100] train_loss: 0.13449 valid_loss: 0.15130 test_loss: 0.11214 \n",
      "[ 27/100] train_loss: 0.13052 valid_loss: 0.14587 test_loss: 0.10906 \n",
      "Validation loss decreased (0.147940 --> 0.145867).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12859 valid_loss: 0.14457 test_loss: 0.10887 \n",
      "Validation loss decreased (0.145867 --> 0.144575).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12596 valid_loss: 0.14043 test_loss: 0.10465 \n",
      "Validation loss decreased (0.144575 --> 0.140433).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12532 valid_loss: 0.14392 test_loss: 0.10790 \n",
      "[ 31/100] train_loss: 0.12638 valid_loss: 0.14492 test_loss: 0.10939 \n",
      "[ 32/100] train_loss: 0.12262 valid_loss: 0.14191 test_loss: 0.10690 \n",
      "[ 33/100] train_loss: 0.12366 valid_loss: 0.14125 test_loss: 0.10869 \n",
      "[ 34/100] train_loss: 0.12069 valid_loss: 0.13504 test_loss: 0.10199 \n",
      "Validation loss decreased (0.140433 --> 0.135037).  Saving model ...\n",
      "[ 35/100] train_loss: 0.11981 valid_loss: 0.13561 test_loss: 0.10337 \n",
      "[ 36/100] train_loss: 0.11819 valid_loss: 0.13588 test_loss: 0.10462 \n",
      "[ 37/100] train_loss: 0.12147 valid_loss: 0.13359 test_loss: 0.10257 \n",
      "Validation loss decreased (0.135037 --> 0.133595).  Saving model ...\n",
      "[ 38/100] train_loss: 0.12044 valid_loss: 0.13589 test_loss: 0.10295 \n",
      "[ 39/100] train_loss: 0.11614 valid_loss: 0.13137 test_loss: 0.10006 \n",
      "Validation loss decreased (0.133595 --> 0.131365).  Saving model ...\n",
      "[ 40/100] train_loss: 0.11637 valid_loss: 0.13680 test_loss: 0.10347 \n",
      "[ 41/100] train_loss: 0.11874 valid_loss: 0.13716 test_loss: 0.10764 \n",
      "[ 42/100] train_loss: 0.11927 valid_loss: 0.12823 test_loss: 0.09625 \n",
      "Validation loss decreased (0.131365 --> 0.128225).  Saving model ...\n",
      "[ 43/100] train_loss: 0.11377 valid_loss: 0.13529 test_loss: 0.10310 \n",
      "[ 44/100] train_loss: 0.11858 valid_loss: 0.12760 test_loss: 0.09815 \n",
      "Validation loss decreased (0.128225 --> 0.127597).  Saving model ...\n",
      "[ 45/100] train_loss: 0.11451 valid_loss: 0.12958 test_loss: 0.10039 \n",
      "[ 46/100] train_loss: 0.11256 valid_loss: 0.12931 test_loss: 0.10087 \n",
      "[ 47/100] train_loss: 0.11089 valid_loss: 0.13098 test_loss: 0.10023 \n",
      "[ 48/100] train_loss: 0.11535 valid_loss: 0.13063 test_loss: 0.10281 \n",
      "[ 49/100] train_loss: 0.11272 valid_loss: 0.12688 test_loss: 0.09838 \n",
      "Validation loss decreased (0.127597 --> 0.126878).  Saving model ...\n",
      "[ 50/100] train_loss: 0.11126 valid_loss: 0.12497 test_loss: 0.09521 \n",
      "Validation loss decreased (0.126878 --> 0.124966).  Saving model ...\n",
      "[ 51/100] train_loss: 0.11103 valid_loss: 0.12826 test_loss: 0.09805 \n",
      "[ 52/100] train_loss: 0.10927 valid_loss: 0.12782 test_loss: 0.09892 \n",
      "[ 53/100] train_loss: 0.11085 valid_loss: 0.12618 test_loss: 0.09541 \n",
      "[ 54/100] train_loss: 0.10961 valid_loss: 0.12284 test_loss: 0.09923 \n",
      "Validation loss decreased (0.124966 --> 0.122836).  Saving model ...\n",
      "[ 55/100] train_loss: 0.10761 valid_loss: 0.12312 test_loss: 0.09618 \n",
      "[ 56/100] train_loss: 0.10919 valid_loss: 0.12320 test_loss: 0.09537 \n",
      "[ 57/100] train_loss: 0.11043 valid_loss: 0.12176 test_loss: 0.09270 \n",
      "Validation loss decreased (0.122836 --> 0.121761).  Saving model ...\n",
      "[ 58/100] train_loss: 0.10850 valid_loss: 0.12268 test_loss: 0.09534 \n",
      "[ 59/100] train_loss: 0.10704 valid_loss: 0.12060 test_loss: 0.09379 \n",
      "Validation loss decreased (0.121761 --> 0.120599).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10683 valid_loss: 0.12372 test_loss: 0.09566 \n",
      "[ 61/100] train_loss: 0.10976 valid_loss: 0.12088 test_loss: 0.09361 \n",
      "[ 62/100] train_loss: 0.10691 valid_loss: 0.11806 test_loss: 0.09196 \n",
      "Validation loss decreased (0.120599 --> 0.118060).  Saving model ...\n",
      "[ 63/100] train_loss: 0.10751 valid_loss: 0.11866 test_loss: 0.09023 \n",
      "[ 64/100] train_loss: 0.10479 valid_loss: 0.12049 test_loss: 0.09341 \n",
      "[ 65/100] train_loss: 0.10736 valid_loss: 0.12333 test_loss: 0.09550 \n",
      "[ 66/100] train_loss: 0.10372 valid_loss: 0.11907 test_loss: 0.09084 \n",
      "[ 67/100] train_loss: 0.10180 valid_loss: 0.11663 test_loss: 0.09086 \n",
      "Validation loss decreased (0.118060 --> 0.116631).  Saving model ...\n",
      "[ 68/100] train_loss: 0.10559 valid_loss: 0.11980 test_loss: 0.09344 \n",
      "[ 69/100] train_loss: 0.10795 valid_loss: 0.11712 test_loss: 0.09039 \n",
      "[ 70/100] train_loss: 0.10379 valid_loss: 0.11604 test_loss: 0.09037 \n",
      "Validation loss decreased (0.116631 --> 0.116038).  Saving model ...\n",
      "[ 71/100] train_loss: 0.10435 valid_loss: 0.11623 test_loss: 0.09268 \n",
      "[ 72/100] train_loss: 0.10279 valid_loss: 0.11830 test_loss: 0.09279 \n",
      "[ 73/100] train_loss: 0.10146 valid_loss: 0.11805 test_loss: 0.09305 \n",
      "[ 74/100] train_loss: 0.10118 valid_loss: 0.11694 test_loss: 0.09042 \n",
      "[ 75/100] train_loss: 0.09756 valid_loss: 0.11505 test_loss: 0.09059 \n",
      "Validation loss decreased (0.116038 --> 0.115047).  Saving model ...\n",
      "[ 76/100] train_loss: 0.09991 valid_loss: 0.11434 test_loss: 0.08990 \n",
      "Validation loss decreased (0.115047 --> 0.114342).  Saving model ...\n",
      "[ 77/100] train_loss: 0.09934 valid_loss: 0.11538 test_loss: 0.08922 \n",
      "[ 78/100] train_loss: 0.09850 valid_loss: 0.11683 test_loss: 0.08946 \n",
      "[ 79/100] train_loss: 0.10159 valid_loss: 0.11484 test_loss: 0.09056 \n",
      "[ 80/100] train_loss: 0.09834 valid_loss: 0.11095 test_loss: 0.08642 \n",
      "Validation loss decreased (0.114342 --> 0.110953).  Saving model ...\n",
      "[ 81/100] train_loss: 0.10021 valid_loss: 0.11521 test_loss: 0.08876 \n",
      "[ 82/100] train_loss: 0.09724 valid_loss: 0.11363 test_loss: 0.08803 \n",
      "[ 83/100] train_loss: 0.10036 valid_loss: 0.11273 test_loss: 0.08885 \n",
      "[ 84/100] train_loss: 0.09860 valid_loss: 0.11404 test_loss: 0.08803 \n",
      "[ 85/100] train_loss: 0.09919 valid_loss: 0.11305 test_loss: 0.08809 \n",
      "[ 86/100] train_loss: 0.09986 valid_loss: 0.11253 test_loss: 0.08851 \n",
      "[ 87/100] train_loss: 0.09602 valid_loss: 0.11088 test_loss: 0.08409 \n",
      "Validation loss decreased (0.110953 --> 0.110884).  Saving model ...\n",
      "[ 88/100] train_loss: 0.09622 valid_loss: 0.10976 test_loss: 0.08604 \n",
      "Validation loss decreased (0.110884 --> 0.109761).  Saving model ...\n",
      "[ 89/100] train_loss: 0.09886 valid_loss: 0.11363 test_loss: 0.08764 \n",
      "[ 90/100] train_loss: 0.09703 valid_loss: 0.11077 test_loss: 0.08713 \n",
      "[ 91/100] train_loss: 0.09532 valid_loss: 0.10879 test_loss: 0.08436 \n",
      "Validation loss decreased (0.109761 --> 0.108794).  Saving model ...\n",
      "[ 92/100] train_loss: 0.09777 valid_loss: 0.11110 test_loss: 0.08896 \n",
      "[ 93/100] train_loss: 0.09685 valid_loss: 0.11226 test_loss: 0.08689 \n",
      "[ 94/100] train_loss: 0.09604 valid_loss: 0.11141 test_loss: 0.08617 \n",
      "[ 95/100] train_loss: 0.09995 valid_loss: 0.11180 test_loss: 0.08964 \n",
      "[ 96/100] train_loss: 0.09607 valid_loss: 0.11154 test_loss: 0.08701 \n",
      "[ 97/100] train_loss: 0.09406 valid_loss: 0.11083 test_loss: 0.08511 \n",
      "[ 98/100] train_loss: 0.09318 valid_loss: 0.10995 test_loss: 0.08532 \n",
      "[ 99/100] train_loss: 0.09306 valid_loss: 0.11135 test_loss: 0.08573 \n",
      "[100/100] train_loss: 0.09553 valid_loss: 0.11058 test_loss: 0.08689 \n",
      "Model 16 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 17\n",
      "[  1/100] train_loss: 0.56375 valid_loss: 0.44835 test_loss: 0.41144 \n",
      "Validation loss decreased (inf --> 0.448349).  Saving model ...\n",
      "[  2/100] train_loss: 0.38991 valid_loss: 0.33731 test_loss: 0.29494 \n",
      "Validation loss decreased (0.448349 --> 0.337312).  Saving model ...\n",
      "[  3/100] train_loss: 0.30055 valid_loss: 0.28288 test_loss: 0.23612 \n",
      "Validation loss decreased (0.337312 --> 0.282880).  Saving model ...\n",
      "[  4/100] train_loss: 0.24986 valid_loss: 0.24371 test_loss: 0.19839 \n",
      "Validation loss decreased (0.282880 --> 0.243715).  Saving model ...\n",
      "[  5/100] train_loss: 0.22058 valid_loss: 0.22454 test_loss: 0.17293 \n",
      "Validation loss decreased (0.243715 --> 0.224536).  Saving model ...\n",
      "[  6/100] train_loss: 0.20090 valid_loss: 0.20254 test_loss: 0.15529 \n",
      "Validation loss decreased (0.224536 --> 0.202536).  Saving model ...\n",
      "[  7/100] train_loss: 0.18380 valid_loss: 0.19437 test_loss: 0.14522 \n",
      "Validation loss decreased (0.202536 --> 0.194375).  Saving model ...\n",
      "[  8/100] train_loss: 0.17710 valid_loss: 0.18794 test_loss: 0.13874 \n",
      "Validation loss decreased (0.194375 --> 0.187943).  Saving model ...\n",
      "[  9/100] train_loss: 0.16852 valid_loss: 0.18018 test_loss: 0.13183 \n",
      "Validation loss decreased (0.187943 --> 0.180177).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16400 valid_loss: 0.17321 test_loss: 0.12597 \n",
      "Validation loss decreased (0.180177 --> 0.173209).  Saving model ...\n",
      "[ 11/100] train_loss: 0.15305 valid_loss: 0.17159 test_loss: 0.12678 \n",
      "Validation loss decreased (0.173209 --> 0.171589).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15326 valid_loss: 0.16887 test_loss: 0.12142 \n",
      "Validation loss decreased (0.171589 --> 0.168871).  Saving model ...\n",
      "[ 13/100] train_loss: 0.14909 valid_loss: 0.16131 test_loss: 0.11783 \n",
      "Validation loss decreased (0.168871 --> 0.161306).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14442 valid_loss: 0.16044 test_loss: 0.11661 \n",
      "Validation loss decreased (0.161306 --> 0.160445).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14783 valid_loss: 0.15532 test_loss: 0.11253 \n",
      "Validation loss decreased (0.160445 --> 0.155320).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14170 valid_loss: 0.15617 test_loss: 0.11421 \n",
      "[ 17/100] train_loss: 0.13552 valid_loss: 0.15274 test_loss: 0.11089 \n",
      "Validation loss decreased (0.155320 --> 0.152739).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13380 valid_loss: 0.15081 test_loss: 0.11100 \n",
      "Validation loss decreased (0.152739 --> 0.150813).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13549 valid_loss: 0.14987 test_loss: 0.10999 \n",
      "Validation loss decreased (0.150813 --> 0.149870).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13103 valid_loss: 0.14679 test_loss: 0.10900 \n",
      "Validation loss decreased (0.149870 --> 0.146787).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12984 valid_loss: 0.14070 test_loss: 0.10375 \n",
      "Validation loss decreased (0.146787 --> 0.140699).  Saving model ...\n",
      "[ 22/100] train_loss: 0.12946 valid_loss: 0.14552 test_loss: 0.10668 \n",
      "[ 23/100] train_loss: 0.12751 valid_loss: 0.14089 test_loss: 0.10516 \n",
      "[ 24/100] train_loss: 0.12836 valid_loss: 0.13496 test_loss: 0.10387 \n",
      "Validation loss decreased (0.140699 --> 0.134963).  Saving model ...\n",
      "[ 25/100] train_loss: 0.12333 valid_loss: 0.13452 test_loss: 0.10212 \n",
      "Validation loss decreased (0.134963 --> 0.134522).  Saving model ...\n",
      "[ 26/100] train_loss: 0.12157 valid_loss: 0.13547 test_loss: 0.10193 \n",
      "[ 27/100] train_loss: 0.12134 valid_loss: 0.13511 test_loss: 0.10227 \n",
      "[ 28/100] train_loss: 0.11956 valid_loss: 0.13415 test_loss: 0.10156 \n",
      "Validation loss decreased (0.134522 --> 0.134152).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12202 valid_loss: 0.13143 test_loss: 0.10252 \n",
      "Validation loss decreased (0.134152 --> 0.131426).  Saving model ...\n",
      "[ 30/100] train_loss: 0.11723 valid_loss: 0.13294 test_loss: 0.10060 \n",
      "[ 31/100] train_loss: 0.11669 valid_loss: 0.13189 test_loss: 0.10057 \n",
      "[ 32/100] train_loss: 0.11693 valid_loss: 0.13112 test_loss: 0.10111 \n",
      "Validation loss decreased (0.131426 --> 0.131116).  Saving model ...\n",
      "[ 33/100] train_loss: 0.11972 valid_loss: 0.13013 test_loss: 0.10227 \n",
      "Validation loss decreased (0.131116 --> 0.130127).  Saving model ...\n",
      "[ 34/100] train_loss: 0.11218 valid_loss: 0.12882 test_loss: 0.09876 \n",
      "Validation loss decreased (0.130127 --> 0.128825).  Saving model ...\n",
      "[ 35/100] train_loss: 0.11473 valid_loss: 0.12540 test_loss: 0.09798 \n",
      "Validation loss decreased (0.128825 --> 0.125403).  Saving model ...\n",
      "[ 36/100] train_loss: 0.11321 valid_loss: 0.12576 test_loss: 0.09834 \n",
      "[ 37/100] train_loss: 0.11133 valid_loss: 0.12737 test_loss: 0.09973 \n",
      "[ 38/100] train_loss: 0.11665 valid_loss: 0.12478 test_loss: 0.09714 \n",
      "Validation loss decreased (0.125403 --> 0.124785).  Saving model ...\n",
      "[ 39/100] train_loss: 0.11441 valid_loss: 0.12778 test_loss: 0.10318 \n",
      "[ 40/100] train_loss: 0.11159 valid_loss: 0.12846 test_loss: 0.10100 \n",
      "[ 41/100] train_loss: 0.11294 valid_loss: 0.12355 test_loss: 0.09889 \n",
      "Validation loss decreased (0.124785 --> 0.123545).  Saving model ...\n",
      "[ 42/100] train_loss: 0.10959 valid_loss: 0.12737 test_loss: 0.09775 \n",
      "[ 43/100] train_loss: 0.10778 valid_loss: 0.12357 test_loss: 0.09969 \n",
      "[ 44/100] train_loss: 0.10662 valid_loss: 0.11849 test_loss: 0.09483 \n",
      "Validation loss decreased (0.123545 --> 0.118486).  Saving model ...\n",
      "[ 45/100] train_loss: 0.10582 valid_loss: 0.11950 test_loss: 0.09497 \n",
      "[ 46/100] train_loss: 0.10831 valid_loss: 0.12212 test_loss: 0.09697 \n",
      "[ 47/100] train_loss: 0.10588 valid_loss: 0.12492 test_loss: 0.09896 \n",
      "[ 48/100] train_loss: 0.10955 valid_loss: 0.12145 test_loss: 0.09591 \n",
      "[ 49/100] train_loss: 0.10651 valid_loss: 0.12351 test_loss: 0.09615 \n",
      "[ 50/100] train_loss: 0.10581 valid_loss: 0.12059 test_loss: 0.09779 \n",
      "[ 51/100] train_loss: 0.10510 valid_loss: 0.11950 test_loss: 0.09221 \n",
      "[ 52/100] train_loss: 0.10559 valid_loss: 0.12019 test_loss: 0.09392 \n",
      "[ 53/100] train_loss: 0.10789 valid_loss: 0.11985 test_loss: 0.09670 \n",
      "[ 54/100] train_loss: 0.10629 valid_loss: 0.11783 test_loss: 0.09363 \n",
      "Validation loss decreased (0.118486 --> 0.117831).  Saving model ...\n",
      "[ 55/100] train_loss: 0.10202 valid_loss: 0.11593 test_loss: 0.09264 \n",
      "Validation loss decreased (0.117831 --> 0.115930).  Saving model ...\n",
      "[ 56/100] train_loss: 0.10353 valid_loss: 0.11757 test_loss: 0.09214 \n",
      "[ 57/100] train_loss: 0.10256 valid_loss: 0.11676 test_loss: 0.09202 \n",
      "[ 58/100] train_loss: 0.10372 valid_loss: 0.11564 test_loss: 0.09219 \n",
      "Validation loss decreased (0.115930 --> 0.115638).  Saving model ...\n",
      "[ 59/100] train_loss: 0.10176 valid_loss: 0.11562 test_loss: 0.09217 \n",
      "Validation loss decreased (0.115638 --> 0.115625).  Saving model ...\n",
      "[ 60/100] train_loss: 0.09985 valid_loss: 0.11710 test_loss: 0.09320 \n",
      "[ 61/100] train_loss: 0.10194 valid_loss: 0.11643 test_loss: 0.09300 \n",
      "[ 62/100] train_loss: 0.10211 valid_loss: 0.11566 test_loss: 0.09361 \n",
      "[ 63/100] train_loss: 0.10233 valid_loss: 0.11505 test_loss: 0.09092 \n",
      "Validation loss decreased (0.115625 --> 0.115053).  Saving model ...\n",
      "[ 64/100] train_loss: 0.10099 valid_loss: 0.11329 test_loss: 0.08963 \n",
      "Validation loss decreased (0.115053 --> 0.113292).  Saving model ...\n",
      "[ 65/100] train_loss: 0.09828 valid_loss: 0.11556 test_loss: 0.09203 \n",
      "[ 66/100] train_loss: 0.10292 valid_loss: 0.11412 test_loss: 0.09198 \n",
      "[ 67/100] train_loss: 0.09777 valid_loss: 0.11603 test_loss: 0.09238 \n",
      "[ 68/100] train_loss: 0.09755 valid_loss: 0.11564 test_loss: 0.09305 \n",
      "[ 69/100] train_loss: 0.09779 valid_loss: 0.11502 test_loss: 0.09127 \n",
      "[ 70/100] train_loss: 0.09428 valid_loss: 0.11304 test_loss: 0.08995 \n",
      "Validation loss decreased (0.113292 --> 0.113044).  Saving model ...\n",
      "[ 71/100] train_loss: 0.09745 valid_loss: 0.11255 test_loss: 0.08859 \n",
      "Validation loss decreased (0.113044 --> 0.112547).  Saving model ...\n",
      "[ 72/100] train_loss: 0.10162 valid_loss: 0.11754 test_loss: 0.09457 \n",
      "[ 73/100] train_loss: 0.09704 valid_loss: 0.11177 test_loss: 0.08613 \n",
      "Validation loss decreased (0.112547 --> 0.111773).  Saving model ...\n",
      "[ 74/100] train_loss: 0.09821 valid_loss: 0.11226 test_loss: 0.08912 \n",
      "[ 75/100] train_loss: 0.09812 valid_loss: 0.11216 test_loss: 0.08817 \n",
      "[ 76/100] train_loss: 0.09707 valid_loss: 0.11273 test_loss: 0.08704 \n",
      "[ 77/100] train_loss: 0.09620 valid_loss: 0.11240 test_loss: 0.08922 \n",
      "[ 78/100] train_loss: 0.09886 valid_loss: 0.11216 test_loss: 0.08837 \n",
      "[ 79/100] train_loss: 0.09525 valid_loss: 0.11409 test_loss: 0.08831 \n",
      "[ 80/100] train_loss: 0.09606 valid_loss: 0.11168 test_loss: 0.08814 \n",
      "Validation loss decreased (0.111773 --> 0.111678).  Saving model ...\n",
      "[ 81/100] train_loss: 0.09550 valid_loss: 0.11182 test_loss: 0.08595 \n",
      "[ 82/100] train_loss: 0.09508 valid_loss: 0.11024 test_loss: 0.08664 \n",
      "Validation loss decreased (0.111678 --> 0.110240).  Saving model ...\n",
      "[ 83/100] train_loss: 0.09328 valid_loss: 0.11264 test_loss: 0.08959 \n",
      "[ 84/100] train_loss: 0.09506 valid_loss: 0.11182 test_loss: 0.08799 \n",
      "[ 85/100] train_loss: 0.09370 valid_loss: 0.11319 test_loss: 0.08981 \n",
      "[ 86/100] train_loss: 0.09697 valid_loss: 0.10784 test_loss: 0.08875 \n",
      "Validation loss decreased (0.110240 --> 0.107841).  Saving model ...\n",
      "[ 87/100] train_loss: 0.09285 valid_loss: 0.11203 test_loss: 0.08887 \n",
      "[ 88/100] train_loss: 0.09580 valid_loss: 0.10933 test_loss: 0.08674 \n",
      "[ 89/100] train_loss: 0.09334 valid_loss: 0.11108 test_loss: 0.08743 \n",
      "[ 90/100] train_loss: 0.09084 valid_loss: 0.10945 test_loss: 0.08329 \n",
      "[ 91/100] train_loss: 0.09148 valid_loss: 0.11137 test_loss: 0.08833 \n",
      "[ 92/100] train_loss: 0.09502 valid_loss: 0.10763 test_loss: 0.08670 \n",
      "Validation loss decreased (0.107841 --> 0.107629).  Saving model ...\n",
      "[ 93/100] train_loss: 0.09207 valid_loss: 0.10881 test_loss: 0.08723 \n",
      "[ 94/100] train_loss: 0.09198 valid_loss: 0.11025 test_loss: 0.08753 \n",
      "[ 95/100] train_loss: 0.09424 valid_loss: 0.10537 test_loss: 0.08957 \n",
      "Validation loss decreased (0.107629 --> 0.105366).  Saving model ...\n",
      "[ 96/100] train_loss: 0.09028 valid_loss: 0.10917 test_loss: 0.09906 \n",
      "[ 97/100] train_loss: 0.09033 valid_loss: 0.10484 test_loss: 0.09890 \n",
      "Validation loss decreased (0.105366 --> 0.104840).  Saving model ...\n",
      "[ 98/100] train_loss: 0.09065 valid_loss: 0.10911 test_loss: 0.08759 \n",
      "[ 99/100] train_loss: 0.08651 valid_loss: 0.10777 test_loss: 0.08456 \n",
      "[100/100] train_loss: 0.09176 valid_loss: 0.10509 test_loss: 0.08783 \n",
      "Model 17 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 18\n",
      "[  1/100] train_loss: 0.59225 valid_loss: 0.48265 test_loss: 0.44191 \n",
      "Validation loss decreased (inf --> 0.482649).  Saving model ...\n",
      "[  2/100] train_loss: 0.40646 valid_loss: 0.36091 test_loss: 0.31524 \n",
      "Validation loss decreased (0.482649 --> 0.360912).  Saving model ...\n",
      "[  3/100] train_loss: 0.32265 valid_loss: 0.30475 test_loss: 0.26073 \n",
      "Validation loss decreased (0.360912 --> 0.304751).  Saving model ...\n",
      "[  4/100] train_loss: 0.27144 valid_loss: 0.27266 test_loss: 0.22365 \n",
      "Validation loss decreased (0.304751 --> 0.272660).  Saving model ...\n",
      "[  5/100] train_loss: 0.24429 valid_loss: 0.24761 test_loss: 0.19894 \n",
      "Validation loss decreased (0.272660 --> 0.247615).  Saving model ...\n",
      "[  6/100] train_loss: 0.21947 valid_loss: 0.22704 test_loss: 0.17607 \n",
      "Validation loss decreased (0.247615 --> 0.227038).  Saving model ...\n",
      "[  7/100] train_loss: 0.20203 valid_loss: 0.21269 test_loss: 0.16650 \n",
      "Validation loss decreased (0.227038 --> 0.212686).  Saving model ...\n",
      "[  8/100] train_loss: 0.19068 valid_loss: 0.19769 test_loss: 0.15999 \n",
      "Validation loss decreased (0.212686 --> 0.197689).  Saving model ...\n",
      "[  9/100] train_loss: 0.18023 valid_loss: 0.19066 test_loss: 0.14756 \n",
      "Validation loss decreased (0.197689 --> 0.190661).  Saving model ...\n",
      "[ 10/100] train_loss: 0.17485 valid_loss: 0.18343 test_loss: 0.14112 \n",
      "Validation loss decreased (0.190661 --> 0.183426).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16603 valid_loss: 0.18195 test_loss: 0.13886 \n",
      "Validation loss decreased (0.183426 --> 0.181954).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16233 valid_loss: 0.17824 test_loss: 0.13418 \n",
      "Validation loss decreased (0.181954 --> 0.178244).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15799 valid_loss: 0.17144 test_loss: 0.12394 \n",
      "Validation loss decreased (0.178244 --> 0.171435).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15570 valid_loss: 0.17128 test_loss: 0.12450 \n",
      "Validation loss decreased (0.171435 --> 0.171285).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14979 valid_loss: 0.16586 test_loss: 0.12105 \n",
      "Validation loss decreased (0.171285 --> 0.165860).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14911 valid_loss: 0.16491 test_loss: 0.11817 \n",
      "Validation loss decreased (0.165860 --> 0.164912).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14499 valid_loss: 0.16000 test_loss: 0.11573 \n",
      "Validation loss decreased (0.164912 --> 0.159996).  Saving model ...\n",
      "[ 18/100] train_loss: 0.14241 valid_loss: 0.15351 test_loss: 0.11428 \n",
      "Validation loss decreased (0.159996 --> 0.153512).  Saving model ...\n",
      "[ 19/100] train_loss: 0.14133 valid_loss: 0.15918 test_loss: 0.11376 \n",
      "[ 20/100] train_loss: 0.13874 valid_loss: 0.15472 test_loss: 0.11592 \n",
      "[ 21/100] train_loss: 0.13696 valid_loss: 0.15381 test_loss: 0.11309 \n",
      "[ 22/100] train_loss: 0.13787 valid_loss: 0.15348 test_loss: 0.11196 \n",
      "Validation loss decreased (0.153512 --> 0.153484).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13413 valid_loss: 0.15554 test_loss: 0.11139 \n",
      "[ 24/100] train_loss: 0.13121 valid_loss: 0.14679 test_loss: 0.10878 \n",
      "Validation loss decreased (0.153484 --> 0.146791).  Saving model ...\n",
      "[ 25/100] train_loss: 0.13078 valid_loss: 0.14757 test_loss: 0.10707 \n",
      "[ 26/100] train_loss: 0.13112 valid_loss: 0.14245 test_loss: 0.10624 \n",
      "Validation loss decreased (0.146791 --> 0.142447).  Saving model ...\n",
      "[ 27/100] train_loss: 0.13307 valid_loss: 0.14710 test_loss: 0.11151 \n",
      "[ 28/100] train_loss: 0.12574 valid_loss: 0.14538 test_loss: 0.10770 \n",
      "[ 29/100] train_loss: 0.13036 valid_loss: 0.14269 test_loss: 0.10555 \n",
      "[ 30/100] train_loss: 0.12619 valid_loss: 0.13939 test_loss: 0.10419 \n",
      "Validation loss decreased (0.142447 --> 0.139391).  Saving model ...\n",
      "[ 31/100] train_loss: 0.12747 valid_loss: 0.14116 test_loss: 0.10586 \n",
      "[ 32/100] train_loss: 0.12135 valid_loss: 0.13747 test_loss: 0.10194 \n",
      "Validation loss decreased (0.139391 --> 0.137465).  Saving model ...\n",
      "[ 33/100] train_loss: 0.12531 valid_loss: 0.13788 test_loss: 0.10229 \n",
      "[ 34/100] train_loss: 0.12187 valid_loss: 0.13544 test_loss: 0.09986 \n",
      "Validation loss decreased (0.137465 --> 0.135437).  Saving model ...\n",
      "[ 35/100] train_loss: 0.11879 valid_loss: 0.13198 test_loss: 0.09865 \n",
      "Validation loss decreased (0.135437 --> 0.131984).  Saving model ...\n",
      "[ 36/100] train_loss: 0.12591 valid_loss: 0.13610 test_loss: 0.10292 \n",
      "[ 37/100] train_loss: 0.11907 valid_loss: 0.13217 test_loss: 0.10067 \n",
      "[ 38/100] train_loss: 0.12225 valid_loss: 0.13431 test_loss: 0.10197 \n",
      "[ 39/100] train_loss: 0.11669 valid_loss: 0.13321 test_loss: 0.10215 \n",
      "[ 40/100] train_loss: 0.12230 valid_loss: 0.13070 test_loss: 0.09881 \n",
      "Validation loss decreased (0.131984 --> 0.130696).  Saving model ...\n",
      "[ 41/100] train_loss: 0.11674 valid_loss: 0.13163 test_loss: 0.10144 \n",
      "[ 42/100] train_loss: 0.11580 valid_loss: 0.13119 test_loss: 0.09937 \n",
      "[ 43/100] train_loss: 0.11586 valid_loss: 0.13500 test_loss: 0.10191 \n",
      "[ 44/100] train_loss: 0.11818 valid_loss: 0.13086 test_loss: 0.09955 \n",
      "[ 45/100] train_loss: 0.11915 valid_loss: 0.12847 test_loss: 0.09730 \n",
      "Validation loss decreased (0.130696 --> 0.128472).  Saving model ...\n",
      "[ 46/100] train_loss: 0.11272 valid_loss: 0.13273 test_loss: 0.10086 \n",
      "[ 47/100] train_loss: 0.11683 valid_loss: 0.12725 test_loss: 0.09665 \n",
      "Validation loss decreased (0.128472 --> 0.127249).  Saving model ...\n",
      "[ 48/100] train_loss: 0.11292 valid_loss: 0.12496 test_loss: 0.09440 \n",
      "Validation loss decreased (0.127249 --> 0.124964).  Saving model ...\n",
      "[ 49/100] train_loss: 0.11127 valid_loss: 0.12759 test_loss: 0.09578 \n",
      "[ 50/100] train_loss: 0.11284 valid_loss: 0.12686 test_loss: 0.09649 \n",
      "[ 51/100] train_loss: 0.11312 valid_loss: 0.12545 test_loss: 0.09546 \n",
      "[ 52/100] train_loss: 0.10648 valid_loss: 0.12551 test_loss: 0.09684 \n",
      "[ 53/100] train_loss: 0.10800 valid_loss: 0.12522 test_loss: 0.09506 \n",
      "[ 54/100] train_loss: 0.10876 valid_loss: 0.12623 test_loss: 0.09709 \n",
      "[ 55/100] train_loss: 0.10914 valid_loss: 0.12403 test_loss: 0.09549 \n",
      "Validation loss decreased (0.124964 --> 0.124030).  Saving model ...\n",
      "[ 56/100] train_loss: 0.10809 valid_loss: 0.12676 test_loss: 0.09579 \n",
      "[ 57/100] train_loss: 0.10863 valid_loss: 0.12360 test_loss: 0.09382 \n",
      "Validation loss decreased (0.124030 --> 0.123596).  Saving model ...\n",
      "[ 58/100] train_loss: 0.10972 valid_loss: 0.12861 test_loss: 0.09917 \n",
      "[ 59/100] train_loss: 0.10901 valid_loss: 0.12173 test_loss: 0.09239 \n",
      "Validation loss decreased (0.123596 --> 0.121732).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10482 valid_loss: 0.12648 test_loss: 0.09673 \n",
      "[ 61/100] train_loss: 0.10929 valid_loss: 0.12568 test_loss: 0.09346 \n",
      "[ 62/100] train_loss: 0.10751 valid_loss: 0.12225 test_loss: 0.09332 \n",
      "[ 63/100] train_loss: 0.10115 valid_loss: 0.12207 test_loss: 0.09349 \n",
      "[ 64/100] train_loss: 0.10495 valid_loss: 0.12729 test_loss: 0.09692 \n",
      "[ 65/100] train_loss: 0.10532 valid_loss: 0.12043 test_loss: 0.09263 \n",
      "Validation loss decreased (0.121732 --> 0.120430).  Saving model ...\n",
      "[ 66/100] train_loss: 0.10383 valid_loss: 0.11906 test_loss: 0.09066 \n",
      "Validation loss decreased (0.120430 --> 0.119064).  Saving model ...\n",
      "[ 67/100] train_loss: 0.10511 valid_loss: 0.12275 test_loss: 0.09248 \n",
      "[ 68/100] train_loss: 0.10143 valid_loss: 0.12118 test_loss: 0.09272 \n",
      "[ 69/100] train_loss: 0.10361 valid_loss: 0.11697 test_loss: 0.08975 \n",
      "Validation loss decreased (0.119064 --> 0.116968).  Saving model ...\n",
      "[ 70/100] train_loss: 0.09822 valid_loss: 0.11981 test_loss: 0.09217 \n",
      "[ 71/100] train_loss: 0.10285 valid_loss: 0.12388 test_loss: 0.09335 \n",
      "[ 72/100] train_loss: 0.10285 valid_loss: 0.12476 test_loss: 0.09530 \n",
      "[ 73/100] train_loss: 0.10162 valid_loss: 0.11845 test_loss: 0.09220 \n",
      "[ 74/100] train_loss: 0.10386 valid_loss: 0.11668 test_loss: 0.09123 \n",
      "Validation loss decreased (0.116968 --> 0.116677).  Saving model ...\n",
      "[ 75/100] train_loss: 0.09700 valid_loss: 0.11817 test_loss: 0.09065 \n",
      "[ 76/100] train_loss: 0.09784 valid_loss: 0.11578 test_loss: 0.08922 \n",
      "Validation loss decreased (0.116677 --> 0.115780).  Saving model ...\n",
      "[ 77/100] train_loss: 0.09822 valid_loss: 0.11844 test_loss: 0.08966 \n",
      "[ 78/100] train_loss: 0.10071 valid_loss: 0.11333 test_loss: 0.08760 \n",
      "Validation loss decreased (0.115780 --> 0.113331).  Saving model ...\n",
      "[ 79/100] train_loss: 0.10315 valid_loss: 0.12080 test_loss: 0.09178 \n",
      "[ 80/100] train_loss: 0.09928 valid_loss: 0.11407 test_loss: 0.08929 \n",
      "[ 81/100] train_loss: 0.09759 valid_loss: 0.11569 test_loss: 0.08902 \n",
      "[ 82/100] train_loss: 0.09662 valid_loss: 0.11294 test_loss: 0.08804 \n",
      "Validation loss decreased (0.113331 --> 0.112940).  Saving model ...\n",
      "[ 83/100] train_loss: 0.09621 valid_loss: 0.11729 test_loss: 0.09078 \n",
      "[ 84/100] train_loss: 0.09809 valid_loss: 0.11523 test_loss: 0.08818 \n",
      "[ 85/100] train_loss: 0.09915 valid_loss: 0.11554 test_loss: 0.08973 \n",
      "[ 86/100] train_loss: 0.09808 valid_loss: 0.11966 test_loss: 0.09257 \n",
      "[ 87/100] train_loss: 0.09653 valid_loss: 0.11492 test_loss: 0.09071 \n",
      "[ 88/100] train_loss: 0.10083 valid_loss: 0.11438 test_loss: 0.09011 \n",
      "[ 89/100] train_loss: 0.09509 valid_loss: 0.11338 test_loss: 0.08928 \n",
      "[ 90/100] train_loss: 0.09714 valid_loss: 0.11032 test_loss: 0.08805 \n",
      "Validation loss decreased (0.112940 --> 0.110319).  Saving model ...\n",
      "[ 91/100] train_loss: 0.09530 valid_loss: 0.11568 test_loss: 0.09074 \n",
      "[ 92/100] train_loss: 0.09606 valid_loss: 0.11594 test_loss: 0.09150 \n",
      "[ 93/100] train_loss: 0.09310 valid_loss: 0.11438 test_loss: 0.08850 \n",
      "[ 94/100] train_loss: 0.09528 valid_loss: 0.11400 test_loss: 0.08852 \n",
      "[ 95/100] train_loss: 0.09623 valid_loss: 0.11192 test_loss: 0.08699 \n",
      "[ 96/100] train_loss: 0.09404 valid_loss: 0.11224 test_loss: 0.08709 \n",
      "[ 97/100] train_loss: 0.09321 valid_loss: 0.11212 test_loss: 0.08556 \n",
      "[ 98/100] train_loss: 0.09481 valid_loss: 0.11398 test_loss: 0.08809 \n",
      "[ 99/100] train_loss: 0.09403 valid_loss: 0.11449 test_loss: 0.08926 \n",
      "[100/100] train_loss: 0.09414 valid_loss: 0.11175 test_loss: 0.08742 \n",
      "Model 18 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 19\n",
      "[  1/100] train_loss: 0.50932 valid_loss: 0.41428 test_loss: 0.36682 \n",
      "Validation loss decreased (inf --> 0.414281).  Saving model ...\n",
      "[  2/100] train_loss: 0.34880 valid_loss: 0.32473 test_loss: 0.28141 \n",
      "Validation loss decreased (0.414281 --> 0.324729).  Saving model ...\n",
      "[  3/100] train_loss: 0.28551 valid_loss: 0.27575 test_loss: 0.23655 \n",
      "Validation loss decreased (0.324729 --> 0.275754).  Saving model ...\n",
      "[  4/100] train_loss: 0.23969 valid_loss: 0.24424 test_loss: 0.20102 \n",
      "Validation loss decreased (0.275754 --> 0.244235).  Saving model ...\n",
      "[  5/100] train_loss: 0.21687 valid_loss: 0.22638 test_loss: 0.18098 \n",
      "Validation loss decreased (0.244235 --> 0.226384).  Saving model ...\n",
      "[  6/100] train_loss: 0.19736 valid_loss: 0.20765 test_loss: 0.15913 \n",
      "Validation loss decreased (0.226384 --> 0.207649).  Saving model ...\n",
      "[  7/100] train_loss: 0.18721 valid_loss: 0.19948 test_loss: 0.15332 \n",
      "Validation loss decreased (0.207649 --> 0.199478).  Saving model ...\n",
      "[  8/100] train_loss: 0.17358 valid_loss: 0.18595 test_loss: 0.14201 \n",
      "Validation loss decreased (0.199478 --> 0.185946).  Saving model ...\n",
      "[  9/100] train_loss: 0.16994 valid_loss: 0.18291 test_loss: 0.13607 \n",
      "Validation loss decreased (0.185946 --> 0.182907).  Saving model ...\n",
      "[ 10/100] train_loss: 0.15919 valid_loss: 0.17907 test_loss: 0.13095 \n",
      "Validation loss decreased (0.182907 --> 0.179071).  Saving model ...\n",
      "[ 11/100] train_loss: 0.15910 valid_loss: 0.17427 test_loss: 0.12860 \n",
      "Validation loss decreased (0.179071 --> 0.174271).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15478 valid_loss: 0.17204 test_loss: 0.12661 \n",
      "Validation loss decreased (0.174271 --> 0.172042).  Saving model ...\n",
      "[ 13/100] train_loss: 0.14782 valid_loss: 0.16956 test_loss: 0.12142 \n",
      "Validation loss decreased (0.172042 --> 0.169560).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14716 valid_loss: 0.16597 test_loss: 0.11818 \n",
      "Validation loss decreased (0.169560 --> 0.165969).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14524 valid_loss: 0.16239 test_loss: 0.11766 \n",
      "Validation loss decreased (0.165969 --> 0.162387).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14179 valid_loss: 0.16265 test_loss: 0.11683 \n",
      "[ 17/100] train_loss: 0.13814 valid_loss: 0.15919 test_loss: 0.11351 \n",
      "Validation loss decreased (0.162387 --> 0.159189).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13678 valid_loss: 0.15050 test_loss: 0.11100 \n",
      "Validation loss decreased (0.159189 --> 0.150497).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13342 valid_loss: 0.15460 test_loss: 0.11230 \n",
      "[ 20/100] train_loss: 0.13541 valid_loss: 0.15251 test_loss: 0.11033 \n",
      "[ 21/100] train_loss: 0.12899 valid_loss: 0.15135 test_loss: 0.11017 \n",
      "[ 22/100] train_loss: 0.12867 valid_loss: 0.14565 test_loss: 0.10994 \n",
      "Validation loss decreased (0.150497 --> 0.145649).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13012 valid_loss: 0.14303 test_loss: 0.10448 \n",
      "Validation loss decreased (0.145649 --> 0.143025).  Saving model ...\n",
      "[ 24/100] train_loss: 0.12745 valid_loss: 0.14255 test_loss: 0.10496 \n",
      "Validation loss decreased (0.143025 --> 0.142548).  Saving model ...\n",
      "[ 25/100] train_loss: 0.12628 valid_loss: 0.14505 test_loss: 0.10614 \n",
      "[ 26/100] train_loss: 0.12450 valid_loss: 0.14560 test_loss: 0.10755 \n",
      "[ 27/100] train_loss: 0.12339 valid_loss: 0.13935 test_loss: 0.10536 \n",
      "Validation loss decreased (0.142548 --> 0.139351).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12456 valid_loss: 0.13701 test_loss: 0.10238 \n",
      "Validation loss decreased (0.139351 --> 0.137009).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12196 valid_loss: 0.14002 test_loss: 0.10538 \n",
      "[ 30/100] train_loss: 0.12065 valid_loss: 0.13909 test_loss: 0.10489 \n",
      "[ 31/100] train_loss: 0.11911 valid_loss: 0.13442 test_loss: 0.10138 \n",
      "Validation loss decreased (0.137009 --> 0.134417).  Saving model ...\n",
      "[ 32/100] train_loss: 0.11990 valid_loss: 0.13547 test_loss: 0.10167 \n",
      "[ 33/100] train_loss: 0.11813 valid_loss: 0.13507 test_loss: 0.10152 \n",
      "[ 34/100] train_loss: 0.11719 valid_loss: 0.13459 test_loss: 0.10288 \n",
      "[ 35/100] train_loss: 0.11649 valid_loss: 0.13412 test_loss: 0.10170 \n",
      "Validation loss decreased (0.134417 --> 0.134121).  Saving model ...\n",
      "[ 36/100] train_loss: 0.11726 valid_loss: 0.13275 test_loss: 0.10222 \n",
      "Validation loss decreased (0.134121 --> 0.132748).  Saving model ...\n",
      "[ 37/100] train_loss: 0.11396 valid_loss: 0.13633 test_loss: 0.10768 \n",
      "[ 38/100] train_loss: 0.11399 valid_loss: 0.13093 test_loss: 0.10061 \n",
      "Validation loss decreased (0.132748 --> 0.130934).  Saving model ...\n",
      "[ 39/100] train_loss: 0.11288 valid_loss: 0.13105 test_loss: 0.10047 \n",
      "[ 40/100] train_loss: 0.11232 valid_loss: 0.13134 test_loss: 0.10125 \n",
      "[ 41/100] train_loss: 0.11185 valid_loss: 0.12660 test_loss: 0.09633 \n",
      "Validation loss decreased (0.130934 --> 0.126597).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11015 valid_loss: 0.12898 test_loss: 0.09980 \n",
      "[ 43/100] train_loss: 0.11317 valid_loss: 0.12733 test_loss: 0.09983 \n",
      "[ 44/100] train_loss: 0.11062 valid_loss: 0.12536 test_loss: 0.09815 \n",
      "Validation loss decreased (0.126597 --> 0.125364).  Saving model ...\n",
      "[ 45/100] train_loss: 0.11014 valid_loss: 0.12063 test_loss: 0.09349 \n",
      "Validation loss decreased (0.125364 --> 0.120631).  Saving model ...\n",
      "[ 46/100] train_loss: 0.11294 valid_loss: 0.12100 test_loss: 0.09488 \n",
      "[ 47/100] train_loss: 0.10669 valid_loss: 0.12167 test_loss: 0.09407 \n",
      "[ 48/100] train_loss: 0.11185 valid_loss: 0.12736 test_loss: 0.09889 \n",
      "[ 49/100] train_loss: 0.10799 valid_loss: 0.12379 test_loss: 0.09766 \n",
      "[ 50/100] train_loss: 0.10429 valid_loss: 0.12229 test_loss: 0.09533 \n",
      "[ 51/100] train_loss: 0.11099 valid_loss: 0.12229 test_loss: 0.09387 \n",
      "[ 52/100] train_loss: 0.10576 valid_loss: 0.11854 test_loss: 0.09025 \n",
      "Validation loss decreased (0.120631 --> 0.118537).  Saving model ...\n",
      "[ 53/100] train_loss: 0.10727 valid_loss: 0.11989 test_loss: 0.09442 \n",
      "[ 54/100] train_loss: 0.10555 valid_loss: 0.12294 test_loss: 0.09774 \n",
      "[ 55/100] train_loss: 0.10678 valid_loss: 0.12164 test_loss: 0.09566 \n",
      "[ 56/100] train_loss: 0.10533 valid_loss: 0.12528 test_loss: 0.09955 \n",
      "[ 57/100] train_loss: 0.10413 valid_loss: 0.12066 test_loss: 0.09534 \n",
      "[ 58/100] train_loss: 0.10376 valid_loss: 0.12037 test_loss: 0.09449 \n",
      "[ 59/100] train_loss: 0.10301 valid_loss: 0.12416 test_loss: 0.09590 \n",
      "[ 60/100] train_loss: 0.10185 valid_loss: 0.11760 test_loss: 0.09037 \n",
      "Validation loss decreased (0.118537 --> 0.117602).  Saving model ...\n",
      "[ 61/100] train_loss: 0.10284 valid_loss: 0.11670 test_loss: 0.08915 \n",
      "Validation loss decreased (0.117602 --> 0.116702).  Saving model ...\n",
      "[ 62/100] train_loss: 0.10350 valid_loss: 0.11682 test_loss: 0.09244 \n",
      "[ 63/100] train_loss: 0.09881 valid_loss: 0.11555 test_loss: 0.08970 \n",
      "Validation loss decreased (0.116702 --> 0.115554).  Saving model ...\n",
      "[ 64/100] train_loss: 0.10277 valid_loss: 0.11388 test_loss: 0.09110 \n",
      "Validation loss decreased (0.115554 --> 0.113884).  Saving model ...\n",
      "[ 65/100] train_loss: 0.10198 valid_loss: 0.11923 test_loss: 0.09619 \n",
      "[ 66/100] train_loss: 0.10005 valid_loss: 0.11918 test_loss: 0.09089 \n",
      "[ 67/100] train_loss: 0.09866 valid_loss: 0.11884 test_loss: 0.09189 \n",
      "[ 68/100] train_loss: 0.10001 valid_loss: 0.11622 test_loss: 0.08904 \n",
      "[ 69/100] train_loss: 0.09828 valid_loss: 0.11447 test_loss: 0.08813 \n",
      "[ 70/100] train_loss: 0.09991 valid_loss: 0.11797 test_loss: 0.09146 \n",
      "[ 71/100] train_loss: 0.09976 valid_loss: 0.11492 test_loss: 0.08799 \n",
      "[ 72/100] train_loss: 0.10115 valid_loss: 0.11674 test_loss: 0.09093 \n",
      "[ 73/100] train_loss: 0.09572 valid_loss: 0.11291 test_loss: 0.08890 \n",
      "Validation loss decreased (0.113884 --> 0.112906).  Saving model ...\n",
      "[ 74/100] train_loss: 0.09654 valid_loss: 0.11859 test_loss: 0.09343 \n",
      "[ 75/100] train_loss: 0.09444 valid_loss: 0.11513 test_loss: 0.09118 \n",
      "[ 76/100] train_loss: 0.09687 valid_loss: 0.11772 test_loss: 0.09010 \n",
      "[ 77/100] train_loss: 0.09731 valid_loss: 0.11408 test_loss: 0.08903 \n",
      "[ 78/100] train_loss: 0.09622 valid_loss: 0.11413 test_loss: 0.09090 \n",
      "[ 79/100] train_loss: 0.09682 valid_loss: 0.11124 test_loss: 0.08595 \n",
      "Validation loss decreased (0.112906 --> 0.111242).  Saving model ...\n",
      "[ 80/100] train_loss: 0.09722 valid_loss: 0.11457 test_loss: 0.08949 \n",
      "[ 81/100] train_loss: 0.09452 valid_loss: 0.11432 test_loss: 0.08832 \n",
      "[ 82/100] train_loss: 0.09541 valid_loss: 0.11131 test_loss: 0.08698 \n",
      "[ 83/100] train_loss: 0.09509 valid_loss: 0.10956 test_loss: 0.08779 \n",
      "Validation loss decreased (0.111242 --> 0.109563).  Saving model ...\n",
      "[ 84/100] train_loss: 0.09496 valid_loss: 0.11380 test_loss: 0.08776 \n",
      "[ 85/100] train_loss: 0.09532 valid_loss: 0.11044 test_loss: 0.08527 \n",
      "[ 86/100] train_loss: 0.09520 valid_loss: 0.11047 test_loss: 0.08811 \n",
      "[ 87/100] train_loss: 0.09547 valid_loss: 0.11367 test_loss: 0.08934 \n",
      "[ 88/100] train_loss: 0.09553 valid_loss: 0.11328 test_loss: 0.08862 \n",
      "[ 89/100] train_loss: 0.09557 valid_loss: 0.10903 test_loss: 0.08513 \n",
      "Validation loss decreased (0.109563 --> 0.109031).  Saving model ...\n",
      "[ 90/100] train_loss: 0.09535 valid_loss: 0.10881 test_loss: 0.08344 \n",
      "Validation loss decreased (0.109031 --> 0.108807).  Saving model ...\n",
      "[ 91/100] train_loss: 0.09310 valid_loss: 0.10829 test_loss: 0.08337 \n",
      "Validation loss decreased (0.108807 --> 0.108293).  Saving model ...\n",
      "[ 92/100] train_loss: 0.09432 valid_loss: 0.11134 test_loss: 0.08934 \n",
      "[ 93/100] train_loss: 0.09393 valid_loss: 0.10967 test_loss: 0.08716 \n",
      "[ 94/100] train_loss: 0.09381 valid_loss: 0.10717 test_loss: 0.08449 \n",
      "Validation loss decreased (0.108293 --> 0.107174).  Saving model ...\n",
      "[ 95/100] train_loss: 0.09297 valid_loss: 0.10948 test_loss: 0.08787 \n",
      "[ 96/100] train_loss: 0.09151 valid_loss: 0.10736 test_loss: 0.08541 \n",
      "[ 97/100] train_loss: 0.09320 valid_loss: 0.10689 test_loss: 0.08535 \n",
      "Validation loss decreased (0.107174 --> 0.106890).  Saving model ...\n",
      "[ 98/100] train_loss: 0.09102 valid_loss: 0.10648 test_loss: 0.08450 \n",
      "Validation loss decreased (0.106890 --> 0.106475).  Saving model ...\n",
      "[ 99/100] train_loss: 0.09309 valid_loss: 0.10626 test_loss: 0.08538 \n",
      "Validation loss decreased (0.106475 --> 0.106257).  Saving model ...\n",
      "[100/100] train_loss: 0.09155 valid_loss: 0.11121 test_loss: 0.08878 \n",
      "Model 19 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 20\n",
      "[  1/100] train_loss: 0.57100 valid_loss: 0.48080 test_loss: 0.43592 \n",
      "Validation loss decreased (inf --> 0.480800).  Saving model ...\n",
      "[  2/100] train_loss: 0.40115 valid_loss: 0.35488 test_loss: 0.30493 \n",
      "Validation loss decreased (0.480800 --> 0.354883).  Saving model ...\n",
      "[  3/100] train_loss: 0.31825 valid_loss: 0.30509 test_loss: 0.25986 \n",
      "Validation loss decreased (0.354883 --> 0.305085).  Saving model ...\n",
      "[  4/100] train_loss: 0.27390 valid_loss: 0.27080 test_loss: 0.22128 \n",
      "Validation loss decreased (0.305085 --> 0.270802).  Saving model ...\n",
      "[  5/100] train_loss: 0.23905 valid_loss: 0.24578 test_loss: 0.19698 \n",
      "Validation loss decreased (0.270802 --> 0.245776).  Saving model ...\n",
      "[  6/100] train_loss: 0.21676 valid_loss: 0.22293 test_loss: 0.17290 \n",
      "Validation loss decreased (0.245776 --> 0.222933).  Saving model ...\n",
      "[  7/100] train_loss: 0.19911 valid_loss: 0.20954 test_loss: 0.15846 \n",
      "Validation loss decreased (0.222933 --> 0.209537).  Saving model ...\n",
      "[  8/100] train_loss: 0.18919 valid_loss: 0.19792 test_loss: 0.14809 \n",
      "Validation loss decreased (0.209537 --> 0.197923).  Saving model ...\n",
      "[  9/100] train_loss: 0.17559 valid_loss: 0.18933 test_loss: 0.14338 \n",
      "Validation loss decreased (0.197923 --> 0.189330).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16883 valid_loss: 0.18170 test_loss: 0.13272 \n",
      "Validation loss decreased (0.189330 --> 0.181705).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16538 valid_loss: 0.17398 test_loss: 0.13114 \n",
      "Validation loss decreased (0.181705 --> 0.173977).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16040 valid_loss: 0.17114 test_loss: 0.12800 \n",
      "Validation loss decreased (0.173977 --> 0.171145).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15293 valid_loss: 0.17202 test_loss: 0.12260 \n",
      "[ 14/100] train_loss: 0.15046 valid_loss: 0.16988 test_loss: 0.12020 \n",
      "Validation loss decreased (0.171145 --> 0.169876).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14861 valid_loss: 0.16464 test_loss: 0.11915 \n",
      "Validation loss decreased (0.169876 --> 0.164635).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14415 valid_loss: 0.16153 test_loss: 0.11482 \n",
      "Validation loss decreased (0.164635 --> 0.161534).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14143 valid_loss: 0.16017 test_loss: 0.11555 \n",
      "Validation loss decreased (0.161534 --> 0.160167).  Saving model ...\n",
      "[ 18/100] train_loss: 0.14114 valid_loss: 0.15861 test_loss: 0.11276 \n",
      "Validation loss decreased (0.160167 --> 0.158605).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13729 valid_loss: 0.15680 test_loss: 0.11251 \n",
      "Validation loss decreased (0.158605 --> 0.156798).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13763 valid_loss: 0.15265 test_loss: 0.11135 \n",
      "Validation loss decreased (0.156798 --> 0.152647).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13175 valid_loss: 0.14834 test_loss: 0.10964 \n",
      "Validation loss decreased (0.152647 --> 0.148335).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13340 valid_loss: 0.14881 test_loss: 0.10881 \n",
      "[ 23/100] train_loss: 0.13028 valid_loss: 0.14815 test_loss: 0.10850 \n",
      "Validation loss decreased (0.148335 --> 0.148149).  Saving model ...\n",
      "[ 24/100] train_loss: 0.12635 valid_loss: 0.14721 test_loss: 0.10789 \n",
      "Validation loss decreased (0.148149 --> 0.147210).  Saving model ...\n",
      "[ 25/100] train_loss: 0.12649 valid_loss: 0.14103 test_loss: 0.10596 \n",
      "Validation loss decreased (0.147210 --> 0.141033).  Saving model ...\n",
      "[ 26/100] train_loss: 0.12838 valid_loss: 0.13864 test_loss: 0.10521 \n",
      "Validation loss decreased (0.141033 --> 0.138640).  Saving model ...\n",
      "[ 27/100] train_loss: 0.12595 valid_loss: 0.13427 test_loss: 0.10350 \n",
      "Validation loss decreased (0.138640 --> 0.134273).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12333 valid_loss: 0.13473 test_loss: 0.10432 \n",
      "[ 29/100] train_loss: 0.12347 valid_loss: 0.13424 test_loss: 0.10334 \n",
      "Validation loss decreased (0.134273 --> 0.134244).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12286 valid_loss: 0.13699 test_loss: 0.10341 \n",
      "[ 31/100] train_loss: 0.12335 valid_loss: 0.13156 test_loss: 0.10165 \n",
      "Validation loss decreased (0.134244 --> 0.131555).  Saving model ...\n",
      "[ 32/100] train_loss: 0.12086 valid_loss: 0.13328 test_loss: 0.10226 \n",
      "[ 33/100] train_loss: 0.11874 valid_loss: 0.13214 test_loss: 0.10219 \n",
      "[ 34/100] train_loss: 0.11936 valid_loss: 0.13256 test_loss: 0.10064 \n",
      "[ 35/100] train_loss: 0.11868 valid_loss: 0.13045 test_loss: 0.09907 \n",
      "Validation loss decreased (0.131555 --> 0.130446).  Saving model ...\n",
      "[ 36/100] train_loss: 0.11554 valid_loss: 0.13222 test_loss: 0.10224 \n",
      "[ 37/100] train_loss: 0.11709 valid_loss: 0.13018 test_loss: 0.09943 \n",
      "Validation loss decreased (0.130446 --> 0.130179).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11534 valid_loss: 0.13131 test_loss: 0.10140 \n",
      "[ 39/100] train_loss: 0.11464 valid_loss: 0.13046 test_loss: 0.10174 \n",
      "[ 40/100] train_loss: 0.11480 valid_loss: 0.13114 test_loss: 0.10159 \n",
      "[ 41/100] train_loss: 0.11285 valid_loss: 0.12352 test_loss: 0.09630 \n",
      "Validation loss decreased (0.130179 --> 0.123523).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11160 valid_loss: 0.12754 test_loss: 0.09886 \n",
      "[ 43/100] train_loss: 0.11225 valid_loss: 0.12577 test_loss: 0.09735 \n",
      "[ 44/100] train_loss: 0.10906 valid_loss: 0.12736 test_loss: 0.09814 \n",
      "[ 45/100] train_loss: 0.11236 valid_loss: 0.12664 test_loss: 0.09786 \n",
      "[ 46/100] train_loss: 0.11274 valid_loss: 0.12554 test_loss: 0.09708 \n",
      "[ 47/100] train_loss: 0.11131 valid_loss: 0.12518 test_loss: 0.09710 \n",
      "[ 48/100] train_loss: 0.10927 valid_loss: 0.12227 test_loss: 0.09546 \n",
      "Validation loss decreased (0.123523 --> 0.122267).  Saving model ...\n",
      "[ 49/100] train_loss: 0.11023 valid_loss: 0.12263 test_loss: 0.09773 \n",
      "[ 50/100] train_loss: 0.11066 valid_loss: 0.11984 test_loss: 0.09324 \n",
      "Validation loss decreased (0.122267 --> 0.119838).  Saving model ...\n",
      "[ 51/100] train_loss: 0.10955 valid_loss: 0.11857 test_loss: 0.09535 \n",
      "Validation loss decreased (0.119838 --> 0.118571).  Saving model ...\n",
      "[ 52/100] train_loss: 0.10857 valid_loss: 0.12252 test_loss: 0.09670 \n",
      "[ 53/100] train_loss: 0.10858 valid_loss: 0.12365 test_loss: 0.09792 \n",
      "[ 54/100] train_loss: 0.10282 valid_loss: 0.12019 test_loss: 0.09283 \n",
      "[ 55/100] train_loss: 0.10704 valid_loss: 0.12051 test_loss: 0.09477 \n",
      "[ 56/100] train_loss: 0.10489 valid_loss: 0.11986 test_loss: 0.09426 \n",
      "[ 57/100] train_loss: 0.10612 valid_loss: 0.12134 test_loss: 0.09478 \n",
      "[ 58/100] train_loss: 0.10306 valid_loss: 0.11976 test_loss: 0.09569 \n",
      "[ 59/100] train_loss: 0.10292 valid_loss: 0.12165 test_loss: 0.09498 \n",
      "[ 60/100] train_loss: 0.10081 valid_loss: 0.12035 test_loss: 0.09491 \n",
      "[ 61/100] train_loss: 0.10537 valid_loss: 0.11640 test_loss: 0.09210 \n",
      "Validation loss decreased (0.118571 --> 0.116404).  Saving model ...\n",
      "[ 62/100] train_loss: 0.10432 valid_loss: 0.11446 test_loss: 0.08903 \n",
      "Validation loss decreased (0.116404 --> 0.114461).  Saving model ...\n",
      "[ 63/100] train_loss: 0.10421 valid_loss: 0.11492 test_loss: 0.08949 \n",
      "[ 64/100] train_loss: 0.10141 valid_loss: 0.11687 test_loss: 0.09075 \n",
      "[ 65/100] train_loss: 0.09965 valid_loss: 0.11705 test_loss: 0.09122 \n",
      "[ 66/100] train_loss: 0.10326 valid_loss: 0.11897 test_loss: 0.09527 \n",
      "[ 67/100] train_loss: 0.09816 valid_loss: 0.11279 test_loss: 0.08999 \n",
      "Validation loss decreased (0.114461 --> 0.112788).  Saving model ...\n",
      "[ 68/100] train_loss: 0.10124 valid_loss: 0.11529 test_loss: 0.08934 \n",
      "[ 69/100] train_loss: 0.09971 valid_loss: 0.11314 test_loss: 0.08999 \n",
      "[ 70/100] train_loss: 0.09659 valid_loss: 0.11387 test_loss: 0.08995 \n",
      "[ 71/100] train_loss: 0.09623 valid_loss: 0.11485 test_loss: 0.08975 \n",
      "[ 72/100] train_loss: 0.10182 valid_loss: 0.11315 test_loss: 0.09288 \n",
      "[ 73/100] train_loss: 0.10034 valid_loss: 0.11314 test_loss: 0.09000 \n",
      "[ 74/100] train_loss: 0.09838 valid_loss: 0.11645 test_loss: 0.09214 \n",
      "[ 75/100] train_loss: 0.09837 valid_loss: 0.11598 test_loss: 0.09226 \n",
      "[ 76/100] train_loss: 0.09752 valid_loss: 0.11615 test_loss: 0.09216 \n",
      "[ 77/100] train_loss: 0.09863 valid_loss: 0.11151 test_loss: 0.08957 \n",
      "Validation loss decreased (0.112788 --> 0.111506).  Saving model ...\n",
      "[ 78/100] train_loss: 0.09641 valid_loss: 0.11329 test_loss: 0.08967 \n",
      "[ 79/100] train_loss: 0.09774 valid_loss: 0.11099 test_loss: 0.08874 \n",
      "Validation loss decreased (0.111506 --> 0.110993).  Saving model ...\n",
      "[ 80/100] train_loss: 0.09405 valid_loss: 0.11152 test_loss: 0.08836 \n",
      "[ 81/100] train_loss: 0.09563 valid_loss: 0.10759 test_loss: 0.08674 \n",
      "Validation loss decreased (0.110993 --> 0.107589).  Saving model ...\n",
      "[ 82/100] train_loss: 0.09837 valid_loss: 0.11260 test_loss: 0.09003 \n",
      "[ 83/100] train_loss: 0.09505 valid_loss: 0.11231 test_loss: 0.08903 \n",
      "[ 84/100] train_loss: 0.09714 valid_loss: 0.11096 test_loss: 0.08858 \n",
      "[ 85/100] train_loss: 0.09502 valid_loss: 0.11160 test_loss: 0.08825 \n",
      "[ 86/100] train_loss: 0.09329 valid_loss: 0.11325 test_loss: 0.08885 \n",
      "[ 87/100] train_loss: 0.09562 valid_loss: 0.11179 test_loss: 0.08989 \n",
      "[ 88/100] train_loss: 0.09473 valid_loss: 0.10841 test_loss: 0.08575 \n",
      "[ 89/100] train_loss: 0.09264 valid_loss: 0.11127 test_loss: 0.08762 \n",
      "[ 90/100] train_loss: 0.09627 valid_loss: 0.10775 test_loss: 0.08680 \n",
      "[ 91/100] train_loss: 0.09270 valid_loss: 0.10845 test_loss: 0.08570 \n",
      "[ 92/100] train_loss: 0.09351 valid_loss: 0.10771 test_loss: 0.08475 \n",
      "[ 93/100] train_loss: 0.09294 valid_loss: 0.10858 test_loss: 0.08673 \n",
      "[ 94/100] train_loss: 0.09342 valid_loss: 0.10885 test_loss: 0.08663 \n",
      "[ 95/100] train_loss: 0.09049 valid_loss: 0.11086 test_loss: 0.09001 \n",
      "[ 96/100] train_loss: 0.09078 valid_loss: 0.10868 test_loss: 0.08743 \n",
      "[ 97/100] train_loss: 0.09523 valid_loss: 0.10916 test_loss: 0.08738 \n",
      "[ 98/100] train_loss: 0.09102 valid_loss: 0.10827 test_loss: 0.08762 \n",
      "[ 99/100] train_loss: 0.09324 valid_loss: 0.10446 test_loss: 0.09450 \n",
      "Validation loss decreased (0.107589 --> 0.104455).  Saving model ...\n",
      "[100/100] train_loss: 0.09252 valid_loss: 0.10628 test_loss: 0.08652 \n",
      "Model 20 trained for unseen data.\n",
      "TRAINING MODEL for unseen house 21\n",
      "[  1/100] train_loss: 0.55610 valid_loss: 0.45932 test_loss: 0.42538 \n",
      "Validation loss decreased (inf --> 0.459319).  Saving model ...\n",
      "[  2/100] train_loss: 0.38183 valid_loss: 0.34511 test_loss: 0.29829 \n",
      "Validation loss decreased (0.459319 --> 0.345113).  Saving model ...\n",
      "[  3/100] train_loss: 0.30049 valid_loss: 0.29256 test_loss: 0.24506 \n",
      "Validation loss decreased (0.345113 --> 0.292564).  Saving model ...\n",
      "[  4/100] train_loss: 0.25987 valid_loss: 0.26051 test_loss: 0.21417 \n",
      "Validation loss decreased (0.292564 --> 0.260508).  Saving model ...\n",
      "[  5/100] train_loss: 0.22718 valid_loss: 0.23744 test_loss: 0.18510 \n",
      "Validation loss decreased (0.260508 --> 0.237440).  Saving model ...\n",
      "[  6/100] train_loss: 0.20901 valid_loss: 0.22783 test_loss: 0.17404 \n",
      "Validation loss decreased (0.237440 --> 0.227828).  Saving model ...\n",
      "[  7/100] train_loss: 0.19132 valid_loss: 0.20446 test_loss: 0.15016 \n",
      "Validation loss decreased (0.227828 --> 0.204460).  Saving model ...\n",
      "[  8/100] train_loss: 0.18257 valid_loss: 0.19689 test_loss: 0.14534 \n",
      "Validation loss decreased (0.204460 --> 0.196894).  Saving model ...\n",
      "[  9/100] train_loss: 0.17394 valid_loss: 0.19575 test_loss: 0.13628 \n",
      "Validation loss decreased (0.196894 --> 0.195746).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16309 valid_loss: 0.18374 test_loss: 0.12811 \n",
      "Validation loss decreased (0.195746 --> 0.183741).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16528 valid_loss: 0.17654 test_loss: 0.12693 \n",
      "Validation loss decreased (0.183741 --> 0.176537).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15796 valid_loss: 0.17454 test_loss: 0.12187 \n",
      "Validation loss decreased (0.176537 --> 0.174543).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15020 valid_loss: 0.17241 test_loss: 0.12158 \n",
      "Validation loss decreased (0.174543 --> 0.172406).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14995 valid_loss: 0.16656 test_loss: 0.11876 \n",
      "Validation loss decreased (0.172406 --> 0.166556).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14829 valid_loss: 0.16666 test_loss: 0.11769 \n",
      "[ 16/100] train_loss: 0.14504 valid_loss: 0.16110 test_loss: 0.11240 \n",
      "Validation loss decreased (0.166556 --> 0.161104).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14380 valid_loss: 0.15967 test_loss: 0.11608 \n",
      "Validation loss decreased (0.161104 --> 0.159668).  Saving model ...\n",
      "[ 18/100] train_loss: 0.14126 valid_loss: 0.15534 test_loss: 0.11205 \n",
      "Validation loss decreased (0.159668 --> 0.155335).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13678 valid_loss: 0.15801 test_loss: 0.11292 \n",
      "[ 20/100] train_loss: 0.13360 valid_loss: 0.15273 test_loss: 0.10958 \n",
      "Validation loss decreased (0.155335 --> 0.152726).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13350 valid_loss: 0.15558 test_loss: 0.11263 \n",
      "[ 22/100] train_loss: 0.13066 valid_loss: 0.14750 test_loss: 0.10828 \n",
      "Validation loss decreased (0.152726 --> 0.147496).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13079 valid_loss: 0.14577 test_loss: 0.10569 \n",
      "Validation loss decreased (0.147496 --> 0.145765).  Saving model ...\n",
      "[ 24/100] train_loss: 0.12726 valid_loss: 0.14289 test_loss: 0.10559 \n",
      "Validation loss decreased (0.145765 --> 0.142890).  Saving model ...\n",
      "[ 25/100] train_loss: 0.12415 valid_loss: 0.14340 test_loss: 0.10478 \n",
      "[ 26/100] train_loss: 0.12632 valid_loss: 0.14556 test_loss: 0.10679 \n",
      "[ 27/100] train_loss: 0.12163 valid_loss: 0.14432 test_loss: 0.10681 \n",
      "[ 28/100] train_loss: 0.12200 valid_loss: 0.13729 test_loss: 0.10139 \n",
      "Validation loss decreased (0.142890 --> 0.137289).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12061 valid_loss: 0.13865 test_loss: 0.10426 \n",
      "[ 30/100] train_loss: 0.12168 valid_loss: 0.13682 test_loss: 0.10336 \n",
      "Validation loss decreased (0.137289 --> 0.136819).  Saving model ...\n",
      "[ 31/100] train_loss: 0.12161 valid_loss: 0.13984 test_loss: 0.10506 \n",
      "[ 32/100] train_loss: 0.12171 valid_loss: 0.13287 test_loss: 0.10127 \n",
      "Validation loss decreased (0.136819 --> 0.132875).  Saving model ...\n",
      "[ 33/100] train_loss: 0.11893 valid_loss: 0.13290 test_loss: 0.10040 \n",
      "[ 34/100] train_loss: 0.11742 valid_loss: 0.13348 test_loss: 0.10189 \n",
      "[ 35/100] train_loss: 0.11840 valid_loss: 0.13465 test_loss: 0.10095 \n",
      "[ 36/100] train_loss: 0.11905 valid_loss: 0.13354 test_loss: 0.10165 \n",
      "[ 37/100] train_loss: 0.11685 valid_loss: 0.13369 test_loss: 0.10080 \n",
      "[ 38/100] train_loss: 0.11533 valid_loss: 0.12934 test_loss: 0.09802 \n",
      "Validation loss decreased (0.132875 --> 0.129341).  Saving model ...\n",
      "[ 39/100] train_loss: 0.11346 valid_loss: 0.13492 test_loss: 0.10301 \n",
      "[ 40/100] train_loss: 0.11154 valid_loss: 0.13026 test_loss: 0.09903 \n",
      "[ 41/100] train_loss: 0.11277 valid_loss: 0.13085 test_loss: 0.09893 \n",
      "[ 42/100] train_loss: 0.10887 valid_loss: 0.13093 test_loss: 0.09953 \n",
      "[ 43/100] train_loss: 0.11255 valid_loss: 0.12894 test_loss: 0.09642 \n",
      "Validation loss decreased (0.129341 --> 0.128941).  Saving model ...\n",
      "[ 44/100] train_loss: 0.10953 valid_loss: 0.12940 test_loss: 0.09790 \n",
      "[ 45/100] train_loss: 0.10841 valid_loss: 0.12931 test_loss: 0.09724 \n",
      "[ 46/100] train_loss: 0.10996 valid_loss: 0.12610 test_loss: 0.09597 \n",
      "Validation loss decreased (0.128941 --> 0.126103).  Saving model ...\n",
      "[ 47/100] train_loss: 0.11208 valid_loss: 0.12672 test_loss: 0.09531 \n",
      "[ 48/100] train_loss: 0.10849 valid_loss: 0.12439 test_loss: 0.09735 \n",
      "Validation loss decreased (0.126103 --> 0.124387).  Saving model ...\n",
      "[ 49/100] train_loss: 0.10632 valid_loss: 0.12351 test_loss: 0.09201 \n",
      "Validation loss decreased (0.124387 --> 0.123513).  Saving model ...\n",
      "[ 50/100] train_loss: 0.10597 valid_loss: 0.12389 test_loss: 0.09615 \n",
      "[ 51/100] train_loss: 0.10798 valid_loss: 0.12179 test_loss: 0.09427 \n",
      "Validation loss decreased (0.123513 --> 0.121788).  Saving model ...\n",
      "[ 52/100] train_loss: 0.10418 valid_loss: 0.12164 test_loss: 0.09374 \n",
      "Validation loss decreased (0.121788 --> 0.121638).  Saving model ...\n",
      "[ 53/100] train_loss: 0.10391 valid_loss: 0.12238 test_loss: 0.09576 \n",
      "[ 54/100] train_loss: 0.10495 valid_loss: 0.12490 test_loss: 0.09500 \n",
      "[ 55/100] train_loss: 0.10869 valid_loss: 0.12252 test_loss: 0.09421 \n",
      "[ 56/100] train_loss: 0.10333 valid_loss: 0.12039 test_loss: 0.09214 \n",
      "Validation loss decreased (0.121638 --> 0.120390).  Saving model ...\n",
      "[ 57/100] train_loss: 0.10322 valid_loss: 0.11775 test_loss: 0.09228 \n",
      "Validation loss decreased (0.120390 --> 0.117754).  Saving model ...\n",
      "[ 58/100] train_loss: 0.10578 valid_loss: 0.11676 test_loss: 0.09051 \n",
      "Validation loss decreased (0.117754 --> 0.116764).  Saving model ...\n",
      "[ 59/100] train_loss: 0.10197 valid_loss: 0.12063 test_loss: 0.09291 \n",
      "[ 60/100] train_loss: 0.10321 valid_loss: 0.11647 test_loss: 0.09350 \n",
      "Validation loss decreased (0.116764 --> 0.116471).  Saving model ...\n",
      "[ 61/100] train_loss: 0.10482 valid_loss: 0.11425 test_loss: 0.09081 \n",
      "Validation loss decreased (0.116471 --> 0.114252).  Saving model ...\n",
      "[ 62/100] train_loss: 0.10580 valid_loss: 0.11694 test_loss: 0.09128 \n",
      "[ 63/100] train_loss: 0.10433 valid_loss: 0.11814 test_loss: 0.09234 \n",
      "[ 64/100] train_loss: 0.10599 valid_loss: 0.11678 test_loss: 0.09259 \n",
      "[ 65/100] train_loss: 0.10005 valid_loss: 0.11728 test_loss: 0.08918 \n",
      "[ 66/100] train_loss: 0.09896 valid_loss: 0.11546 test_loss: 0.09163 \n",
      "[ 67/100] train_loss: 0.10135 valid_loss: 0.11656 test_loss: 0.09197 \n",
      "[ 68/100] train_loss: 0.10017 valid_loss: 0.11439 test_loss: 0.08863 \n",
      "[ 69/100] train_loss: 0.10037 valid_loss: 0.11291 test_loss: 0.08856 \n",
      "Validation loss decreased (0.114252 --> 0.112912).  Saving model ...\n",
      "[ 70/100] train_loss: 0.10029 valid_loss: 0.11381 test_loss: 0.08910 \n",
      "[ 71/100] train_loss: 0.09785 valid_loss: 0.11493 test_loss: 0.09009 \n",
      "[ 72/100] train_loss: 0.09720 valid_loss: 0.11377 test_loss: 0.08883 \n",
      "[ 73/100] train_loss: 0.10136 valid_loss: 0.11485 test_loss: 0.08859 \n",
      "[ 74/100] train_loss: 0.09943 valid_loss: 0.11324 test_loss: 0.09058 \n",
      "[ 75/100] train_loss: 0.10069 valid_loss: 0.11041 test_loss: 0.08717 \n",
      "Validation loss decreased (0.112912 --> 0.110409).  Saving model ...\n",
      "[ 76/100] train_loss: 0.10114 valid_loss: 0.11025 test_loss: 0.08812 \n",
      "Validation loss decreased (0.110409 --> 0.110245).  Saving model ...\n",
      "[ 77/100] train_loss: 0.09547 valid_loss: 0.11247 test_loss: 0.09063 \n",
      "[ 78/100] train_loss: 0.09532 valid_loss: 0.11372 test_loss: 0.09192 \n",
      "[ 79/100] train_loss: 0.09728 valid_loss: 0.11080 test_loss: 0.08659 \n",
      "[ 80/100] train_loss: 0.09690 valid_loss: 0.11141 test_loss: 0.08766 \n",
      "[ 81/100] train_loss: 0.09439 valid_loss: 0.10930 test_loss: 0.08588 \n",
      "Validation loss decreased (0.110245 --> 0.109296).  Saving model ...\n",
      "[ 82/100] train_loss: 0.09568 valid_loss: 0.10811 test_loss: 0.08847 \n",
      "Validation loss decreased (0.109296 --> 0.108111).  Saving model ...\n",
      "[ 83/100] train_loss: 0.09516 valid_loss: 0.10985 test_loss: 0.08876 \n",
      "[ 84/100] train_loss: 0.09374 valid_loss: 0.10999 test_loss: 0.08814 \n",
      "[ 85/100] train_loss: 0.09556 valid_loss: 0.11155 test_loss: 0.08878 \n",
      "[ 86/100] train_loss: 0.09570 valid_loss: 0.10860 test_loss: 0.08587 \n",
      "[ 87/100] train_loss: 0.09281 valid_loss: 0.10872 test_loss: 0.08619 \n",
      "[ 88/100] train_loss: 0.09301 valid_loss: 0.10829 test_loss: 0.08631 \n",
      "[ 89/100] train_loss: 0.09300 valid_loss: 0.10938 test_loss: 0.08434 \n",
      "[ 90/100] train_loss: 0.09214 valid_loss: 0.10804 test_loss: 0.08602 \n",
      "Validation loss decreased (0.108111 --> 0.108040).  Saving model ...\n",
      "[ 91/100] train_loss: 0.09476 valid_loss: 0.11193 test_loss: 0.08715 \n",
      "[ 92/100] train_loss: 0.09268 valid_loss: 0.10960 test_loss: 0.08735 \n",
      "[ 93/100] train_loss: 0.09317 valid_loss: 0.10844 test_loss: 0.08705 \n",
      "[ 94/100] train_loss: 0.09394 valid_loss: 0.10815 test_loss: 0.08968 \n",
      "[ 95/100] train_loss: 0.09148 valid_loss: 0.10797 test_loss: 0.08961 \n",
      "Validation loss decreased (0.108040 --> 0.107966).  Saving model ...\n",
      "[ 96/100] train_loss: 0.09154 valid_loss: 0.10899 test_loss: 0.08810 \n",
      "[ 97/100] train_loss: 0.09429 valid_loss: 0.10908 test_loss: 0.08570 \n",
      "[ 98/100] train_loss: 0.09383 valid_loss: 0.10844 test_loss: 0.08451 \n",
      "[ 99/100] train_loss: 0.09223 valid_loss: 0.10908 test_loss: 0.08411 \n",
      "[100/100] train_loss: 0.09321 valid_loss: 0.10823 test_loss: 0.08510 \n",
      "Model 21 trained for unseen data.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model weights saved to UKDALE_final_model_weights.pth\n",
      "Evaluating fridge model\n",
      "Loading model from UKDALE_seen_model_1.pth\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_activation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     47\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_filename))\n\u001b[1;32m---> 48\u001b[0m x_true, p_true, s_true, s_hat \u001b[38;5;241m=\u001b[39m evaluate_activation(model, dl_house_test[\u001b[38;5;241m0\u001b[39m], i)\n\u001b[0;32m     50\u001b[0m s_hat \u001b[38;5;241m=\u001b[39m get_status(s_hat, thr, MIN_OFF[i], MIN_ON[i])\n\u001b[0;32m     51\u001b[0m p_hat \u001b[38;5;241m=\u001b[39m ds_appliance[\u001b[38;5;241m0\u001b[39m][appliance]\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m ds_status[\u001b[38;5;241m0\u001b[39m][appliance]\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m*\u001b[39m s_hat\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate_activation' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Unseen data training and validation\n",
    "train_loader = dl_train_unseen\n",
    "valid_loader = dl_valid_unseen\n",
    "test_loader = dl_test_unseen\n",
    "n_epochs = 100\n",
    "for i in range(20):\n",
    "    print('TRAINING MODEL for unseen house %d' % (i + 2))\n",
    "    model = PTPNet(1, 3, 32)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    model_filename = f'UKDALE_unseen_model_{i+2}.pth'\n",
    "    model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, model_filename)\n",
    "    print(f\"Model {i+2} trained for unseen data.\")\n",
    "\n",
    "# Plot the training and validation loss for seen and unseen houses\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(valid_loss, label='Validation Loss')\n",
    "plt.plot(test_loss, label='Test Loss')\n",
    "plt.yscale('log')\n",
    "plt.title('Training, Validation, and Test Loss for Seen and Unseen Houses')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save the final model weights after training is completed\n",
    "final_model_weights_path = 'UKDALE_final_model_weights.pth'\n",
    "torch.save(model.state_dict(), final_model_weights_path)\n",
    "print(f\"Final model weights saved to {final_model_weights_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTPNet(\n",
       "  (encoder1): Encoder(\n",
       "    (conv): Conv1d(1, 32, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder2): Encoder(\n",
       "    (conv): Conv1d(32, 64, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder3): Encoder(\n",
       "    (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder4): Encoder(\n",
       "    (conv): Conv1d(128, 256, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool1): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(5,), stride=(5,), padding=(0,))\n",
       "    (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool2): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(10,), stride=(10,), padding=(0,))\n",
       "    (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool3): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(20,), stride=(20,), padding=(0,))\n",
       "    (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool4): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(30,), stride=(30,), padding=(0,))\n",
       "    (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv): ConvTranspose1d(512, 32, kernel_size=(8,), stride=(8,), bias=False)\n",
       "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (activation): Conv1d(32, 3, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PTPNet(1,3,32).to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./UKDALE_seen_0.pth\n",
      "./UKDALE_seen_1.pth\n",
      "./UKDALE_seen_2.pth\n",
      "./UKDALE_seen_3.pth\n",
      "./UKDALE_seen_4.pth\n",
      "./UKDALE_seen_5.pth\n",
      "./UKDALE_seen_6.pth\n",
      "./UKDALE_seen_7.pth\n",
      "./UKDALE_seen_8.pth\n",
      "./UKDALE_seen_9.pth\n",
      "./UKDALE_seen_10.pth\n",
      "./UKDALE_seen_11.pth\n",
      "./UKDALE_seen_12.pth\n",
      "./UKDALE_seen_13.pth\n",
      "./UKDALE_seen_14.pth\n",
      "./UKDALE_seen_15.pth\n",
      "./UKDALE_seen_16.pth\n",
      "./UKDALE_seen_17.pth\n",
      "./UKDALE_seen_18.pth\n",
      "./UKDALE_seen_19.pth\n",
      "\n",
      "fridge\n",
      "F1 score  : 0.838 (0.835, 0.841)\n",
      "Precision : 0.847 (0.839, 0.856)\n",
      "Recall    : 0.828 (0.820, 0.841)\n",
      "Accuracy  : 0.854 (0.851, 0.857)\n",
      "MCC       : 0.706 (0.699, 0.712)\n",
      "MAE       : 17.50 (17.23, 17.77)\n",
      "SAE       : -0.025 (-0.043, -0.003)\n",
      "\n",
      "dish_washer\n",
      "F1 score  : 0.907 (0.879, 0.933)\n",
      "Precision : 0.922 (0.890, 0.947)\n",
      "Recall    : 0.893 (0.848, 0.945)\n",
      "Accuracy  : 0.996 (0.994, 0.997)\n",
      "MCC       : 0.905 (0.876, 0.932)\n",
      "MAE       : 20.64 (20.05, 21.25)\n",
      "SAE       : -0.048 (-0.114, 0.006)\n",
      "\n",
      "washing_machine\n",
      "F1 score  : 0.971 (0.963, 0.976)\n",
      "Precision : 0.963 (0.948, 0.973)\n",
      "Recall    : 0.980 (0.971, 0.985)\n",
      "Accuracy  : 0.996 (0.995, 0.996)\n",
      "MCC       : 0.969 (0.960, 0.974)\n",
      "MAE       : 42.43 (42.00, 42.93)\n",
      "SAE       : -0.066 (-0.082, -0.052)\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for a in range(3):\n",
    "    scores[a] = {}\n",
    "    scores[a]['F1'] = []\n",
    "    scores[a]['Precision'] = []\n",
    "    scores[a]['Recall'] = []\n",
    "    scores[a]['Accuracy'] = []\n",
    "    scores[a]['MCC'] = []\n",
    "    scores[a]['MAE'] = []\n",
    "    scores[a]['SAE'] = []\n",
    "\n",
    "thr = 0.5\n",
    "predicted_consumptions = {appliance: [] for appliance in APPLIANCE}\n",
    "for i in range(20):\n",
    "    \n",
    "    filename = './UKDALE_seen_%d.pth' %i\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(3):\n",
    "        appliance_name = APPLIANCE[a]\n",
    "        pm = ds_appliance[0][APPLIANCE[a]].sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_test[0], a)\n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "        p_hat = pm * s_hat\n",
    "        scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        scores[a]['MAE'].append(mean_absolute_error(p_true, p_hat)*MAX_POWER)\n",
    "        scores[a]['SAE'].append((p_hat.sum() - p_true.sum()) / p_true.sum())\n",
    "        # Collect predicted consumptions\n",
    "        predicted_consumptions[appliance_name].extend(p_hat.tolist())\n",
    "# Retrieve timestamps from the dataset\n",
    "timestamps = ds_meter[0].index.tolist()\n",
    "\n",
    "# Truncate data to match the shortest sequence length\n",
    "min_length = min(len(timestamps), *[len(predicted_consumptions[appliance]) for appliance in APPLIANCE])\n",
    "timestamps = timestamps[:min_length]\n",
    "for appliance in predicted_consumptions:\n",
    "    predicted_consumptions[appliance] = predicted_consumptions[appliance][:min_length]\n",
    "\n",
    "# Save predictions and timestamps for each appliance to CSV\n",
    "for appliance in APPLIANCE:\n",
    "    df = pd.DataFrame({\n",
    "        'timestamps': timestamps,\n",
    "        'predicted_consumption': predicted_consumptions[appliance]\n",
    "    })\n",
    "    output_file = f'{appliance}_predicted_consumptions.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "for i,a in enumerate(APPLIANCE):\n",
    "    print()\n",
    "    print(a)\n",
    "    print('F1 score  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['F1']), sorted(scores[i]['F1'])[1], sorted(scores[i]['F1'])[18]))\n",
    "    print('Precision : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Precision']), sorted(scores[i]['Precision'])[1], sorted(scores[i]['Precision'])[18]))\n",
    "    print('Recall    : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Recall']), sorted(scores[i]['Recall'])[1], sorted(scores[i]['Recall'])[18]))\n",
    "    print('Accuracy  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Accuracy']), sorted(scores[i]['Accuracy'])[1], sorted(scores[i]['Accuracy'])[18]))\n",
    "    print('MCC       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['MCC']), sorted(scores[i]['MCC'])[1], sorted(scores[i]['MCC'])[18]))\n",
    "    print('MAE       : %.2f (%.2f, %.2f)' %(np.mean(scores[i]['MAE']), sorted(scores[i]['MAE'])[1], sorted(scores[i]['MAE'])[18]))\n",
    "    print('SAE       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['SAE']), sorted(scores[i]['SAE'])[1], sorted(scores[i]['SAE'])[18]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
